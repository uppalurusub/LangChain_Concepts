{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e5ab75ddb5c45c594efe23839b6304d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d81724a18b96491eb045a28a6a157880",
              "IPY_MODEL_5b61af2fcdaa4288a95bb36bc3001b97",
              "IPY_MODEL_bb9384c27ee047dcbe046f6269337d6a"
            ],
            "layout": "IPY_MODEL_14d676e5ce8646658b509aa317549222"
          }
        },
        "d81724a18b96491eb045a28a6a157880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eec4a6f1a6840199715e1ebbafb1ca1",
            "placeholder": "​",
            "style": "IPY_MODEL_301de1218a224863b689c668cc0da964",
            "value": "Parsing nodes: 100%"
          }
        },
        "5b61af2fcdaa4288a95bb36bc3001b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe323d6eec64305afdda93609afc5dc",
            "max": 362,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0557a96d527a499aba27fbc7acf6185c",
            "value": 362
          }
        },
        "bb9384c27ee047dcbe046f6269337d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_350d089dc83f43e3a61f11336bf2164d",
            "placeholder": "​",
            "style": "IPY_MODEL_dfcda29dd0b64c86bb51f723e278beff",
            "value": " 362/362 [00:00&lt;00:00, 2218.98it/s]"
          }
        },
        "14d676e5ce8646658b509aa317549222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eec4a6f1a6840199715e1ebbafb1ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301de1218a224863b689c668cc0da964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffe323d6eec64305afdda93609afc5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0557a96d527a499aba27fbc7acf6185c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "350d089dc83f43e3a61f11336bf2164d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfcda29dd0b64c86bb51f723e278beff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dbfdddc836a4cafabf53a9368fd0d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ca702fe73044817a211115dd4c4286c",
              "IPY_MODEL_f59374d6fbde467db971807f530bae71",
              "IPY_MODEL_85ad4f6d5c9f4550870266182c8527b7"
            ],
            "layout": "IPY_MODEL_343792ebfc6e4b1fbe780238e3d831a9"
          }
        },
        "4ca702fe73044817a211115dd4c4286c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1cc7de5e4484c40a9c93420d5d5ab4b",
            "placeholder": "​",
            "style": "IPY_MODEL_68b96396440142b293383ea30cfb9452",
            "value": "Generating embeddings: 100%"
          }
        },
        "f59374d6fbde467db971807f530bae71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80eb7b00f0b545f4ac6ee1382efdd8d9",
            "max": 361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f898e21b7e9486f9f55cb7e5650aae6",
            "value": 361
          }
        },
        "85ad4f6d5c9f4550870266182c8527b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018f258bb20346809591a4221b85d94e",
            "placeholder": "​",
            "style": "IPY_MODEL_6db830500443440d8d413a7d98069f67",
            "value": " 361/361 [00:06&lt;00:00, 59.55it/s]"
          }
        },
        "343792ebfc6e4b1fbe780238e3d831a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1cc7de5e4484c40a9c93420d5d5ab4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b96396440142b293383ea30cfb9452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80eb7b00f0b545f4ac6ee1382efdd8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f898e21b7e9486f9f55cb7e5650aae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "018f258bb20346809591a4221b85d94e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db830500443440d8d413a7d98069f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR46vInw4nfM",
        "outputId": "7fe454dc-c18b-4617-815b-f040b3b1c0d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.44)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.13)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpKNd8BM5OpX",
        "outputId": "45af9953-194f-4e18-9f12-d36363cb81b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.24-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.24 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.24.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.24->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.24->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.24->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.24->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading llama_index-0.12.24-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.24.post1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl (16 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud_services-0.6.5-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, tiktoken, llama-index-core, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.14 llama-cloud-services-0.6.5 llama-index-0.12.24 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.24.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.9 llama-index-llms-openai-0.3.25 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 pypdf-5.4.0 striprtf-0.0.26 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ehzsd8ol4aqP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import LlamaIndexRetriever\n",
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,GPTVectorStoreIndex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "02h-4bBt4b8V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Your Data"
      ],
      "metadata": {
        "id": "BwQ1X-ZP45yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/Documents.zip\" -d \"/content/Documents\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMvNJjrzLhyo",
        "outputId": "d257e6a1-fbd2-4170-987f-85c2a9b69aae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Documents.zip\n",
            "   creating: /content/Documents/Documents/\n",
            "  inflating: /content/Documents/Documents/Deep Learning1.pdf  \n",
            "  inflating: /content/Documents/Documents/Deep Learning2.pdf  \n",
            "  inflating: /content/Documents/Documents/Deep Learning3.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# Load documents from a local directory\n",
        "documents = SimpleDirectoryReader('/content/Documents/Documents/').load_data()\n",
        "print(\"Loaded Documents:\", documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGwwDiTJ7ETn",
        "outputId": "f174761c-edf6-4fa7-bd14-54d0853d3dd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Documents: [Document(id_='be4d9ef9-033c-4d39-9aae-44f41351c6f4', embedding=None, metadata={'page_label': '1', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fb8171a1-c8a4-48ca-9bfe-3fd7bb069884', embedding=None, metadata={'page_label': '2', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ef336df4-2518-4cc8-9be2-ef56973d18f8', embedding=None, metadata={'page_label': '3', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='24be7236-a299-415c-b70a-d1c4ff4bebc1', embedding=None, metadata={'page_label': '4', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='897ad57c-8c51-4416-b3ea-23140f9a184a', embedding=None, metadata={'page_label': '5', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='997ad30c-654e-4786-9b88-fe74c67dc313', embedding=None, metadata={'page_label': '6', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='86d0d5b2-930d-418b-8721-c6de3ad793e8', embedding=None, metadata={'page_label': '7', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7416ad1a-0dae-4007-b70b-206e2a1ff584', embedding=None, metadata={'page_label': '8', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='eb6175ea-4825-460b-89cd-612c27a419c8', embedding=None, metadata={'page_label': '9', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='85e205bc-0dce-4bdd-a078-a80a8e255dcb', embedding=None, metadata={'page_label': '10', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b0137cbb-b790-44f6-b0dc-6ab0f1c88b38', embedding=None, metadata={'page_label': '11', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cac6d43e-f037-4046-b939-1a55c15575a2', embedding=None, metadata={'page_label': '12', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9db2a9cf-d464-4180-bf04-1ee69718c310', embedding=None, metadata={'page_label': '13', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d836ed11-8dc2-4b72-aa0d-ae5fa41e6e98', embedding=None, metadata={'page_label': '14', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='486f60de-791a-4153-818c-523a33210d32', embedding=None, metadata={'page_label': '15', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2bcefc7a-36a8-4575-8167-1b8495648aae', embedding=None, metadata={'page_label': '16', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='306b9f4a-59ea-4c24-94c1-e676e73cf8b3', embedding=None, metadata={'page_label': '17', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0c6833bd-ce6f-4a21-9e79-a9cf43bfef10', embedding=None, metadata={'page_label': '18', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1015ea17-2c8f-44f6-95e7-5ce27ee52d01', embedding=None, metadata={'page_label': '19', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cb9d0a67-f4b1-4f8f-8161-995d77035f5c', embedding=None, metadata={'page_label': '20', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a66f51c7-4664-45fb-b9f4-f2c81cbcb554', embedding=None, metadata={'page_label': '21', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ab2a4761-8a3b-4f8f-911e-d5880b611df3', embedding=None, metadata={'page_label': '22', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='38c7e257-7eae-4c8e-9439-ca2bd1271160', embedding=None, metadata={'page_label': '23', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='99df3929-d887-4c9e-84ed-baa16cef6cae', embedding=None, metadata={'page_label': '24', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='12e6f410-717f-427e-bf22-c86f962939c3', embedding=None, metadata={'page_label': '25', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3d2b2863-c9e9-4a61-8174-15af2daf424e', embedding=None, metadata={'page_label': '26', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f359ae31-1aa7-4a7e-8402-e7f0f9d12b63', embedding=None, metadata={'page_label': '27', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d32c8676-ac35-4639-81d1-9966f6c27da9', embedding=None, metadata={'page_label': '28', 'file_name': 'Deep Learning1.pdf', 'file_path': '/content/Documents/Documents/Deep Learning1.pdf', 'file_type': 'application/pdf', 'file_size': 9009431, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='da869372-bc0b-4fc7-8ac4-0b05fbcb4aba', embedding=None, metadata={'page_label': '1', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='52f01b54-6fd2-419d-a4e0-740aa05423e5', embedding=None, metadata={'page_label': '2', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\tDeep\\tLearning:\\tRecurrent\\tNeural\\n\\t\\nNetworks\\tin\\tPython\\n\\t\\nLSTM,\\tGRU,\\tand\\tmore\\tRNN\\tmachine\\tlearning\\narchitectures\\tin\\tPython\\tand\\tTheano\\n\\t\\nBy:\\tThe\\tLazyProgrammer\\t(\\nhttps://lazyprogrammer.me\\n)\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5372e823-93e0-4b7d-a5fa-03c7ef4bede8', embedding=None, metadata={'page_label': '3', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Introduction\\nChapter\\t1:\\tThe\\tSimple\\tRecurrent\\tUnit\\nChapter\\t2:\\tThe\\tParity\\tProblem\\nChapter\\t3:\\tRecurrent\\tNeural\\tNetworks\\tfor\\tNLP\\nChapter\\t4:\\tGenerating\\tand\\tClassifying\\tPoetry\\nChapter\\t5:\\tAdvanced\\tRNN\\tUnits\\t-\\tGRU\\tand\\tLSTM\\nChapter\\t6:\\tLearning\\tfrom\\tWikipedia\\tData\\nConclusion\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='207fa6de-9397-489f-8e46-b119ef41ee87', embedding=None, metadata={'page_label': '4', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nIntroduction\\n\\t\\n\\t\\n\\t\\nRecurrent\\tNeural\\tNetworks\\tare\\tall\\tabout\\tlearning\\tsequences.\\n\\t\\n\\t\\n\\t\\nYou\\tmay\\thave\\talready\\tlearned\\tabout\\tMarkov\\tmodels\\tfor\\tsequence\\tmodeling,\\nwhich\\tmake\\tuse\\tof\\tthe\\tMarkov\\tassumption,\\tp{\\tx(t)\\t|\\tx(t-1),\\t...,\\tx(1)\\t}\\t=\\tp{\\tx(t)\\t|\\nx(t-1)\\t}.\\tIn\\tother\\twords,\\tthe\\tcurrent\\tvalue\\tdepends\\tonly\\ton\\tthe\\tlast\\tvalue.\\tWhile\\neasy\\tto\\ttrain,\\tone\\tcan\\timagine\\tthis\\tmay\\tnot\\tbe\\tvery\\trealistic.\\tEx.\\tthe\\tprevious\\nword\\tin\\ta\\tsentence\\tis\\t“and”,\\twhat’s\\tthe\\tnext\\tword?\\n\\t\\n\\t\\n\\t\\nWhereas\\tMarkov\\tModels\\tare\\tlimited\\tby\\tthe\\tMarkov\\tassumption,\\tRecurrent\\nNeural\\tNetworks\\tare\\tnot.\\tAs\\ta\\tresult,\\tthey\\tare\\tmore\\texpressive,\\tand\\tmore\\npowerful\\tthan\\tanything\\twe’ve\\tseen\\ton\\ttasks\\tthat\\twe\\thaven’t\\tmade\\tprogress\\ton\\tin\\ndecades.\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tfirst\\tsection\\tof\\tthe\\tbook\\twe\\tare\\tgoing\\tto\\tadd\\ttime\\tto\\tour\\tneural\\tnetworks.\\nI’ll\\tintroduce\\tyou\\tto\\tthe\\tSimple\\tRecurrent\\tUnit,\\talso\\tknown\\tas\\tthe\\tElman\\tunit.\\n\\t\\n\\t\\n\\t\\nThe\\tSimple\\tRecurrent\\tUnit\\twill\\thelp\\tyou\\tunderstand\\tthe\\tbasics\\tof\\trecurrent\\nneural\\tnetworks\\t-\\tthe\\ttypes\\tof\\ttasks\\tthey\\tcan\\tbe\\tused\\tfor,\\thow\\tto\\tconstruct\\tthe\\nobjective\\tfunctions\\tfor\\tthese\\ttasks,\\tand\\tbackpropagation\\tthrough\\ttime.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2802a4ba-5225-4ffa-ab27-e5140927e0d4', embedding=None, metadata={'page_label': '5', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nWe\\tare\\tgoing\\tto\\trevisit\\ta\\tclassical\\tneural\\tnetwork\\tproblem\\t-\\tthe\\tXOR\\tproblem,\\nbut\\twe’re\\tgoing\\tto\\textend\\tit\\tso\\tthat\\tit\\tbecomes\\tthe\\tparity\\tproblem\\t-\\tyou’ll\\tsee\\nthat\\tregular\\tfeedforward\\tneural\\tnetworks\\twill\\thave\\ttrouble\\tsolving\\tthis\\tproblem\\nbut\\trecurrent\\tnetworks\\twill\\twork\\tbecause\\tthe\\tkey\\tis\\tto\\ttreat\\tthe\\tinput\\tas\\ta\\nsequence.\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tnext\\tsection\\tof\\tthe\\tcourse,\\twe\\tare\\tgoing\\tto\\trevisit\\tone\\tof\\tthe\\tmost\\tpopular\\napplications\\tof\\trecurrent\\tneural\\tnetworks\\t-\\tlanguage\\tmodeling,\\twhich\\tplays\\ta\\nlarge\\trole\\tin\\tnatural\\tlanguage\\tprocessing\\tor\\tNLP.\\n\\t\\n\\t\\n\\t\\nAnother\\tpopular\\tapplication\\tof\\tneural\\tnetworks\\tfor\\tlanguage\\tis\\tword\\tvectors\\tor\\nword\\tembeddings.\\tThe\\tmost\\tcommon\\ttechnique\\tfor\\tthis\\tis\\tcalled\\tWord2Vec,\\tbut\\nI’ll\\tshow\\tyou\\thow\\trecurrent\\tneural\\tnetworks\\tcan\\talso\\tbe\\tused\\tfor\\tcreating\\tword\\nvectors.\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tsection\\tafter,\\twe’ll\\tlook\\tat\\tthe\\tvery\\tpopular\\tLSTM,\\tor\\tlong\\tshort-term\\nmemory\\tunit,\\tand\\tthe\\tmore\\tmodern\\tand\\tefficient\\tGRU,\\tor\\tgated\\trecurrent\\tunit,\\nwhich\\thas\\tbeen\\tproven\\tto\\tyield\\tcomparable\\tperformance.\\n\\t\\n\\t\\n\\t\\nWe’ll\\tapply\\tthese\\tto\\tsome\\tmore\\tpractical\\tproblems,\\tsuch\\tas\\tlearning\\ta\\tlanguage\\nmodel\\tfrom\\tWikipedia\\tdata\\tand\\tvisualizing\\tthe\\tword\\tembeddings\\twe\\tget\\tas\\ta\\nresult.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d24cb05e-c41e-4c02-9378-d6ff5826a1ae', embedding=None, metadata={'page_label': '6', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nOne\\ttip\\tfor\\tgetting\\tthrough\\tthis\\tbook:\\n\\t\\n\\t\\n\\t\\nUnderstand\\tthe\\tmechanics\\tfirst,\\tworry\\tabout\\tthe\\t“meaning”\\tlater.\\n\\t\\n\\t\\n\\t\\nWhen\\twe\\ttalk\\tabout\\tLSTMs\\twe’re\\tgoing\\tto\\tdiscuss\\tits\\tability\\tto\\t“remember”\\tand\\n“forget”\\tthings\\t-\\tkeep\\tin\\tmind\\tthese\\tare\\tjust\\tconvenient\\tnames\\tby\\tway\\tof\\nanalogy.\\n\\t\\n\\t\\n\\t\\nWe’re\\tnot\\tactually\\tbuilding\\tsomething\\tthat’s\\t“remembering”\\tor\\t“forgetting”\\t-\\nthey\\tare\\tjust\\tmathematical\\tformulas.\\n\\t\\n\\t\\n\\t\\nSo\\tworry\\tabout\\tthe\\tmath,\\tand\\tlet\\tthe\\tmeaning\\tcome\\tnaturally\\tto\\tyou.\\n\\t\\n\\t\\n\\t\\nWhat\\tyou\\tespecially\\tdon’t\\twant\\tto\\tdo\\tis\\tthe\\topposite\\t-\\ttry\\tto\\tunderstand\\tthe\\nmeaning\\twithout\\tunderstanding\\tthe\\tmechanics.\\n\\t\\n\\t\\n\\t\\nWhen\\tyou\\tdo\\tthat,\\tthe\\tresult\\tis\\tusually\\ta\\tsensationalist\\tmedia\\tarticle\\tor\\ta\\tpop-\\nscience\\tbook\\t-\\tthis\\tbook\\tis\\tthe\\topposite\\tof\\tthat.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='be38b967-7423-483a-aa91-4895e546c382', embedding=None, metadata={'page_label': '7', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nWe\\twant\\tto\\tunderstand\\ton\\ta\\ttechnical\\tlevel\\twhat’s\\thappening\\t-\\texplaining\\tthings\\nin\\tlayman\\tterms\\tor\\tthinking\\tof\\treal-life\\tanalogies\\tis\\ticing\\ton\\tthe\\tcake\\t\\nonly\\tif\\n\\tyou\\nunderstand\\tthe\\ttechnicalities.\\n\\t\\n\\t\\n\\t\\nAll\\tthe\\tcode\\tfor\\tthis\\tclass\\tis\\thosted\\ton\\tGithub,\\tand\\tyou\\tcan\\tget\\tit\\tfrom\\nhttps://github.com/lazyprogrammer/machine_learning_examples\\n\\t in\\t the\\t folder\\nrnn_class.\\n\\t\\n\\t\\n\\t\\nGit\\tis\\ta\\tversion\\tcontrol\\tsystem\\tthat\\tallows\\tme\\tto\\tpush\\tupdates\\tand\\tkeep\\ta\\thistory\\nof\\tall\\tthe\\tchanges.\\n\\t\\n\\t\\n\\t\\nYou\\tshould\\talways\\tdo\\ta\\t“git\\tpull”\\tto\\tmake\\tsure\\tyou\\thave\\tthe\\tlatest\\tversion.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='55816f70-1996-4c70-a48e-86b9bf9b0312', embedding=None, metadata={'page_label': '8', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nFormatting\\n\\t\\n\\t\\n\\t\\nI\\tknow\\tthat\\tthe\\te-book\\tformat\\tcan\\tbe\\tquite\\tlimited\\ton\\tmany\\tplatforms.\\tIf\\tyou\\nfind\\tthe\\tformatting\\tin\\tthis\\tbook\\tlacking,\\tparticularly\\tfor\\tthe\\tcode\\tor\\tdiagrams,\\nplease\\tshoot\\tme\\tan\\temail\\tat\\t\\ninfo@lazyprogrammer.me\\n\\talong\\twith\\ta\\tproof-of-\\npurchase,\\tand\\tI\\twill\\tsend\\tyou\\tthe\\toriginal\\tePub\\tfrom\\twhich\\tthis\\tbook\\twas\\ncreated.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='39ad5a37-27d9-4520-99bb-6f30eaf5a00d', embedding=None, metadata={'page_label': '9', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t1:\\tThe\\tSimple\\tRecurrent\\tUnit\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tchapter\\twe\\tare\\tgoing\\tto\\ttalk\\tabout\\tthe\\t“Simple\\tRecurrent\\tUnit”,\\talso\\nknown\\tas\\tthe\\t“Elman\\tUnit”.\\tThis\\tis\\tthe\\tmost\\tbasic\\trecurrent\\tunit.\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e2f8d676-a4b4-4103-a3e6-afd50bd87053', embedding=None, metadata={'page_label': '10', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nSequence\\tRepresentation\\n\\t\\n\\t\\n\\t\\nBefore\\twe\\tgo\\tinto\\tthe\\trecurrent\\tunit\\titself\\t-\\tlet’s\\ttalk\\tabout\\tsequences.\\tWhy?\\nBecause\\tthese\\twill\\tlook\\tslightly\\tdifferent\\tfrom\\twhat\\twe’re\\tused\\tto.\\n\\t\\n\\t\\n\\t\\nRecall\\tthat\\tour\\tinput\\tdata\\tX\\tis\\tusually\\trepresented\\twith\\tan\\tNxD\\tmatrix.\\tThat’s\\tN\\nsamples\\tand\\tD\\tfeatures.\\n\\t\\n\\t\\n\\t\\nBut\\tthat’s\\twhen\\twe\\tweren’t\\tworking\\twith\\tsequences.\\n\\t\\n\\t\\n\\t\\nWell\\tlet’s\\tsuppose\\twe\\tdid\\thave\\ta\\tsequence.\\tHow\\tmany\\tdimensions\\twould\\tthat\\nrequire?\\n\\t\\n\\t\\n\\t\\nCall\\tthe\\tlength\\tof\\tthe\\tsequence\\tT.\\n\\t\\n\\t\\n\\t\\nIf\\tthe\\tobservation\\tis\\ta\\tD-dimensional\\tvector,\\tand\\twe\\thave\\tT\\tof\\tthem,\\tthen\\tone\\nsequence\\tof\\tobservations\\twill\\tbe\\ta\\tTxD\\tmatrix.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2022117a-1797-40cd-901b-625bc8ab7ff5', embedding=None, metadata={'page_label': '11', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nIf\\twe\\thave\\tN\\ttraining\\tsamples,\\tthat\\twill\\tbe\\tan\\tNxTxD\\tmatrix\\t-\\ta\\t3-dimensional\\nobject.\\n\\t\\n\\t\\n\\t\\nSometimes,\\tour\\tsequences\\tare\\tnot\\tof\\tequal\\tlength.\\tThis\\tcan\\thappen\\twhen\\twe’re\\ntalking\\tabout\\tsentences,\\twhich\\tare\\tobviously\\tof\\tarbitrary\\tlength,\\tor\\tperhaps\\nsounds\\tand\\tmusic,\\tor\\tsomeone’s\\tcredit\\thistory.\\tHow\\tcan\\twe\\thandle\\tthis?\\n\\t\\n\\t\\n\\t\\nInstead\\tof\\ta\\t3-D\\tmatrix,\\twe’ll\\thave\\ta\\tlength-N\\tlist,\\twhere\\teach\\telement\\tis\\ta\\t2-D\\nobservation\\tsequence\\tof\\tsize\\tT(n)xD.\\n\\t\\n\\t\\n\\t\\nSince\\tPython\\tlists\\tcan\\tcontain\\tany\\tobject\\tas\\tan\\telement,\\tthis\\tis\\tok.\\n\\t\\n\\t\\n\\t\\nIn\\tgeneral,\\twe\\tcan\\trepresent\\tour\\tsequence\\tas:\\tx(1),\\tx(2),\\t…,\\tx(t),\\t…,\\tx(T-1),\\nx(T)\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='04b69a5f-b6db-4449-879c-b7379801904a', embedding=None, metadata={'page_label': '12', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nThe\\tRecurrent\\tUnit\\n\\t\\n\\t\\n\\t\\nBack\\tto\\tour\\tsimple\\trecurrent\\tunit.\\n\\t\\n\\t\\n\\t\\nThe\\tway\\tit\\tworks\\tis\\tthis.\\tTake\\ta\\tsimple\\tfeedforward\\tneural\\tnetwork\\twith\\t1\\nhidden\\tlayer.\\n\\t\\n\\t\\n\\t\\nThe\\tinput\\tis\\ta\\tD-dimensional\\tvector,\\tx(t).\\n\\t\\n\\t\\n\\t\\no-----o-----o\\n\\t\\nx(t)\\t\\th(t)\\t\\ty(t)\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nWhat\\twe\\twant\\tto\\tdo\\tis\\tcreate\\ta\\tfeedback\\tconnection\\tfrom\\tthe\\thidden\\tlayer\\tto\\nitself.\\n\\t\\n\\t\\n\\t\\n_', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='231d3a50-e2e7-427d-b314-3fbc6d36022e', embedding=None, metadata={'page_label': '13', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='_\\n\\t\\n/\\t\\\\\\n\\t\\n\\\\\\t/\\n\\t\\no-----o-----o\\n\\t\\nx(t)\\t\\th(t)\\t\\ty(t)\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nHere’s\\tthe\\tpicture\\tagain\\tbut\\tshowing\\tthe\\tweights.\\n\\t\\n\\t\\n\\t\\n_\\n\\t\\n/\\t\\\\\\tWh\\n\\t\\n\\\\\\t/\\n\\t\\no-----o-----o\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d3aed5fc-9340-403b-b4da-4b7f1096e829', embedding=None, metadata={'page_label': '14', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nWi\\t\\t\\t\\t\\tWo\\n\\t\\n\\t\\n\\t\\nNotice\\tthat\\tthe\\tfeedback\\tloop\\timplies\\tthere’s\\ta\\tdelay\\tof\\t1\\ttime\\tunit.\\n\\t\\n\\t\\n\\t\\nSo\\tthe\\tinput\\tinto\\th(t)\\tis\\tnow\\tnot\\tjust\\tx(t),\\tbut\\th(t-1)\\talso.\\n\\t\\n\\t\\n\\t\\nIn\\tmath,\\twe\\tcan\\trepresent\\th(t)\\tas\\t(assuming\\trow\\tvectors):\\n\\t\\n\\t\\n\\t\\nh(t)\\t=\\tf(x(t)W\\ni\\n\\t+\\th(t-1)W\\nh\\n\\t+\\tb\\nh\\n)\\n\\t\\n\\t\\n\\t\\nWhat\\tis\\tthe\\tsize\\tof\\tW\\nh\\n?\\n\\t\\n\\t\\n\\t\\nJust\\tlike\\tthe\\tother\\tlayers,\\twe\\tconnect\\t“everything-to-everything”.\\n\\t\\n\\t\\n\\t\\nLet\\tM\\t=\\tnumber\\tof\\tdimensions\\tof\\th(t).\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1f67e072-b141-47a4-a933-16a942be992c', embedding=None, metadata={'page_label': '15', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nSo\\tif\\tthere\\tare\\tM\\thidden\\tunits,\\tthe\\tfirst\\thidden\\tunit\\tconnects\\tback\\tto\\tall\\tM\\nhidden\\tunits,\\tthe\\tsecond\\thidden\\tunit\\tconnects\\tback\\tto\\tall\\tM\\thidden\\tunits,\\tand\\tso\\non.\\n\\t\\n\\t\\n\\t\\nSo\\tin\\ttotal\\tthere\\twill\\tbe\\tM\\n2\\n\\thidden-to-hidden\\tweights.\\n\\t\\n\\t\\n\\t\\nAnother\\tway\\tto\\tthink\\tof\\tthis\\tis\\tthat\\th(t)\\tand\\th(t-1)\\tmust\\tbe\\tthe\\tsame\\tsize,\\ntherefore,\\tW\\nh\\n\\tmust\\tbe\\tof\\tsize\\tMxM\\tfor\\tthe\\tequation\\tto\\tbe\\tvalid.\\n\\t\\n\\t\\n\\t\\nNotice\\tthat\\tthe\\t“f”\\tfunction\\tcan\\tbe\\tany\\tone\\tof\\tthe\\tusual\\thidden\\tlayer\\nnonlinearities\\t-\\tusually\\tsigmoid,\\ttanh,\\tor\\trelu.\\n\\t\\n\\t\\n\\t\\nIt’s\\ta\\thyperparameter,\\tjust\\tlike\\twith\\tother\\ttypes\\tof\\tneural\\tnetworks.\\n\\t\\n\\t\\n\\t\\nQuestion\\tfor\\tthe\\treader\\tto\\tthink\\tabout:\\tWhy\\tdoes\\tthe\\tRNN\\tnot\\tmake\\tthe\\tMarkov\\nassumption?\\n\\t\\n\\t\\n\\t\\nDue\\tto\\tthe\\trecursive\\tformulation,\\th(t)\\tneeds\\tto\\thave\\tan\\tinitial\\tstate,\\th(0).\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='684e6c45-4a9d-4afd-a755-dbf319497879', embedding=None, metadata={'page_label': '16', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nThat\\tis,\\tassuming\\teach\\tsequence\\tstarts\\tat\\tthe\\tindex\\t1.\\n\\t\\n\\t\\n\\t\\nSometimes,\\tresearchers\\tjust\\tset\\tthis\\tto\\t0,\\tand\\tother\\ttimes,\\tthey\\ttreat\\tit\\tas\\ta\\nparameter\\tthat\\twe\\tcan\\tuse\\tbackpropagation\\ton.\\n\\t\\n\\t\\n\\t\\nSince\\tTheano\\tautomatically\\tdifferentiates\\tthings\\tfor\\tus,\\twe’ll\\ttreat\\tit\\tas\\tan\\nupdeatable\\tparameter.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\tyou\\tmay\\tadd\\tmultiple\\trecurrent\\tlayers\\tto\\tyour\\trecurrent\\tneural\\nnetwork,\\tjust\\tlike\\twith\\tfeedforward\\tneural\\tnetworks.\\n\\t\\n\\t\\n\\t\\nThe\\tnumber\\tof\\trecurrent\\tlayers\\tis\\ta\\thyperparameter\\tjust\\tlike\\thow\\tthe\\tnumber\\tof\\nhidden\\tlayers\\tis\\tfor\\ta\\tregular\\tfeedforward\\tneural\\tnetwork.\\n\\t\\n\\t\\n\\t\\nThe\\tquestion\\tof\\t“how\\tmany”\\tis\\tthe\\tbest\\tdepends\\ton\\twhat’s\\tbest\\tfor\\tyour\\tspecific\\nproblem.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='229a6a07-d154-47d8-a9e2-96ecf9503b1a', embedding=None, metadata={'page_label': '17', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We\\tcan\\tdo\\tthis\\tlayering\\tfor\\tall\\ttypes\\tof\\tthe\\trecurrent\\tunits\\twe’ll\\tsee\\t-\\tso\\tthat’s\\nthe\\tElman,\\tthe\\tGRU,\\tand\\tthe\\tLSTM.\\n\\t\\n\\t\\n\\t\\n\\t\\t\\t\\t\\t\\n\\t\\n\\t\\\\\\t\\t\\t\\n\\t\\\\\\n\\t\\n\\\\\\t\\n\\t\\t\\t\\\\\\n\\t\\n\\t\\no-----o-----o-----o\\n\\t\\nx(t)\\t\\th1(t)\\th2(t)\\t\\ty(t)\\n\\t\\n\\t\\n\\t\\nAnd\\tthat’s\\tall\\tthere\\tis\\tto\\tit!\\n\\t\\n\\t\\n\\t\\nJust\\tby\\tadding\\tthat\\tone\\trecurrent\\tconnection,\\twe’ve\\talready\\tcreated\\ta\\trecurrent\\nneural\\tnetwork.\\n\\t\\n\\t\\n\\t\\nWe’ll\\tsee\\tin\\tthe\\tcoding\\tlectures\\thow\\tthis\\tcan\\talready\\tdo\\tsome\\tamazing\\tthings,\\nlike\\texponentially\\tdecrease\\tthe\\tnumber\\tof\\thidden\\tunits\\twe\\twould\\thave\\tneeded\\tin\\na\\tfeedforward\\tneural\\tnetwork.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3b1d3559-1c02-4ea6-bf7d-a92137ceecb8', embedding=None, metadata={'page_label': '18', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nPrediction\\tand\\tRelation\\tto\\tMarkov\\tModels\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\tlook\\tmore\\tclosely\\tat\\twhat\\ta\\trecurrent\\tneural\\nnetwork\\tcan\\tpredict\\tand\\ttalk\\tabout\\thow,\\tunder\\tcertain\\tcircumstances,\\twe\\tcan\\nrelate\\tit\\tto\\twhat\\twe\\tknow\\tabout\\tMarkov\\tmodels.\\n\\t\\n\\t\\n\\t\\nAdding\\ta\\ttime\\tcomponent\\tgives\\tus\\ta\\tfew\\tmore\\toptions\\tin\\tterms\\tof\\tthe\\tobjective,\\nor\\tin\\tother\\twords,\\twhat\\twe’re\\ttrying\\tto\\tpredict.\\n\\t\\n\\t\\n\\t\\nLet’s\\tstart\\twith\\ta\\tsimple\\texample.\\n\\t\\n\\t\\n\\t\\nSuppose\\tyou\\twanted\\tto\\tclassify\\tbetween\\tmale\\tand\\tfemale\\tvoices.\\n\\t\\n\\t\\n\\t\\nEach\\tsound\\tsample\\twould\\tthus\\tcorrespond\\tto\\ta\\tsingle\\tlabel.\\n\\t\\n\\t\\n\\t\\nOut\\tof\\tevery\\tdata\\tpoint\\tin\\tthe\\tsequence,\\tx(1),\\t...,\\tx(T),\\tyou\\twould\\tprobably\\twant\\nto\\tconsider\\tthe\\toutput\\tat\\ttime\\tT,\\tsince\\tthat’s\\tthe\\tone\\tthat\\ttakes\\tinto\\taccount\\tthe\\nwhole\\tsequence.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1799eb9c-303e-4471-9878-1734b6032779', embedding=None, metadata={'page_label': '19', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nBut\\tdon’t\\tforget\\tthat\\tfor\\tevery\\th(t)\\tthere\\tis\\ta\\ty(t).\\n\\t\\n\\t\\n\\t\\ny(t)\\tis\\tjust\\tthe\\tfinal\\tlayer\\tcalculated\\tfrom\\th(t).\\tp{y(t)\\t|\\tx(t)}\\t=\\tsoftmax(h(t)W\\no\\n\\t+\\nb\\no\\n)\\t(assuming\\trow\\tvectors).\\n\\t\\n\\t\\n\\t\\nLet’s\\ttry\\tto\\timagine\\tsome\\tsituations\\twhere\\tthis\\tmight\\tbe\\tuseful.\\n\\t\\n\\t\\n\\t\\nThink\\tabout\\tbrain-computer\\tinterfaces.\\n\\t\\n\\t\\n\\t\\nThese\\tare\\tsystems\\tthat\\tare\\tconstantly\\treading\\telectrical\\tsignals\\tfrom\\tyour\\tbrain.\\nSuppose\\tthe\\tpurpose\\tof\\tthis\\tbrain-computer\\tinterface\\tis\\tfor\\tcontrolling\\ta\\nwheelchair.\\n\\t\\n\\t\\n\\t\\nConsidering\\tthat\\tthe\\twheelchair\\tis\\thow\\tyou\\twould\\tget\\taround,\\tyou’ll\\tneed\\tfine-\\ngrain\\tcontrol\\ton\\tthe\\tdevice.\\n\\t\\n\\t\\n\\t\\nSo\\tit\\tmoves\\tforward\\twhen\\tyou\\twant\\tit\\tto\\tmove\\tforward,\\tstops\\twhen\\tyou\\twant\\tit\\nto\\tstop,\\tturns\\tleft\\twhen\\tyou\\twant\\tit\\tto\\tturn\\tleft.\\tThese\\twould\\tbe\\tthe\\tdifferent', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='60037dc6-8756-44cd-9c22-badae6588277', embedding=None, metadata={'page_label': '20', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='output\\tclasses.\\n\\t\\n\\t\\n\\t\\nIdeally\\tyou\\tcould\\tcontrol\\tthis\\tmotion\\tin\\treal-time.\\tSo\\tthe\\taction\\thappens\\t\\nas\\tyou\\nthink\\tof\\tit\\n.\\n\\t\\n\\t\\n\\t\\nThen\\twe\\twould\\tneed\\ta\\ty(t)\\tfor\\tevery\\tx(t),\\tso\\tthis\\tis\\tprecisely\\ta\\tsituation\\twhere\\nwe\\twould\\twant\\tto\\thave\\tnot\\tjust\\tone\\tlabel\\tfor\\tone\\tsequence,\\tbut\\tone\\tlabel\\tfor\\nevery\\tmoment\\tin\\ttime.\\n\\t\\n\\t\\n\\t\\nLet’s\\tnow\\tthink\\tof\\ta\\tthird\\tsituation.\\n\\t\\n\\t\\n\\t\\nSuppose\\twe’re\\tlooking\\tat\\tsequences\\tof\\twords,\\tor\\tin\\tother\\twords,\\tsentences.\\n\\t\\n\\t\\n\\t\\nAs\\tis\\ttypical,\\twe\\twant\\tto\\tpredict\\tthe\\tnext\\tword\\tgiven\\tall\\tthe\\tprevious\\twords.\\n\\t\\n\\t\\n\\t\\nWhat’s\\tinteresting\\tabout\\tthat\\tis\\tthat\\tit’s\\tessentially\\tunsupervised\\tlearning,\\nbecause\\tthere\\tis\\tno\\ttarget\\tlabel,\\tthe\\ttarget\\tis\\tjust\\tthe\\tinput,\\tsimilar\\tto\\nautoencoders.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0f04cc47-3bb9-4a96-b568-4d0aca84549c', embedding=None, metadata={'page_label': '21', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nIn\\tother\\twords,\\twe’re\\ttrying\\tto\\tmodel\\tthe\\tprobability:\\n\\t\\n\\t\\n\\t\\nP{x(t)\\t|\\tx(t-1),\\tx(t-2),\\t…,\\tx(1)}\\n\\t\\n\\t\\n\\t\\nRemember,\\tthis\\tis\\tnot\\tthe\\tMarkov\\tassumption.\\n\\t\\n\\t\\n\\t\\nNow\\twhat\\thappens\\twhen\\twe\\ttry\\tto\\tmake\\tthe\\twhole\\tinput\\tsequence\\tthe\\ttarget\\nsequence?\\n\\t\\n\\t\\n\\t\\nWe\\tare\\tthen\\ttrying\\tto\\tmaximize\\tthe\\tfollowing\\tprobabilities:\\n\\t\\nP{x(1)}\\n\\t\\nP{x(2)|x(1)}\\n\\t\\nP{x(3)|x(2),\\tx(1)}\\n\\t\\nP{x(4)|x(3),\\tx(2),\\tx(1)}\\n\\t\\n...\\n\\t\\nP{x(t)|x(t-1),\\t...,\\tx(1)}\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='30b77a5d-202c-4dee-9ca1-7b7d610965d3', embedding=None, metadata={'page_label': '22', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nWhat\\thappens\\tif\\twe\\tjoin\\tthe\\tfirst\\ttwo\\tterms\\ttogether?\\tUsing\\tBayes\\trule,\\twe\\tget:\\n\\t\\n\\t\\n\\t\\np{x(2),x(1)}\\t=\\tP{x(2)|x(1)}\\tP{x(1)}\\n\\t\\n\\t\\n\\t\\nWhat\\thappens\\tif\\twe\\tjoin\\tthe\\tfirst\\tthree\\tterms\\ttogether?\\tWe\\tget:\\n\\t\\n\\t\\n\\t\\nP{x(3),\\tx(2),\\tx(1)}\\t=\\tP{x(3)|x(2),\\tx(1)}\\tP{x(2)|x(1)}\\tP{x(1)}\\n\\t\\n\\t\\n\\t\\nRemember\\tthis\\tis\\tcalled\\tthe\\t“chain\\trule”\\tof\\tprobability.\\n\\t\\n\\t\\n\\t\\nEventually\\twhat\\twe\\tend\\tup\\twith\\tis\\tjust\\tthe\\tjoint\\tprobability\\tof\\tthe\\tsequence!\\n\\t\\n\\t\\n\\t\\nRecall\\tthat\\tthis\\tis\\talso\\twhat\\twe\\ttry\\tto\\toptimize\\twhen\\tusing\\tHidden\\tMarkov\\nModels\\t(in\\tmy\\tHidden\\tMarkov\\tModels\\tclass),\\texcept\\twe\\trun\\tinto\\tthe\\tunderflow\\nproblem\\tas\\tthe\\tprobability\\tapproaches\\t0.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6487991b-e88e-486c-be13-2c89061b48a3', embedding=None, metadata={'page_label': '23', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nWith\\trecurrent\\tnets,\\twe\\toptimize\\tthis\\tjoint\\tprobability\\timplicitly,\\tso\\twe\\tnever\\nhave\\tto\\tactually\\tcalculate\\tit,\\tthus\\tavoiding\\tany\\tunderflow.\\n\\t\\n\\t\\n\\t\\nIn\\taddition,\\trecurrent\\tnets\\tneed\\tnot\\tmake\\tthe\\tMarkov\\tassumption.\\n\\t\\n\\t\\n\\t\\nThis\\tcould\\tlead\\tus\\tto\\tthe\\tintuition\\tthat\\tthese\\trecurrent\\tneural\\tnetworks\\tmight\\tbe\\nmore\\tpowerful.\\n\\t\\n\\t\\n\\t\\nSo\\tto\\tconclude,\\twe’ve\\tidentified\\t3\\tdifferent\\tways\\tof\\tusing\\trecurrent\\tneural\\nnetworks\\tfor\\tprediction.\\n\\t\\n\\t\\n\\t\\n1)\\tWe\\tcan\\tpredict\\ta\\tlabel\\tover\\tan\\tentire\\tsequence.\\tThe\\texample\\twe\\tused\\twas\\ndifferentiating\\tbetween\\tmale\\tand\\tfemale\\tvoice\\tsamples.\\n\\t\\n\\t\\n\\t\\n2)\\tWe\\tcan\\tpredict\\ta\\tlabel\\tfor\\tevery\\tstep\\tof\\tan\\tinput\\tsequence.\\tThe\\texample\\twe\\nused\\twas\\tcontrolling\\tan\\taccessibility\\tdevice\\tusing\\ta\\tbrain-computer\\tinterface.\\n\\t\\n\\t\\n\\t\\n3)\\tWe\\tcan\\tpredict\\tthe\\tnext\\tvalue\\tin\\ta\\tsequence,\\tgiven\\tthe\\tpast\\tvalues\\tof\\ta\\nsequence.\\tThe\\texample\\twe\\tused\\twas\\tlearning\\tto\\tpredict\\tthe\\tnext\\tword\\tin\\ta\\nsentence.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ab624d12-c532-4521-aed6-9c6a166c7f59', embedding=None, metadata={'page_label': '24', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5eb9b749-6054-4c66-806a-979325c30258', embedding=None, metadata={'page_label': '25', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nBackpropagation\\tThrough\\tTime\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\treturn\\tto\\tthe\\tidea\\tof\\tbackpropagation.\\n\\t\\n\\t\\n\\t\\nThis\\tis\\tjust\\ta\\tmental\\texercise\\tonly\\t-\\tin\\tcode\\twe’re\\tgoing\\tto\\tuse\\tTheano\\tand\\nTensorFlow\\tto\\tcalculate\\tthe\\tgradients\\tand\\tdo\\tour\\tneural\\tnetwork\\ttraining.\\n\\t\\n\\t\\n\\t\\nIn\\tmy\\tprevious\\tbooks\\tand\\tcourses,\\twe\\tlearned\\tthat\\t“backpropagation”\\tis\\tactually\\njust\\ta\\tfancy\\tname\\tfor\\tgradient\\tdescent.\\n\\t\\n\\t\\n\\t\\nIt\\thas\\tsome\\tinteresting\\tproperties,\\tbut\\tthe\\tmethod\\tbehind\\tit\\twas\\texactly\\tthe\\tsame\\nas\\twhat\\twe\\tdid\\twith\\tlogistic\\tregression,\\tjust\\tsimply\\tcalculating\\tthe\\tgradient\\tand\\nmoving\\tin\\tthat\\tdirection.\\n\\t\\n\\t\\n\\t\\nSimilarly,\\tBackpropagation\\tThrough\\tTime,\\tusually\\tabbreviated\\tas\\tBPTT,\\tis\\tjust\\na\\tfancy\\tname\\tfor\\tbackpropagation,\\twhich\\titself\\tis\\tjust\\ta\\tfancy\\tname\\tfor\\tgradient\\ndescent.\\n\\t\\n\\t\\n\\t\\nSo\\twhat\\tdoes\\tthis\\tmean\\tfor\\tus?\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ef0e2ba6-6d59-4b68-8c99-9cc5f4207d8a', embedding=None, metadata={'page_label': '26', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nThat\\tmeans\\tin\\tthe\\tcode,\\tupdating\\tall\\tthe\\tweights\\tis\\tgoing\\tto\\tlook\\texactly\\tthe\\nsame,\\twhich\\tmakes\\tthings\\tvery\\teasy\\tfor\\tus.\\n\\t\\n\\t\\n\\t\\nHowever,\\twe’re\\tstill\\tinterested\\tin\\ttaking\\tthe\\tgradient.\\n\\t\\n\\t\\n\\t\\nFirst,\\tlet’s\\tconsider\\twhat\\tour\\tneural\\tnetwork\\toutput\\twould\\tbe\\tcalculated\\tas\\n(assuming\\tcolumn\\tvectors):\\n\\t\\n\\t\\n\\t\\ny(t)=softmax(W\\no\\nT\\nh(t))\\n\\t\\n\\t\\n\\t\\ny(t)=softmax(W\\no\\nT\\nf(W\\nh\\nT\\nh(t-1)+W\\nx\\nT\\nx(t)))\\n\\t\\n\\t\\n\\t\\ny(t)=softmax(W\\no\\nT\\nf(W\\nh\\nT\\nf(W\\nh\\nT\\nh(t-2)+W\\nx\\nT\\nx(t-1))+W\\nx\\nT\\nx(t)))\\n\\t\\n\\t\\n\\t\\ny(t)=softmax(W\\no\\nT\\nf(W\\nh\\nT\\nf(W\\nh\\nT\\nf(W\\nh\\nT\\nh(t-3)+W\\nx\\nT\\nx(t-2))+W\\nx\\nT\\nx(t-1))+W\\nx\\nT\\nx(t)))\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e2399e24-7534-4fca-9116-819c5bc7f5ea', embedding=None, metadata={'page_label': '27', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nI’ve\\tdropped\\tthe\\tbiases\\tto\\tmake\\tthe\\tpattern\\tmore\\tobvious.\\n\\t\\n\\t\\n\\t\\nThe\\toutput\\tweight\\tof\\tcourse\\toccurs\\tafter\\tthe\\trecurrence,\\tso\\twe\\tdon’t\\tneed\\tto\\nconsider\\ttime\\tin\\tthat\\tcase.\\n\\t\\n\\t\\n\\t\\nBut\\tlet’s\\tsay\\twe\\twanted\\tto\\tgo\\tback\\tin\\ttime\\t3\\tsteps\\tfor\\tW\\nh\\n.\\tWe\\tsee\\tthat\\tW\\nh\\n\\toccurs\\nmultiple\\ttimes!\\tSo\\twe\\tneed\\tto\\tbe\\tvery\\tcareful\\ttaking\\tour\\tderivative.\\n\\t\\n\\t\\n\\t\\nI’ll\\tassume\\tyou\\talready\\tknow\\thow\\tto\\ttake\\tthe\\tderivative\\tof\\tthe\\tcross-entropy\\tand\\nsoftmax,\\twhich\\twe\\tdid\\tin\\tDeep\\tLearning\\tpart\\t1,\\tso\\tthe\\timportant\\tpart\\tto\\tfocus\\ton\\nhere\\tis\\twhat\\thappens\\tafter\\tthat.\\n\\t\\n\\t\\n\\t\\nThe\\timportant\\tthing\\tto\\trealize\\tis\\tthat\\tthere\\tis\\ta\\tproduct\\twhere\\tboth\\tterms\\tdepend\\non\\tW\\nh\\n,\\tso\\tyou\\tneed\\tto\\tuse\\tthe\\tproduct\\trule\\tfrom\\tcalculus.\\n\\t\\n\\t\\n\\t\\nd[W\\nh\\nT\\nh(t-1)]\\t/\\tdW\\nh\\n\\t\\n\\t\\n\\t\\nWe\\twon’t\\tdive\\ttoo\\tdeep\\tinto\\tthis\\tproblem\\tsince\\twe\\twon’t\\tever\\thave\\tto\\tuse\\tthe\\nsolution,\\tbut\\tinstead\\tI\\tjust\\twanted\\tto\\toutline\\ta\\tstrategy\\tfor\\tsolving\\tit\\tif\\tyou\\tever', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a8696cd0-a3aa-466b-a5cf-0c78802c4ff3', embedding=None, metadata={'page_label': '28', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='wanted\\tto.\\n\\t\\n\\t\\n\\t\\nOne\\timportant\\tpattern\\tyou\\twant\\tto\\ttry\\tto\\tsee\\tis\\tthat\\tthe\\tsame\\tthings\\tare\\tgoing\\tto\\nbe\\tmultiplied\\ttogether\\tover\\tand\\tover\\tagain,\\tdue\\tto\\tthe\\tchain\\trule\\tof\\tcalculus.\\n\\t\\n\\t\\n\\t\\nThis\\twill\\thappen\\tfor\\tboth\\tthe\\thidden-to-hidden\\tweights\\tas\\twell\\tas\\tthe\\tinput-to-\\nhidden\\tweights.\\n\\t\\n\\t\\n\\t\\nThe\\tresult\\tis\\tthat\\tyou’ll\\teither\\tget\\tsomething\\tthat\\tgoes\\tdown\\tto\\t0,\\tor\\tsomething\\nthat\\tgets\\tvery\\tlarge\\tvery\\tquickly.\\n\\t\\n\\t\\n\\t\\nThese\\tproblems\\tare\\tcalled\\tthe\\tvanishing\\tgradient\\tproblem\\tand\\tthe\\texploding\\ngradient\\tproblem,\\trespectively.\\n\\t\\n\\t\\n\\t\\nOne\\tsolution\\tthat\\thas\\tbeen\\tproposed\\tfor\\tthe\\tvanishing\\tgradient\\tproblem\\tis\\ngradient\\tclipping.\\n\\t\\n\\t\\n\\t\\nGradient\\tclipping\\tworks\\tas\\tfollows.\\tLet\\tg\\t=\\tgradient\\tof\\tcost\\twrt\\tvariable\\tof\\ninterest,\\tand\\t|g|\\tbe\\tits\\tL2\\tnorm.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b4849127-aeb1-40d3-abc2-226622f94c49', embedding=None, metadata={'page_label': '29', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nif\\t|g|\\t>\\tthreshold:\\n\\t\\ng\\t=\\tthreshold\\t*\\tg\\t/\\t|g|\\n\\t\\n\\t\\n\\t\\nIf\\tyou’re\\tused\\tto\\tadding\\tthings\\tlike\\tmomentum\\tand\\tadaptive\\tlearning\\trates\\tto\\nyour\\tbackpropagation\\tthis\\tshould\\tbe\\ta\\tpretty\\tsimple\\tchange\\tin\\tyour\\tTheano\\ncode.\\n\\t\\n\\t\\n\\t\\nAnother\\t modification\\t to\\t backpropagation\\t through\\t time\\t is\\t Truncated\\nbackpropagation\\tthrough\\ttime.\\n\\t\\n\\t\\n\\t\\nBecause\\tthe\\tderivatives\\twith\\trespect\\tto\\tW\\nh\\n\\tand\\tW\\nx\\n\\tdepend\\ton\\tevery\\tsingle\\ttime\\nstep\\tof\\tthe\\tsequence\\tso\\tfar,\\tthe\\tcalculation\\twill\\ttake\\ta\\tvery\\tlong\\ttime\\tfor\\tvery\\nlong\\tsequences.\\n\\t\\n\\t\\n\\t\\nOne\\tcommon\\tapproximation\\tis\\tjust\\tto\\tstop\\tafter\\ta\\tcertain\\tnumber\\tof\\ttime\\tsteps.\\n\\t\\n\\t\\n\\t\\nOne\\tdisadvantage\\tof\\tthis\\tis\\tthat\\tit\\twon’t\\tincorporate\\tthe\\terror\\tat\\tlonger\\tperiods\\tof\\ntime.\\tBut\\tif\\tyou\\tdon’t\\tcare\\tabout\\tdependencies\\tpast,\\tsay,\\t3\\ttime\\tsteps,\\tthen\\tyou\\ncan\\tjust\\ttruncate\\tat\\t3\\ttime\\tsteps.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bf9fce5b-9b3d-4ff1-b99a-f900de36c62c', embedding=None, metadata={'page_label': '30', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nAn\\talternative\\twould\\tjust\\tbe\\tto\\tnever\\ttrain\\twith\\tsequences\\tlonger\\tthan\\tT=3.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b4f1d2fa-bbc0-47ee-886c-b32630ebd0f0', embedding=None, metadata={'page_label': '31', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t2:\\tThe\\tParity\\tProblem\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tchapter\\twe\\tare\\tgoing\\tto\\tgo\\tback\\tto\\tour\\told\\tfriend,\\tthe\\tXOR\\tproblem,\\tbut\\nwith\\ta\\tnew\\ttwist.\\n\\t\\n\\t\\n\\t\\nRecall\\tthat\\tthe\\tXOR\\tproblem\\tis\\tsimply\\tto\\timplement\\tthe\\tXOR\\tlogic\\tgate\\tusing\\ta\\nneural\\tnetwork.\\n\\t\\n\\t\\n\\t\\nThe\\tresult\\tis\\ta\\t0\\tif\\tthe\\t2\\tinputs\\tare\\tthe\\tsame,\\tand\\tthe\\tresult\\tis\\t1\\tif\\tthe\\t2\\tinputs\\tare\\ndifferent.\\n\\t\\n\\t\\n\\t\\n00\\t->\\t0\\n\\t\\n01\\t->\\t1\\n\\t\\n10\\t->\\t1\\n\\t\\n11\\t->\\t0\\n\\t\\n\\t\\n\\t\\nOne\\tobvious\\tway\\tto\\textend\\tthis\\tproblem\\tis\\tto\\tadd\\tmore\\tinputs.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='15a11b36-02af-41ac-91e1-b11ec3cc733d', embedding=None, metadata={'page_label': '32', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nThe\\tlabel\\tis\\t1\\tif\\tthe\\tnumber\\tof\\tbits\\tthat\\tare\\t1\\tis\\todd,\\tand\\tthe\\tlabel\\tis\\t0\\tif\\tthe\\nnumber\\tof\\tbits\\tthat\\tare\\t1\\tis\\teven.\\n\\t\\n\\t\\n\\t\\n000\\t->\\t0\\n\\t\\n001\\t->\\t1\\n\\t\\n010\\t->\\t1\\n\\t\\n011\\t->\\t0\\n\\t\\n100\\t->\\t1\\n\\t\\n101\\t->\\t0\\n\\t\\n110\\t->\\t0\\n\\t\\n111\\t->\\t1\\n\\t\\n\\t\\n\\t\\nParity\\tappears\\tin\\ta\\tfew\\tplaces\\tin\\tcomputer\\tscience.\\n\\t\\n\\t\\n\\t\\nOne\\tapplication\\tis\\tdata\\ttransmission\\tin\\tcommunication\\tsystems.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b266b1fa-6bf0-4f06-85f6-ec5d46e8c4fb', embedding=None, metadata={'page_label': '33', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nIf\\tyou\\timagine\\tthe\\tsource\\tis\\ttrying\\tto\\tsend\\ta\\tbitstream\\tto\\ta\\treceiver,\\tbut\\tone\\tof\\nthe\\tbits\\tgets\\treversed\\tvia\\tnoise\\t-\\thow\\tcan\\twe\\tdetect\\tthis?\\n\\t\\n\\t\\n\\t\\nOne\\tsimple\\tway\\tto\\tdo\\tthis\\tis\\tto\\tadd\\ta\\tparity\\tbit\\tto\\tthe\\tend\\tof\\ta\\tmessage.\\tWe\\tset\\ta\\nrequirement\\tthat\\teach\\tmessage\\tmust\\thave\\tan\\teven\\tnumber\\tof\\t1\\tbits\\tlet’s\\tsay.\\nThen\\tif\\tthe\\tactual\\tmessage\\thas\\tan\\todd\\tnumber\\tof\\t1s,\\twe’ll\\tmake\\tthe\\tparity\\tbit\\ta\\t1\\nto\\tmake\\tthe\\ttotal\\teven.\\n\\t\\n\\t\\n\\t\\nIf\\tthe\\tactual\\tmessage\\talready\\thas\\tan\\teven\\tnumber\\tof\\t1s,\\twe’ll\\tmake\\tthe\\tparity\\tbit\\na\\t0\\tso\\tthat\\tthe\\ttotal\\tis\\tstill\\teven.\\n\\t\\n\\t\\n\\t\\nThis\\tway,\\twhen\\tthe\\treceiver\\tgets\\tthe\\tmessage,\\tit\\tcan\\tcheck\\thow\\tmany\\t1s\\tthere\\nare,\\tand\\tif\\tthat’s\\todd,\\tthen\\twe\\tknow\\tsomething\\thas\\tgone\\twrong\\tin\\tthe\\ntransmission\\tof\\tthe\\tmessage.\\n\\t\\n\\t\\n\\t\\nYou’ll\\tnotice\\tthat\\twhen\\tyou\\ttry\\tto\\tsolve\\tthe\\tparity\\tproblem\\twith\\ta\\tregular\\nfeedforward\\tneural\\tnetwork,\\tthe\\tnumber\\tof\\thidden\\tunits\\trequired\\tis\\tgoing\\tto\\ngrow\\tvery\\tfast\\tcompared\\tto\\tthe\\tnumber\\tof\\tbits.\\n\\t\\n\\t\\n\\t\\nThink\\tabout\\tthe\\tgeometry\\tof\\tthe\\tproblem\\tto\\tunderstand\\twhy\\tthat\\tis.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e7d28d63-d1b2-45f9-bd67-ef17f0d509d8', embedding=None, metadata={'page_label': '34', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nTry\\tit\\ton\\tyour\\town\\tfor\\tD=12\\tbits.\\tYou’ll\\tneed\\tthousands\\tof\\thidden\\tunits.\\n\\t\\n\\t\\n\\t\\nThat’s\\ta\\tlot\\tof\\thidden\\tunits\\tfor\\tsuch\\ta\\tsmall\\tdata\\tproblem.\\n\\t\\n\\t\\n\\t\\nWe\\tcan\\tmitigate\\tthat\\tproblem\\tsomewhat\\tby\\tcreating\\tdeeper\\tnetworks\\twith\\tless\\nunits\\tper\\tlayer.\\n\\t\\n\\t\\n\\t\\nNow\\tthink\\tabout\\thow\\tyou\\twould\\tsolve\\tthis\\tproblem\\tusing\\trecurrent\\tneural\\nnetworks.\\n\\t\\n\\t\\n\\t\\nThe\\tbasic\\tidea\\tis\\twe\\twould\\t\\nlike\\n\\tthe\\tneural\\tnetwork\\tto\\tkeep\\ttrack\\tof\\tits\\tstate.\\n\\t\\n\\t\\n\\t\\nSo\\tif\\tthe\\toutput\\tis\\t0,\\tand\\twe\\tsee\\tanother\\t1\\tin\\tthe\\tinput,\\twe\\twould\\tlike\\tto\\tswitch\\nto\\ta\\t1.\\n\\t\\n\\t\\n\\t\\nIf\\tthe\\toutput\\tis\\tcurrently\\ta\\t1,\\tand\\twe\\tsee\\tanother\\t1\\tin\\tthe\\tinput,\\twe\\twould\\tlike\\tto\\nswitch\\tto\\ta\\t0.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b68df33d-198a-43df-a9db-cee6f3a3e672', embedding=None, metadata={'page_label': '35', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nIf\\twe\\tsee\\ta\\t0\\tin\\tthe\\tinput,\\twe\\twould\\tlike\\tto\\tjust\\tmaintain\\twhatever\\tour\\tcurrent\\noutput\\tis.\\n\\t\\n\\t\\n\\t\\nThat’s\\twhat\\twe\\twould\\t\\nlike\\n\\tto\\thappen\\t-\\tbut\\tcan\\tthe\\tneural\\tnetwork\\tactually\\tlearn\\nto\\tdo\\tthis?\\tI’ll\\tshow\\tyou\\tthat\\tit\\tcan.\\n\\t\\n\\t\\n\\t\\nOf\\tour\\t3\\tdifferent\\tways\\tto\\tconstruct\\tthe\\terror\\tor\\tobjective\\t-\\twhich\\tis\\tthis?\\n\\t\\n\\t\\n\\t\\nThis\\tis\\tof\\tcourse\\tthe\\tversion\\twhere\\twe\\tcare\\tabout\\tthe\\toutput\\tat\\tevery\\tpoint\\tin\\ntime,\\tjust\\tlike\\twith\\tthe\\tbrain-computer\\tinterface.\\n\\t\\n\\t\\n\\t\\nTo\\tdo\\tthis,\\twe’ll\\thave\\tto\\tdiscard\\tthe\\toriginal\\tlabel,\\twhich\\tis\\tjust\\t1\\tnumber,\\tand\\ncreate\\ta\\tsequence\\tof\\tlabels,\\twhich\\twill\\tbe\\ta\\tsequence\\tof\\tthe\\tsame\\tsize\\tas\\tthe\\ninput\\tthat\\tjust\\tkeeps\\ta\\ttally\\tof\\twhether\\tit’s\\tseen\\tan\\teven\\tor\\todd\\tnumber\\tof\\tones\\nso\\tfar.\\n\\t\\n\\t\\n\\t\\nLet’s\\tfirst\\twrite\\tthe\\tcode\\tto\\tgenerate\\tthe\\tdata:\\n\\t\\ndef\\tall_parity_pairs(nbit):\\n\\t\\nN\\t=\\t2**nbit', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='13df54e4-39fb-445c-9295-99386a3ae805', embedding=None, metadata={'page_label': '36', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='N\\t=\\t2**nbit\\n\\t\\nX\\t=\\tnp.zeros((N,\\tnbit))\\n\\t\\nY\\t=\\tnp.zeros(N)\\n\\t\\nfor\\ti\\tin\\txrange(N):\\n\\t\\nfor\\tj\\tin\\txrange(nbit):\\n\\t\\nif\\ti\\t%\\t(2**(j+1))\\t!=\\t0:\\n\\t\\ni\\t-=\\t2**j\\n\\t\\nX[i,j]\\t=\\t1\\n\\t\\nY[i]\\t=\\tX[i].sum()\\t%\\t2\\n\\t\\nreturn\\tX,\\tY\\n\\t\\n\\t\\n\\t\\nOur\\tsimple\\tRNN\\tcan\\tbe\\tdefined\\tas\\tfollows:\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\tmatplotlib.pyplot\\tas\\tplt\\n\\t\\nclass\\tSimpleRNN:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cb2fa17a-fd09-4410-be4a-f53634188185', embedding=None, metadata={'page_label': '37', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='class\\tSimpleRNN:\\n\\t\\ndef\\t__init__(self,\\tM):\\n\\t\\nself.M\\t=\\tM\\t#\\thidden\\tlayer\\tsize\\n\\t\\ndef\\tfit(self,\\tX,\\tY,\\tlearning_rate=10e-5,\\tmu=0.99,\\treg=1.0,\\tactivation=T.tanh,\\nepochs=200,\\tshow_fig=False):\\tD\\t=\\tX[0].shape[1]\\t#\\tX\\tis\\tof\\tsize\\tN\\tx\\tT(n)\\tx\\tD\\n\\t\\nK\\t=\\tlen(set(Y.flatten()))\\n\\t\\nN\\t=\\tlen(Y)\\n\\t\\nM\\t=\\tself.M\\n\\t\\nself.f\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tinitial\\tweights\\n\\t\\nWx\\t=\\tinit_weight(D,\\tM)\\n\\t\\nWh\\t=\\tinit_weight(M,\\tM)\\n\\t\\nbh\\t=\\tnp.zeros(M)\\n\\t\\nh0\\t=\\tnp.zeros(M)\\n\\t\\nWo\\t=\\tinit_weight(M,\\tK)\\n\\t\\nbo\\t=\\tnp.zeros(K)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='89a62919-4173-425f-89c9-4ccdcb6d6a59', embedding=None, metadata={'page_label': '38', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\n\\t\\n\\t\\n#\\tmake\\tthem\\ttheano\\tshared\\n\\t\\nself.Wx\\t=\\ttheano.shared(Wx)\\n\\t\\nself.Wh\\t=\\ttheano.shared(Wh)\\n\\t\\nself.bh\\t=\\ttheano.shared(bh)\\n\\t\\nself.h0\\t=\\ttheano.shared(h0)\\n\\t\\nself.Wo\\t=\\ttheano.shared(Wo)\\n\\t\\nself.bo\\t=\\ttheano.shared(bo)\\n\\t\\nself.params\\t=\\t[self.Wx,\\tself.Wh,\\tself.bh,\\tself.h0,\\tself.Wo,\\tself.bo]\\n\\t\\n\\t\\n\\t\\nthX\\t=\\tT.fmatrix('X')\\n\\t\\nthY\\t=\\tT.ivector('Y')\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(x_t,\\th_t1):\\n\\t\\n#\\treturns\\th(t),\\ty(t)\\n\\t\\nh_t\\t=\\tself.f(x_t.dot(self.Wx)\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\ty_t\\t=\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='01ed9470-742f-4795-a2a9-49ea8536ed6b', embedding=None, metadata={'page_label': '39', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='h_t\\t=\\tself.f(x_t.dot(self.Wx)\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\ty_t\\t=\\nT.nnet.softmax(h_t.dot(self.Wo)\\t+\\tself.bo)\\treturn\\th_t,\\ty_t\\n\\t\\n\\t\\n\\t\\n[h,\\ty],\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=recurrence,\\n\\t\\noutputs_info=[self.h0,\\tNone],\\n\\t\\nsequences=thX,\\n\\t\\nn_steps=thX.shape[0],\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\npy_x\\t=\\ty[:,\\t0,\\t:]\\n\\t\\nprediction\\t=\\tT.argmax(py_x,\\taxis=1)\\n\\t\\ncost\\t=\\t-T.mean(T.log(py_x[T.arange(thY.shape[0]),\\tthY]))\\tgrads\\t=\\tT.grad(cost,\\nself.params)\\tdparams\\t=\\t[theano.shared(p.get_value()*0)\\tfor\\tp\\tin\\tself.params]\\n\\t\\n\\t\\n\\t\\nupdates\\t=\\t[\\n\\t\\n(p,\\tp\\t+\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tp,\\tdp,\\tg\\tin\\tzip(self.params,\\tdparams,\\tgrads)\\n]\\t+\\t[\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d0fcb100-efa2-4e94-b7b3-6b811b6fab65', embedding=None, metadata={'page_label': '40', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n(dp,\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tdp,\\tg\\tin\\tzip(dparams,\\tgrads)\\t]\\n\\t\\n\\t\\n\\t\\nself.predict_op\\t=\\ttheano.function(inputs=[thX],\\toutputs=prediction)\\tself.train_op\\n=\\ttheano.function(\\n\\t\\ninputs=[thX,\\tthY],\\n\\t\\noutputs=[cost,\\tprediction,\\ty],\\tupdates=updates\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ncosts\\t=\\t[]\\n\\t\\nfor\\ti\\tin\\txrange(epochs):\\n\\t\\nX,\\tY\\t=\\tshuffle(X,\\tY)\\n\\t\\nn_correct\\t=\\t0\\n\\t\\ncost\\t=\\t0\\n\\t\\nfor\\tj\\tin\\txrange(N):\\n\\t\\nc,\\tp,\\trout\\t=\\tself.train_op(X[j],\\tY[j])\\tcost\\t+=\\tc\\n\\t\\nif\\tp[-1]\\t==\\tY[j,-1]:\\n\\t\\nn_correct\\t+=\\t1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0da9a036-e888-4da1-ae56-6b64f75e5859', embedding=None, metadata={'page_label': '41', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='n_correct\\t+=\\t1\\n\\t\\nprint\\t\"i:\",\\ti,\\t\"cost:\",\\tcost,\\t\"classification\\trate:\",\\t(float(n_correct)/N)\\ncosts.append(cost)\\n\\t\\n\\t\\n\\t\\nif\\tshow_fig:\\n\\t\\nplt.plot(costs)\\n\\t\\nplt.show()\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nWe\\tgo\\tthrough\\ta\\tTheano\\tscan\\ttutorial\\tin\\tmy\\tcourse\\tif\\tyou\\tdon’t\\tlet\\tknow\\thow\\tit\\nworks.\\n\\t\\n\\t\\n\\t\\nFinally,\\tlet’s\\twrite\\ta\\tfunction\\tto\\tget\\tthe\\tdata\\tand\\tfit\\tour\\tSimpleRNN\\tto\\tit:\\n\\t\\ndef\\tparity(B=12):\\n\\t\\nX,\\tY\\t=\\tall_parity_pairs(B)\\n\\t\\nN,\\tt\\t=\\tX.shape\\n\\t\\n\\t\\n\\t\\n#\\twe\\twant\\tevery\\ttime\\tstep\\tto\\thave\\ta\\tlabel\\tY_t\\t=\\tnp.zeros(X.shape,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7a5d96bc-eb5f-4dcb-b534-37235b3a9b6d', embedding=None, metadata={'page_label': '42', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='#\\twe\\twant\\tevery\\ttime\\tstep\\tto\\thave\\ta\\tlabel\\tY_t\\t=\\tnp.zeros(X.shape,\\ndtype=np.int32)\\tfor\\tn\\tin\\txrange(N):\\n\\t\\nones_count\\t=\\t0\\n\\t\\nfor\\ti\\tin\\txrange(t):\\n\\t\\nif\\tX[n,i]\\t==\\t1:\\n\\t\\nones_count\\t+=\\t1\\n\\t\\nif\\tones_count\\t%\\t2\\t==\\t1:\\n\\t\\nY_t[n,i]\\t=\\t1\\n\\t\\n\\t\\n\\t\\nX\\t=\\tX.reshape(N,\\tt,\\t1).astype(np.float32)\\n\\t\\nrnn\\t=\\tSimpleRNN(4)\\n\\t\\nrnn.fit(X,\\tY_t,\\tactivation=T.nnet.sigmoid,\\tshow_fig=True)\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='04dda47c-7457-4c8e-abb9-35c1f1761456', embedding=None, metadata={'page_label': '43', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nOn\\tAdding\\tComplexity\\n\\t\\n\\t\\n\\t\\nYou’ve\\tseen\\tthat\\twe\\tcan\\tuse\\ta\\tsimple\\trecurrent\\tunit\\tto\\tsolve\\tthe\\tparity\\tproblem\\nwith\\tjust\\tone\\tlayer.\\n\\t\\n\\t\\n\\t\\nOf\\tcourse\\tthe\\tstudents\\tin\\tthis\\tcourse\\tare\\tinterested\\tin\\tlearning\\tabout\\tGRUs\\tand\\nLSTMs,\\thowever,\\tthere\\tis\\tone\\timportant\\tpoint\\tto\\tconsider.\\n\\t\\n\\t\\n\\t\\nIf\\ta\\tsimpler\\tmodel\\talready\\tsolves\\tyour\\tproblem\\t-\\tthen\\tyou\\tdon’t\\t\\nneed\\n\\tto\\tuse\\ta\\nmore\\tcomplex\\tmodel.\\n\\t\\n\\t\\n\\t\\nIf\\tyou\\tdo,\\tyou’re\\tgoing\\tto\\tadd\\tmore\\ttraining\\ttime,\\tand\\tpossibly\\trun\\tthe\\trisk\\tof\\noverfitting.\\n\\t\\n\\t\\n\\t\\nLSTMs\\tare\\tmuch\\tslower\\tthan\\tsimple\\trecurrent\\tunits\\tand\\tthey\\thave\\tmany\\tmore\\ncomponents.\\n\\t\\n\\t\\n\\t\\nYou\\talways\\twant\\tto\\ttest\\tagainst\\tsimpler\\tmodels\\tas\\tbenchmarks.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7ab51a54-8a87-4d4a-8db0-32c3c96a4ac7', embedding=None, metadata={'page_label': '44', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nIt’s\\tinteresting\\tto\\tnote\\tthat\\tdeep\\tlearning\\tis\\tnot\\ta\\tcommon\\ttechnique\\tfor\\tKaggle\\ncontests.\\tMany\\tof\\tthe\\tcontestants\\tdo\\ta\\tlot\\tof\\tfeature\\tengineering\\tand\\tuse\\tsimpler\\nmodels.\\n\\t\\n\\t\\n\\t\\nDeep\\tlearning\\ttouches\\ta\\tlot\\tof\\tdifferent\\tbut\\trelated\\tfields,\\tincluding\\tmachine\\nlearning,\\tcomputational\\tneuroscience,\\tand\\tartificial\\tintelligence\\t-\\tso\\tthat’s\\twhat\\nmakes\\tit\\tinteresting.\\n\\t\\n\\t\\n\\t\\nAt\\tthe\\tsame\\ttime,\\tfor\\tbasic\\ttasks,\\ta\\tvery\\tsimple\\tand\\tfast\\tmodel\\tmight\\tbe\\tthe\\tbest\\nchoice.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b08ed842-9f5a-4a9e-b10d-913f27fbb90f', embedding=None, metadata={'page_label': '45', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t3:\\tRecurrent\\tNeural\\tNetworks\\tfor\\tNLP\\n\\t\\n\\t\\n\\t\\nNLP,\\tor\\tnatural\\tlanguage\\tprocessing,\\tis\\tbasically\\tmachine\\tlearning\\tfor\\ttext,\\nspeech,\\tand\\tlanguage.\\n\\t\\n\\t\\n\\t\\nIt’s\\ttightly\\tcoupled\\twith\\trecurrent\\tneural\\tnetworks,\\tand\\ta\\tlot\\tof\\tthe\\tRNN\\nexamples\\tyou’ll\\tsee\\there\\tand\\telsewhere\\tuse\\tword\\tsequences\\tas\\texamples,\\tfor\\nseveral\\treasons.\\n\\t\\n\\t\\n\\t\\nFirst,\\tlanguage\\tis\\tan\\teasy\\ttopic\\tto\\tcomprehend.\\tYou\\tuse\\tlanguage\\tall\\tthe\\ttime,\\nwhether\\tyou\\tare\\tspeaking,\\treading,\\tor\\twriting.\\n\\t\\n\\t\\n\\t\\nYou\\tmust\\tunderstand\\tlanguage\\tbecause\\tyou\\tuse\\tit\\ton\\ta\\tdaily\\tbasis,\\tand\\tyou’re\\nusing\\tthose\\tinherent\\tabilities\\tright\\tnow\\tto\\tunderstand\\tthis\\tbook.\\n\\t\\n\\t\\n\\t\\nSecond,\\trecurrent\\tneural\\tnetworks\\tfinally\\tgive\\tus\\ta\\tway\\tto\\tavoid\\thaving\\tto\\ttreat\\nsentences\\tas\\ta\\tbag\\tof\\twords.\\n\\t\\n\\t\\n\\t\\nA\\tlot\\tof\\texamples\\tand\\ttutorials\\tyou’ll\\tsee\\ttreat\\tsentences\\tas\\ta\\tbag\\tof\\twords,\\tand', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bcddfc56-ad8b-44de-817b-047a88cab627', embedding=None, metadata={'page_label': '46', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='what\\thappens\\tis\\tyou\\tlose\\ta\\tlot\\tof\\tinformation\\tthat\\tis\\tvital\\tto\\tthe\\tmeaning\\tof\\tthe\\nsentence.\\n\\t\\n\\t\\n\\t\\nFor\\texample,\\tconsider\\tthe\\tsentence:\\n\\t\\n\\t\\n\\t\\n“Dogs\\tlove\\tcats\\tand\\tI.”\\n\\t\\n\\t\\n\\t\\nThis\\tsentence\\tactually\\talmost\\thas\\tcorrect\\tgrammatical\\tstructure,\\tbut\\tits\\tmeaning\\nis\\tmuch\\tdifferent\\tfrom\\tthe\\toriginal\\tsentence,\\twhich\\twas:\\t“I\\tlove\\tdogs\\tand\\tcats.”\\n\\t\\n\\t\\n\\t\\nSo\\tthere\\tis\\ta\\tlot\\tof\\tinformation,\\tin\\tthe\\tquantitative\\tsense,\\tthat\\tyou\\tthrow\\taway\\nwhen\\tyou\\tuse\\tbag\\tof\\twords.\\n\\t\\n\\t\\n\\t\\nTo\\tbe\\tclear,\\twhat\\tdo\\twe\\tmean\\tby\\tbag-of-words?\\n\\t\\n\\t\\n\\t\\nIf\\tyou’ve\\ttaken\\tmy\\tfirst\\tNLP\\tcourse\\tor\\tmy\\tlogistic\\tregression\\tcourse,\\tI\\tdid\\tan\\nexample\\twhere\\twe\\tused\\tlogistic\\tregression\\tto\\tperform\\tsentiment\\tanalysis.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fed3cd07-fb4d-4ab8-bb77-b012b798c0ad', embedding=None, metadata={'page_label': '47', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The\\ttask\\tis\\tto\\tbe\\table\\tto\\tguess\\twhether\\ta\\tsentence\\tis\\tpositive\\tor\\tnegative.\\n\\t\\n\\t\\n\\t\\nA\\tpositive\\tsentence\\tmight\\tbe\\t“Today\\tis\\ta\\tgreat\\tday!”,\\tand\\ta\\tnegative\\tsentence\\nmight\\tbe,\\t“This\\tis\\tthe\\tworst\\tmovie\\tI’ve\\tever\\tseen.”\\n\\t\\n\\t\\n\\t\\nTo\\tturn\\teach\\tsentence\\tinto\\tan\\tinput\\tvector\\tfor\\tthe\\tclassifier,\\twe\\tfirst\\tstart\\twith\\ta\\nvector\\tof\\t0s\\tof\\tsize\\tV,\\twhich\\tis\\tthe\\tvocabulary\\tsize.\\tSo\\tthere’s\\tan\\tentry\\tfor\\tevery\\nindividual\\tword.\\n\\t\\n\\t\\n\\t\\nWe\\twould\\tkeep\\ttrack\\tof\\twhich\\tword\\tgoes\\twith\\twhich\\tindex\\tusing\\ta\\tdictionary.\\n\\t\\n\\t\\n\\t\\nNow,\\tfor\\tevery\\tword\\tin\\tthe\\tsentence,\\twe’ll\\tset\\tthe\\tcorresponding\\tindex\\tin\\tthe\\nvector\\tto\\t1,\\tor\\tperhaps\\tsome\\tother\\tfrequency\\tmeasure.\\n\\t\\n\\t\\n\\t\\nSo\\tthere’s\\ta\\tnonzero\\tvalue\\tin\\tthe\\tvector\\tfor\\tevery\\tword\\tthat\\tappears\\tin\\tthe\\nsentence,\\tand\\teverywhere\\telse\\tit’s\\t0.\\n\\t\\n\\t\\n\\t\\nYou\\tcan\\tsee\\thow\\tgiven\\tthis\\tvector,\\tit\\twouldn’t\\tbe\\teasy\\tto\\tdetermine\\tthe\\tcorrect\\norder\\tof\\twords\\tin\\tthe\\tsentence.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8e87ef03-e30f-4ebb-a9b4-3dd163eb3b67', embedding=None, metadata={'page_label': '48', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nIt’s\\tnot\\tcompletely\\timpossible,\\tif\\tthe\\twords\\tin\\tthe\\tsentence\\tare\\tsuch\\tthat\\tthere’s\\nonly\\tone\\tpossible\\tordering,\\tbut\\tgenerally\\tspeaking,\\tthere\\tis\\tsome\\tinformation\\nlost.\\n\\t\\n\\t\\n\\t\\nNow,\\twhat\\thappens\\twhen\\tyou\\thave\\ta\\tsentence,\\t“Today\\tis\\ta\\tgood\\tday”\\tversus\\n“Today\\tis\\tnot\\ta\\tgood\\tday”?\\n\\t\\n\\t\\n\\t\\nWell\\tthese\\tboth\\tlead\\tto\\talmost\\tthe\\tsame\\tvector.\\tBag-of-words\\tmodels\\tare\\tknown\\nfor\\tnot\\tbeing\\table\\tto\\thandle\\tnegation.\\n\\t\\n\\t\\n\\t\\nYou\\tcan\\timagine\\tthat\\ta\\trecurrent\\tnet\\twould\\tbe\\tideal\\tfor\\tthis\\ttask\\t-\\tbecause\\tthey\\nkeep\\tstate\\t-\\tso\\tyou\\tmight\\tbe\\table\\tto\\tsee\\thow\\tif\\tthe\\tRNN\\tsaw\\ta\\t“not”,\\tit\\twould\\nthen\\tnegate\\teverything\\tthat\\tcomes\\tafter\\tit.\\n\\t\\n\\t\\n\\t\\nSo\\thow\\tare\\twords\\tusually\\ttreated\\tin\\tdeep\\tlearning?\\n\\t\\n\\t\\n\\t\\nThe\\tpopular\\tmethod\\tat\\tthe\\tmoment,\\twhich\\thas\\tproduced\\timpressive\\tresults,\\tis\\nthe\\tuse\\tof\\tword\\tembeddings,\\tor\\tword\\tvectors.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='22d71334-d93b-4bbb-814f-300eed196259', embedding=None, metadata={'page_label': '49', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nThat\\tmeans,\\tgiven\\ta\\tvocabulary\\tsize\\tV,\\twe\\tchoose\\ta\\tdimensionality\\tthat\\tis\\tmuch\\nsmaller\\tthan\\tthat,\\tcall\\tit\\tD,\\tand\\tthen\\tmap\\teach\\tword\\tvector\\tto\\tsomewhere\\tin\\tthe\\nD-dimensional\\tspace.\\n\\t\\n\\t\\n\\t\\nThe\\tword\\tembedding\\tmatrix\\tis\\tthus\\ta\\tVxD\\tmatrix.\\n\\t\\n\\t\\n\\t\\nBy\\ttraining\\tthe\\tmodel\\tto\\tdo\\tcertain\\tthings,\\tlike\\ttry\\tto\\tpredict\\tthe\\tnext\\tword,\\tor\\ntry\\tto\\tpredict\\tthe\\tsurrounding\\twords,\\twe\\tget\\tword\\tvectors\\tcan\\tbe\\tmanipulated\\nusing\\tarithmetic\\tto\\tproduce\\tanalogies,\\tsuch\\tas:\\n\\t\\n\\t\\n\\t\\nKing\\t-\\tman\\t~=\\tQueen\\t-\\twoman\\n\\t\\n\\t\\n\\t\\nHow\\tdo\\twe\\tuse\\tword\\tembeddings\\twith\\trecurrent\\tneural\\tnetworks?\\n\\t\\n\\t\\n\\t\\nTo\\taccomplish\\tthis,\\twe\\tsimply\\tcreate\\tan\\tembedding\\tlayer\\tin\\tthe\\tRNN.\\n\\t\\n\\t\\n\\t\\nWe\\t\\t\\t\\tWi\\t\\t\\t\\tWo\\n\\t\\no-----o-----o-----o\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='02d3dbc2-0033-4a20-b145-571254bbc2ad', embedding=None, metadata={'page_label': '50', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nw(t)\\t\\tx(t)\\t\\th(t)\\t\\ty(t)\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tfigure\\tabove,\\tthe\\trecurrent\\tconnection\\tat\\th(t)\\tis\\timplicit.\\n\\t\\n\\t\\n\\t\\nw(t)\\trepresents\\ta\\tV\\tdimensional\\tone-hot\\tencoded\\tvector,\\tand\\tx(t)\\tis\\ta\\tD\\ndimensional\\tvector.\\n\\t\\n\\t\\n\\t\\n2\\tquestions\\tarise\\there.\\n\\t\\n\\t\\n\\t\\n1)\\tHow\\tdo\\twe\\ttrain\\tthis\\tmodel?\\tThe\\tanswer\\tis\\tof\\tcourse\\tgradient\\tdescent.\\nSimply\\tinclude\\tW\\ne\\n\\tas\\ta\\tparameter\\twhen\\tyou\\tdo\\tbackpropagation\\twith\\tTheano.\\nAnother\\talternative\\tis\\tpretraining\\tthe\\tword\\tembedding\\tusing\\ta\\tmethod\\tlike\\nword2vec\\tor\\tGLoVe.\\n\\t\\n\\t\\n\\t\\n2)\\tWhat\\tare\\tthe\\ttargets?\\tThis\\tis\\ta\\tvery\\tgood\\tquestion,\\tbecause\\tlanguage\\tmodels\\ndon’t\\tnecessarily\\thave\\ttargets.\\tYou\\tcan\\tattempt\\tto\\tlearn\\tword\\tembeddings\\ton\\ta\\nsentiment\\tanalysis\\ttask,\\tso\\tyour\\ttargets\\tcould\\tbe\\tmovie\\tratings\\tor\\tsome\\tkind\\tof\\nsentiment\\tscore,\\tor\\tyou\\tcan\\talso\\tdo\\tnext-word\\tprediction\\tlike\\twe\\tdiscussed\\nearlier.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='33874995-eb7b-4a4b-ad29-86ea6dccf84f', embedding=None, metadata={'page_label': '51', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nRepresenting\\ta\\tsequence\\tof\\twords\\tas\\ta\\tsequence\\tof\\n\\t\\nword\\tembeddings\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\tI\\tam\\tgoing\\tto\\texplain\\tone\\tsmall\\tdetail\\tin\\tthe\\tcode\\tthat\\tmight\\tbe\\nconfusing.\\n\\t\\n\\t\\n\\t\\nWe\\thave\\ta\\tword\\tembedding\\tmatrix\\tW\\ne\\n,\\twhich\\tof\\tsize\\tVxD,\\tand\\twe\\twould\\tlike\\tto\\nget\\ta\\tsequence\\tof\\tword\\tvectors\\tthat\\trepresent\\ta\\tsentence,\\twhich\\tis\\ta\\tTxD\\tvector.\\n\\t\\n\\t\\n\\t\\nThe\\tword\\tembeddings\\tmust\\tbe\\tpart\\tof\\tthe\\tneural\\tnetwork,\\tso\\tthat\\tthey\\tcan\\tbe\\nupdated\\tvia\\tgradient\\tdescent\\twith\\tall\\tthe\\tother\\tweights.\\n\\t\\n\\t\\n\\t\\nThis\\tmeans\\tthe\\tinput\\tto\\tthe\\tneural\\tnetwork\\twill\\tjust\\tbe\\ta\\tsequence\\tof\\tword\\nindexes,\\tand\\tthe\\tindexes\\tcorrespond\\tto\\thowever\\twe\\tbuild\\tour\\tword2idx\\ndictionary.\\n\\t\\n\\t\\n\\t\\nThis\\talso\\tsaves\\ta\\tlot\\tof\\tspace\\tbecause\\tnow\\twe\\tcan\\trepresent\\teach\\tinput\\tby\\ta\\tTx1\\nvector\\tof\\tints,\\trather\\tthan\\ta\\tTxD\\tmatrix\\tof\\tfloats.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bffd6db2-1c9c-43ea-aa8d-01958f74816b', embedding=None, metadata={'page_label': '52', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nConceptually\\tspeaking,\\twhat\\twe’re\\ttrying\\tto\\timplement\\tis\\tthe\\tfollowing\\tfor\\nloop,\\twhere\\twe\\ttake\\teach\\tword\\tindex\\tin\\tthe\\tinput\\tsequence,\\tgrab\\tits\\ncorresponding\\tword\\tvector,\\tand\\tadd\\tit\\tto\\ta\\tlist\\tof\\tword\\tvectors\\twhich\\tis\\tthe\\noutput\\tsequence.\\n\\t\\n\\t\\n\\t\\nlist_of_word_vectors\\t=\\t[]\\n\\t\\nfor\\tword_idx\\tin\\tsentence:\\n\\t\\nv\\t=\\tWe[word_idx,\\t:]\\n\\t\\nlist_of_word_vectors.append(v)\\n\\t\\n\\t\\n\\t\\nMathematically\\tspeaking,\\tthe\\tway\\tyou\\twould\\tget\\ta\\tword\\tvector\\tis\\tby\\nmultiplying\\tyour\\tone-hot\\tencoded\\tword\\tindex\\tvector\\tby\\tthe\\tword\\tembedding\\nmatrix.\\n\\t\\n\\t\\n\\t\\nx(t)\\t=\\tw(t)W\\ne\\n\\t\\n\\t\\n\\t\\nYou\\tget\\ta\\t1xV\\tvector\\tmultiplied\\tby\\ta\\tVxD\\tmatrix\\twhich\\tresults\\tin\\ta\\t1xD\\tvector,\\nas\\texpected.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='36e69fcc-4237-45ac-a28f-7eac753eac65', embedding=None, metadata={'page_label': '53', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='But\\tsince\\tall\\tof\\tthe\\tone-hotted\\tvectors\\tcan\\tonly\\thave\\tone\\tlocation\\tthat’s\\ta\\tone,\\nand\\tthe\\trest\\tmust\\tbe\\t0,\\twe\\tcan\\ttake\\ta\\tshortcut.\\n\\t\\n\\t\\n\\t\\nThe\\tkey\\tis\\tto\\trealize\\tthat\\tmultiplying\\tthe\\tword\\tembedding\\tby\\ta\\tone-hot\\tvector\\njust\\tgives\\tyou\\tthe\\trow\\twhere\\tthat\\tword\\tembedding\\tvector\\tlives.\\n\\t\\n\\t\\n\\t\\nTherefore,\\twe\\tcan\\tsay\\teach\\trow\\tof\\tthe\\tword\\tembedding\\tmatrix\\trepresents\\ta\\ncorresponding\\tword.\\tThis\\tshould\\tbe\\tpretty\\tobvious\\talready\\tgiven\\tthat\\tthe\\tword\\nembedding\\tmatrix\\thas\\tV\\trows.\\n\\t\\n\\t\\n\\t\\nFurthermore,\\tNumpy\\tand\\tTheano\\tallow\\tyou\\tto\\tretrieve\\ta\\trow\\tfrom\\ta\\tmatrix\\tvery\\neasily\\t-\\tsimply\\tindex\\tit\\tlike\\tan\\tarray.\\n\\t\\n\\t\\n\\t\\nNot\\tonly\\tthat,\\tbut\\tthey\\tallow\\tus\\tto\\tindex\\tarrays\\tusing\\tarrays.\\n\\t\\n\\t\\n\\t\\nE.g.\\n\\t\\n\\t\\n\\t\\nx(t)\\t=\\tWe[w(t)]\\n\\t\\nx(1),\\t...,\\tx(T)\\t=\\tWe[\\t[w(1),\\t...,\\tw(T)]\\t]\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a40faea2-df84-4248-a8bb-e4d65af684d2', embedding=None, metadata={'page_label': '54', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nAs\\tan\\texercise,\\ttry\\tthis\\twith\\tNumpy\\tarrays\\tto\\tprove\\tto\\tyourself\\tthat\\tthey\\tcan\\tbe\\nindexed\\tin\\tthis\\tway.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2751afa2-2c11-4f9d-bcc7-621367067c48', embedding=None, metadata={'page_label': '55', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nFinding\\tWord\\tAnalogies\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\ttalk\\tabout\\thow\\tyou\\tcan\\tactually\\tdo\\tcalculations\\nlike\\tshow\\tking\\t-\\tman\\t+\\twoman\\t=\\tqueen.\\n\\t\\n\\t\\n\\t\\nIt’s\\tquite\\tsimple\\tbut\\tworth\\tgoing\\tthrough\\tanyway.\\n\\t\\n\\t\\n\\t\\nI\\twill\\tdescribe\\tit\\tin\\t2\\tsteps:\\n\\t\\n\\t\\n\\t\\n1)\\tis\\tto\\tconvert\\tall\\t3\\tof\\tthe\\twords\\ton\\tthe\\tleft\\tto\\ttheir\\tword\\tembeddings,\\tor\\tword\\nvectors.\\tOnce\\tthey’re\\tin\\tvector\\tform,\\tyou\\tcan\\tsubtract\\tand\\tadd\\tvery\\teasily.\\n\\t\\n\\t\\n\\t\\nRemember\\tthat\\twe\\tcan\\tjust\\tgrab\\tthe\\tword’s\\tcorresponding\\tword\\tvector\\tby\\nindexing\\tthe\\tword\\tembedding\\tmatrix\\twith\\tthe\\tindex\\tof\\tthe\\tword.\\n\\t\\n\\t\\n\\t\\n2)\\tis\\tto\\tfind\\tthe\\tclosest\\tactual\\tword\\tin\\tour\\tvocabulary\\tto\\tthe\\tequation\\ton\\tthe\\tleft.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6c1ea1c5-7edb-4f4a-bff7-54482c7faad9', embedding=None, metadata={'page_label': '56', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nWhy\\tis\\tthat?\\tBecause\\tthe\\tresult\\tof\\tking\\t-\\tman\\t+\\twoman\\tjust\\tgives\\tus\\ta\\tvector\\t-\\nthere’s\\tno\\tway\\tto\\tmap\\tfrom\\tvectors\\tto\\twords,\\tsince\\ta\\tvector\\tspace\\tis\\tcontinuous\\nand\\tthat\\twould\\trequire\\tan\\tinfinite\\tnumber\\tof\\twords.\\n\\t\\n\\t\\n\\t\\nSo\\tthe\\tidea\\tis\\twe\\tjust\\tfind\\tthe\\tclosest\\tword.\\n\\t\\n\\t\\n\\t\\nNow\\tthere\\tare\\tvarious\\tways\\tof\\tdefining\\tdistance.\\n\\t\\n\\t\\n\\t\\nSometimes\\tyou\\tsee\\tjust\\tthe\\tplain\\tsquared\\tdistance\\tused.\\n\\t\\n\\t\\n\\t\\nIt\\tis\\talso\\tcommon\\tto\\tuse\\tthe\\tcosine\\tdistance.\\tIn\\tthis\\tlatter\\tform,\\tsince\\tonly\\tthe\\nangle\\tmatters,\\tduring\\ttraining\\twe\\tnormalize\\tall\\tthe\\tword\\tvectors\\tso\\tthat\\ttheir\\nlength\\tis\\t1.\\n\\t\\n\\t\\n\\t\\nThen\\twe\\tcan\\tsay\\tthat\\tall\\tthe\\tword\\tembeddings\\tlie\\ton\\tthe\\tunit\\tsphere.\\n\\t\\n\\t\\n\\t\\nOnce\\twe\\thave\\tour\\tdistance\\tfunction,\\thow\\tdo\\twe\\tfind\\tthe\\tword?\\tThe\\tsimplest\\nway\\tis\\tjust\\tto\\tlook\\tat\\tevery\\tword\\tin\\tthe\\tvocabulary,\\tand\\tget\\tthe\\tdistance\\tbetween\\neach\\tvector\\tand\\tyour\\texpression\\tvector.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='45844e15-0ca4-486a-a977-f1d0ec2bf96d', embedding=None, metadata={'page_label': '57', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nKeep\\ttrack\\tof\\tthe\\tsmallest\\tdistance\\tand\\tthen\\treturn\\tthat\\tword.\\n\\t\\n\\t\\n\\t\\nYou\\tmay\\twant\\tto\\tleave\\tout\\tthe\\tother\\twords\\tfrom\\tthe\\tleft\\tside\\tof\\tthe\\tequation,\\nnamely\\tking,\\tman,\\tand\\twoman.\\n\\t\\n\\t\\n\\t\\nPseudocode:\\n\\t\\n\\t\\n\\t\\nv1\\t=\\tWe[\\tword2idx[“king”]\\t]\\n\\t\\nv2\\t=\\tWe[\\tword2idx[“man”]\\t]\\n\\t\\nv4\\t=\\tWe[\\tword2idx[“woman”]\\t]\\n\\t\\nv3\\t=\\tv1\\t-\\tv2\\t+\\tv4\\n\\t\\nbest_word\\t=\\tNone\\n\\t\\nbest_dist\\t=\\tfloat(“inf”)\\n\\t\\nfor\\tword,\\tidx\\tin\\tword2idx.iteritems():\\n\\t\\nif\\tword\\tnot\\tin\\t(“king”,\\t“man”,\\t“woman”):\\n\\t\\nv\\t=\\tWe[\\tword2idx[word]\\t]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='edd8b1a9-2fe3-4d4e-8617-3c624373b55d', embedding=None, metadata={'page_label': '58', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='v\\t=\\tWe[\\tword2idx[word]\\t]\\n\\t\\nd\\t=\\tdist(v,\\tv3)\\n\\t\\nif\\td\\t<\\tbest_dist:\\n\\t\\nbest_dist\\t=\\td\\n\\t\\nbest_word\\t=\\tword\\n\\t\\nprint\\tv1,\\t“-“,\\tv2,\\t“=“,\\tv3,\\t“-“,\\tv4\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c2d180a5-f4e2-407b-8a66-a48a93e73134', embedding=None, metadata={'page_label': '59', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t4:\\tGenerating\\tand\\tClassifying\\tPoetry\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tchapter\\tI’m\\tgoing\\tto\\tgive\\tyou\\tan\\toverview\\tof\\thow\\twe’ll\\tuse\\tan\\tRNN\\tto\\ncreate\\ta\\tlanguage\\tmodel\\tand\\tthen\\tgenerate\\tpoetry.\\n\\t\\n\\t\\n\\t\\nAs\\twe’ve\\tdiscussed,\\tthis\\tis\\tan\\tunsupervised\\tmodel,\\tand\\tour\\tsoftmax\\toutput\\tis\\tthe\\nprobability\\tof\\tthe\\tnext\\tword\\tgiven\\tall\\tthe\\tprevious\\twords\\tin\\ta\\tline.\\n\\t\\n\\t\\n\\t\\nBecause\\tthis\\tis\\ta\\tlanguage\\tmodel,\\twe’ll\\talso\\tneed\\tword\\tembeddings.\\n\\t\\n\\t\\n\\t\\nSo\\tthis\\tRNN\\tis\\tgoing\\tto\\tbe\\ta\\tlittle\\tdifferent\\tthan\\tthe\\tRNN\\twe\\tbuilt\\tfor\\tthe\\tparity\\nproblem.\\n\\t\\n\\t\\n\\t\\nSo\\tto\\tenumerate\\tthe\\tparameters:\\n\\t\\nWe\\t(word\\tembedding\\tof\\tsize\\tVxD)\\n\\t\\nWx\\t(input-to-hidden\\tweights\\tof\\tsize\\tDxM)\\tWh\\t(hidden-to-hidden\\tweights\\tof\\nsize\\tMxM)\\tWo\\t(hidden-to-output\\tweights\\tof\\tsize\\tMxK)\\n\\t\\nWx,\\tWh,\\tand\\tWo\\twill\\thave\\tcorresponding\\tbias\\tterms,\\tand\\twe\\tare\\tassuming\\t1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='518e82a2-29bd-4f78-bd10-d6cecb553c00', embedding=None, metadata={'page_label': '60', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='hidden\\tlayer.\\n\\t\\n\\t\\n\\t\\nAnother\\tdifference\\tis\\tthat\\tthe\\tfit\\tfunction\\twill\\tonly\\ttake\\tin\\tan\\tX,\\tsince\\tthere\\tare\\nno\\ttargets.\\n\\t\\n\\t\\n\\t\\nWithin\\tthe\\tfit\\tfunction,\\twe\\twill\\thowever\\tcreate\\tour\\town\\ttargets.\\tThe\\ttarget\\tfor\\nwords\\t1\\tto\\tt-1\\tshould\\tbe\\tthe\\tword\\tat\\ttime\\tt.\\n\\t\\n\\t\\n\\t\\nBut\\twe\\talso\\tneed\\tto\\tpredict\\tthe\\tend\\tof\\ta\\tline,\\totherwise\\twe\\twould\\tjust\\tcreate\\ninfinitely\\tlong\\tlines.\\n\\t\\n\\t\\n\\t\\nSo\\twe’ll\\tmake\\tthe\\ttarget\\tfor\\tthe\\tfull\\tsequence\\tthe\\tEND\\ttoken.\\n\\t\\n\\t\\n\\t\\nSimilarly,\\twe’ll\\tadd\\ta\\tSTART\\ttoken\\tat\\tthe\\tbeginning\\tof\\tthe\\tinput\\tsequence\\tand\\nits\\ttarget\\twill\\tbe\\tthe\\tfirst\\tword.\\n\\t\\n\\t\\n\\t\\nTo\\tsummarize,\\tthe\\tinput\\tsequence\\twill\\tbe\\t\\nprepended\\n\\twith\\tthe\\tstart\\ttoken,\\tand\\nthe\\toutput\\tsequence\\twill\\tbe\\t\\nappended\\n\\twith\\tthe\\tend\\ttoken.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e8469391-e8b8-4bf6-ab83-692bf89602c4', embedding=None, metadata={'page_label': '61', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nUnlike\\tthe\\tparity\\tproblem,\\twe\\twant\\tto\\tmeasure\\tour\\taccuracy\\trate\\tby\\tevery\\npredicted\\tword,\\tnot\\tjust\\tthe\\tlast\\tword.\\n\\t\\n\\t\\n\\t\\nTo\\tthat\\tend,\\twill\\taccumulate\\tthe\\tnumber\\tof\\tcorrect\\twords\\tguessed,\\tand\\tdivide\\tit\\nby\\tthe\\ttotal\\tnumber\\tof\\twords,\\tNOT\\tthe\\ttotal\\tnumber\\tof\\tsentences,\\tto\\tget\\tthe\\tfinal\\naccuracy.\\n\\t\\n\\t\\n\\t\\nNext,\\tbecause\\twe\\tmight\\twant\\tto\\tgenerate\\tnew\\tpoetry\\twithout\\ttraining\\tthe\\tmodel\\nevery\\ttime\\twe\\tdo\\tit,\\twe’ll\\twant\\tto\\tsave\\tour\\tmodel\\tafter\\tit’s\\ttrained,\\tand\\talso\\thave\\na\\tway\\tto\\tload\\tthe\\tsaved\\tmodel.\\n\\t\\n\\t\\n\\t\\nHere’s\\tthe\\tAPI\\tfor\\tthat:\\n\\t\\n\\t\\n\\t\\ndef\\tsave(self,\\tfilename)\\n\\t\\n\\t\\n\\t\\n@staticmethod\\n\\t\\ndef\\tload(filename)\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fc61aee4-34a4-448e-9c0a-d54f47422372', embedding=None, metadata={'page_label': '62', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Notice\\tone\\tis\\ta\\tstatic\\tmethod\\tand\\tthe\\tother\\tis\\tan\\tinstance\\tmethod.\\n\\t\\n\\t\\n\\t\\nBecause\\tTheano\\tfunctions\\tneed\\tto\\tbe\\tcompiled,\\tyou\\tcan’t\\tsimply\\tjust\\tset\\tthe\\nweights\\tto\\tthe\\tsaved\\tnumpy\\tarrays\\teither\\t-\\twe’ll\\tneed\\tto\\tre-initialize\\tthe\\tobject\\nwith\\tall\\tthe\\trequired\\tTheano\\tfunctions\\tin\\torder\\tto\\tmake\\tpredictions.\\n\\t\\n\\t\\n\\t\\nLet’s\\tdiscuss\\tthe\\tdata\\twe’ll\\tbe\\tlooking\\tat.\\tWe’re\\tgoing\\tto\\ttry\\tto\\tgenerate\\tpoetry\\nby\\tlearning\\tfrom\\tRobert\\tFrost\\tpoems\\t(already\\tprovided\\tin\\tthe\\tcourse\\trepo).\\n\\t\\n\\t\\n\\t\\nWe’ve\\tgot\\tabout\\t1500\\tlines\\tof\\tRobert\\tFrost,\\tand\\twe’ll\\ttreat\\teach\\tline\\tas\\ta\\nsequence.\\n\\t\\n\\t\\n\\t\\nThe\\tbasic\\talgorithm\\tis,\\tfor\\teach\\tline,\\tlowercase\\tall\\tthe\\ttext,\\tremove\\tall\\tthe\\npunctuation,\\tand\\tsplit\\tby\\twhitespace\\tto\\tget\\ta\\tlist\\tof\\ttokens.\\n\\t\\n\\t\\n\\t\\nThen\\tfor\\teach\\ttoken,\\tif\\tit’s\\tnot\\tin\\tour\\tword2idx\\tmap,\\tadd\\tit\\tand\\tgive\\tit\\tan\\tindex.\\n\\t\\n\\t\\n\\t\\nThen\\twe’ll\\tsave\\teach\\tsentence\\tas\\ta\\tsequence\\tof\\tword\\tindexes,\\tand\\treturn\\tboth\\nthat\\tand\\tthe\\tword2idx\\tmap.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='05a9f767-57ee-4a86-b526-027ee86ff933', embedding=None, metadata={'page_label': '63', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nThis\\tis\\tgenerally\\tthe\\tsame\\tprocess\\twe’ll\\tfollow\\tfor\\tbuilding\\tany\\tsort\\tof\\tlanguage\\nmodel,\\tbut\\tyou’ll\\tsee\\thow\\twe\\tcan\\tintroduce\\tfurther\\tmodifications\\twhen\\twe\\tlook\\nat\\tmore\\tcomplicated\\tdatasets.\\n\\t\\n\\t\\n\\t\\nFinally,\\tthe\\tcode\\tfor\\tthis\\texercise\\tis\\tin\\tsrn_language.py\\tin\\tthe\\tcourse\\trepo.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='087bd448-dff0-44ea-ba25-f26254bc012d', embedding=None, metadata={'page_label': '64', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nGenerating\\tPoetry\\tCode\\n\\t\\n\\t\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\tmatplotlib.pyplot\\tas\\tplt\\n\\t\\n\\t\\n\\t\\nfrom\\tsklearn.utils\\timport\\tshuffle\\n\\t\\nfrom\\tutil\\timport\\tinit_weight,\\tget_robert_frost,\\tget_wikipedia_data\\n\\t\\n\\t\\n\\t\\nclass\\tSimpleRNN:\\n\\t\\ndef\\t__init__(self,\\tD,\\tM,\\tV):\\n\\t\\nself.D\\t=\\tD\\t#\\tdimensionality\\tof\\tword\\tembedding\\tself.M\\t=\\tM\\t#\\thidden\\tlayer\\tsize\\n\\t\\nself.V\\t=\\tV\\t#\\tvocabulary\\tsize\\n\\t\\n\\t\\n\\t\\ndef\\tfit(self,\\tX,\\tlearning_rate=10e-1,\\tmu=0.99,\\treg=1.0,\\tactivation=T.tanh,\\tepochs=500,\\tshow_fig=False):\\tN\\n=\\tlen(X)\\n\\t\\nD\\t=\\tself.D', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='876d7939-72d2-4a6a-8993-c4ff7b9c414c', embedding=None, metadata={'page_label': '65', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nM\\t=\\tself.M\\n\\t\\nV\\t=\\tself.V\\n\\t\\nself.f\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tinitial\\tweights\\n\\t\\nWe\\t=\\tinit_weight(V,\\tD)\\n\\t\\nWx\\t=\\tinit_weight(D,\\tM)\\n\\t\\nWh\\t=\\tinit_weight(M,\\tM)\\n\\t\\nbh\\t=\\tnp.zeros(M)\\n\\t\\nh0\\t=\\tnp.zeros(M)\\n\\t\\nWo\\t=\\tinit_weight(M,\\tV)\\n\\t\\nbo\\t=\\tnp.zeros(V)\\n\\t\\n\\t\\n\\t\\n#\\tmake\\tthem\\ttheano\\tshared\\n\\t\\nself.We\\t=\\ttheano.shared(We)\\n\\t\\nself.Wx\\t=\\ttheano.shared(Wx)\\n\\t\\nself.Wh\\t=\\ttheano.shared(Wh)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='24ec6f5a-c7e3-4cd4-aa70-beb765511dcb', embedding=None, metadata={'page_label': '66', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"self.bh\\t=\\ttheano.shared(bh)\\n\\t\\nself.h0\\t=\\ttheano.shared(h0)\\n\\t\\nself.Wo\\t=\\ttheano.shared(Wo)\\n\\t\\nself.bo\\t=\\ttheano.shared(bo)\\n\\t\\nself.params\\t=\\t[self.We,\\tself.Wx,\\tself.Wh,\\tself.bh,\\tself.h0,\\tself.Wo,\\tself.bo]\\n\\t\\n\\t\\n\\t\\nthX\\t=\\tT.ivector('X')\\n\\t\\nEi\\t=\\tself.We[thX]\\t#\\twill\\tbe\\ta\\tTxD\\tmatrix\\tthY\\t=\\tT.ivector('Y')\\n\\t\\n\\t\\n\\t\\n#\\tsentence\\tinput:\\n\\t\\n#\\t[START,\\tw1,\\tw2,\\t...,\\twn]\\n\\t\\n#\\tsentence\\ttarget:\\n\\t\\n#\\t[w1,\\t\\t\\t\\tw2,\\tw3,\\t...,\\tEND]\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(x_t,\\th_t1):\\n\\t\\n#\\treturns\\th(t),\\ty(t)\\n\\t\\nh_t\\t=\\tself.f(x_t.dot(self.Wx)\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\ty_t\\t=\\tT.nnet.softmax(h_t.dot(self.Wo)\\t+\\tself.bo)\\nreturn\\th_t,\\ty_t\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='914d7fe9-9616-4492-a6d1-173a5f1f27c6', embedding=None, metadata={'page_label': '67', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n[h,\\ty],\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=recurrence,\\n\\t\\noutputs_info=[self.h0,\\tNone],\\n\\t\\nsequences=Ei,\\n\\t\\nn_steps=Ei.shape[0],\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\npy_x\\t=\\ty[:,\\t0,\\t:]\\n\\t\\nprediction\\t=\\tT.argmax(py_x,\\taxis=1)\\n\\t\\n\\t\\n\\t\\ncost\\t=\\t-T.mean(T.log(py_x[T.arange(thY.shape[0]),\\tthY]))\\tgrads\\t=\\tT.grad(cost,\\tself.params)\\n\\t\\ndparams\\t=\\t[theano.shared(p.get_value()*0)\\tfor\\tp\\tin\\tself.params]\\n\\t\\n\\t\\n\\t\\nupdates\\t=\\t[\\n\\t\\n(p,\\tp\\t+\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tp,\\tdp,\\tg\\tin\\tzip(self.params,\\tdparams,\\tgrads)\\t]\\t+\\t[\\n\\t\\n(dp,\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tdp,\\tg\\tin\\tzip(dparams,\\tgrads)\\t]\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c3440e52-a78d-4b15-87cf-68cec3974639', embedding=None, metadata={'page_label': '68', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nself.predict_op\\t=\\ttheano.function(inputs=[thX],\\toutputs=prediction)\\tself.train_op\\t=\\ttheano.function(\\n\\t\\ninputs=[thX,\\tthY],\\n\\t\\noutputs=[cost,\\tprediction],\\n\\t\\nupdates=updates\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ncosts\\t=\\t[]\\n\\t\\nn_total\\t=\\tsum((len(sentence)+1)\\tfor\\tsentence\\tin\\tX)\\tfor\\ti\\tin\\txrange(epochs):\\n\\t\\nX\\t=\\tshuffle(X)\\n\\t\\nn_correct\\t=\\t0\\n\\t\\ncost\\t=\\t0\\n\\t\\nfor\\tj\\tin\\txrange(N):\\n\\t\\n#\\tproblem!\\tmany\\twords\\t-->\\tEND\\ttoken\\tare\\toverrepresented\\t#\\tresult:\\tgenerated\\tlines\\twill\\tbe\\tvery\\tshort\\t#\\twe\\nwill\\ttry\\tto\\tfix\\tin\\ta\\tlater\\titeration\\t#\\tBAD!\\tmagic\\tnumbers\\t0\\tand\\t1...\\n\\t\\ninput_sequence\\t=\\t[0]\\t+\\tX[j]\\n\\t\\noutput_sequence\\t=\\tX[j]\\t+\\t[1]\\n\\t\\n\\t\\n\\t\\n#\\twe\\tset\\t0\\tto\\tstart\\tand\\t1\\tto\\tend\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f68e19d7-ea5a-4725-adfa-2372ee0a8ba6', embedding=None, metadata={'page_label': '69', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nc,\\tp\\t=\\tself.train_op(input_sequence,\\toutput_sequence)\\tcost\\t+=\\tc\\n\\t\\nfor\\tpj,\\txj\\tin\\tzip(p,\\toutput_sequence):\\tif\\tpj\\t==\\txj:\\n\\t\\nn_correct\\t+=\\t1\\n\\t\\nprint\\t\"i:\",\\ti,\\t\"cost:\",\\tcost,\\t\"correct\\trate:\",\\t(float(n_correct)/n_total)\\tcosts.append(cost)\\n\\t\\n\\t\\n\\t\\nif\\tshow_fig:\\n\\t\\nplt.plot(costs)\\n\\t\\nplt.show()\\n\\t\\n\\t\\n\\t\\ndef\\tsave(self,\\tfilename):\\n\\t\\nnp.savez(filename,\\t*[p.get_value()\\tfor\\tp\\tin\\tself.params])\\n\\t\\n@staticmethod\\n\\t\\ndef\\tload(filename,\\tactivation):\\n\\t\\n#\\tTODO:\\twould\\tprefer\\tto\\tsave\\tactivation\\tto\\tfile\\ttoo\\tnpz\\t=\\tnp.load(filename)\\n\\t\\nWe\\t=\\tnpz[\\'arr_0\\']\\n\\t\\nWx\\t=\\tnpz[\\'arr_1\\']\\n\\t\\nWh\\t=\\tnpz[\\'arr_2\\']\\n\\t\\nbh\\t=\\tnpz[\\'arr_3\\']', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a8501888-0b9e-4a73-ad2f-64a3918a4ce9', embedding=None, metadata={'page_label': '70', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"bh\\t=\\tnpz['arr_3']\\n\\t\\nh0\\t=\\tnpz['arr_4']\\n\\t\\nWo\\t=\\tnpz['arr_5']\\n\\t\\nbo\\t=\\tnpz['arr_6']\\n\\t\\nV,\\tD\\t=\\tWe.shape\\n\\t\\n_,\\tM\\t=\\tWx.shape\\n\\t\\nrnn\\t=\\tSimpleRNN(D,\\tM,\\tV)\\n\\t\\nrnn.set(We,\\tWx,\\tWh,\\tbh,\\th0,\\tWo,\\tbo,\\tactivation)\\treturn\\trnn\\n\\t\\n\\t\\n\\t\\ndef\\tset(self,\\tWe,\\tWx,\\tWh,\\tbh,\\th0,\\tWo,\\tbo,\\tactivation):\\tself.f\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tredundant\\t-\\tsee\\thow\\tyou\\tcan\\timprove\\tit\\tself.We\\t=\\ttheano.shared(We)\\n\\t\\nself.Wx\\t=\\ttheano.shared(Wx)\\n\\t\\nself.Wh\\t=\\ttheano.shared(Wh)\\n\\t\\nself.bh\\t=\\ttheano.shared(bh)\\n\\t\\nself.h0\\t=\\ttheano.shared(h0)\\n\\t\\nself.Wo\\t=\\ttheano.shared(Wo)\\n\\t\\nself.bo\\t=\\ttheano.shared(bo)\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7e0935f2-c0dd-4054-822e-1cd653dcb32b', embedding=None, metadata={'page_label': '71', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nself.params\\t=\\t[self.We,\\tself.Wx,\\tself.Wh,\\tself.bh,\\tself.h0,\\tself.Wo,\\tself.bo]\\n\\t\\n\\t\\n\\t\\nthX\\t=\\tT.ivector('X')\\n\\t\\nEi\\t=\\tself.We[thX]\\t#\\twill\\tbe\\ta\\tTxD\\tmatrix\\tthY\\t=\\tT.ivector('Y')\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(x_t,\\th_t1):\\n\\t\\n#\\treturns\\th(t),\\ty(t)\\n\\t\\nh_t\\t=\\tself.f(x_t.dot(self.Wx)\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\ty_t\\t=\\tT.nnet.softmax(h_t.dot(self.Wo)\\t+\\tself.bo)\\nreturn\\th_t,\\ty_t\\n\\t\\n\\t\\n\\t\\n[h,\\ty],\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=recurrence,\\n\\t\\noutputs_info=[self.h0,\\tNone],\\n\\t\\nsequences=Ei,\\n\\t\\nn_steps=Ei.shape[0],\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\npy_x\\t=\\ty[:,\\t0,\\t:]\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c9193973-313d-4756-815b-a66c5a207b35', embedding=None, metadata={'page_label': '72', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nprediction\\t=\\tT.argmax(py_x,\\taxis=1)\\n\\t\\nself.predict_op\\t=\\ttheano.function(\\n\\t\\ninputs=[thX],\\n\\t\\noutputs=prediction,\\n\\t\\nallow_input_downcast=True,\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ndef\\tgenerate(self,\\tword2idx):\\n\\t\\n#\\tconvert\\tword2idx\\t->\\tidx2word\\n\\t\\nidx2word\\t=\\t{v:k\\tfor\\tk,v\\tin\\tword2idx.iteritems()}\\n\\t\\nV\\t=\\tlen(word2idx)\\n\\t\\n\\t\\n\\t\\n#\\tgenerate\\t4\\tlines\\tat\\ta\\ttime\\n\\t\\nn_lines\\t=\\t0\\n\\t\\n\\t\\n\\t\\nX\\t=\\t[\\t0\\t]\\n\\t\\nwhile\\tn_lines\\t<\\t4:\\n\\t\\nPY_X,\\t\\n\\t=\\tself.predict\\nop(X)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e23f65de-4537-4d58-9f9e-7da0cc7f1915', embedding=None, metadata={'page_label': '73', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nPY_X\\t=\\tPY_X[-1].flatten()\\n\\t\\nP\\t=\\t[\\tnp.random.choice(V,\\tp=PY_X)]\\n\\t\\nX\\t=\\tnp.concatenate([X,\\tP])\\t#\\tappend\\tto\\tthe\\tsequence\\tP\\t=\\tP[-1]\\t#\\tjust\\tgrab\\tthe\\tmost\\trecent\\tprediction\\tif\\tP\\t>\\n1:\\n\\t\\n#\\tit's\\ta\\treal\\tword,\\tnot\\tstart/end\\ttoken\\tword\\t=\\tidx2word[P]\\n\\t\\nprint\\tword,\\n\\t\\nelif\\tP\\t==\\t1:\\n\\t\\n#\\tend\\ttoken\\n\\t\\nn_lines\\t+=\\t1\\n\\t\\nX\\t=\\t[0]\\n\\t\\nprint\\t''\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\ndef\\ttrain_poetry():\\n\\t\\n#\\tstudents:\\ttanh\\tdidn't\\twork\\tbut\\tyou\\tshould\\ttry\\tit\\tsentences,\\tword2idx\\t=\\tget_robert_frost()\\trnn\\t=\\nSimpleRNN(30,\\t30,\\tlen(word2idx))\\trnn.fit(sentences,\\tlearning_rate=10e-5,\\tshow_fig=True,\\nactivation=T.nnet.relu,\\tepochs=2000)\\trnn.save('RNN_D30_M30_epochs2000_relu.npz')\\n\\t\\ndef\\tgenerate_poetry():\\n\\t\\nsentences,\\tword2idx\\t=\\tget_robert_frost()\\trnn\\t=\\tSimpleRNN.load('RNN_D30_M30_epochs2000_relu.npz',\\nT.nnet.relu)\\trnn.generate(word2idx)\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='507b448f-2dbb-4279-baf4-24b6738868ff', embedding=None, metadata={'page_label': '74', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nif\\t__name__\\t==\\t'__main__':\\n\\t\\ntrain_poetry()\\n\\t\\ngenerate_poetry()\\n\\t\\n\\t\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='53cdcb4e-ec55-4bc8-b737-7c55740eca90', embedding=None, metadata={'page_label': '75', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nClassifying\\tPoetry\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\tlook\\tat\\ta\\tproblem\\twe\\tfirst\\tencountered\\tin\\tmy\\nHidden\\tMarkov\\tModel\\tclass\\t-\\tdiscriminating\\tbetween\\tRobert\\tFrost\\tand\\tEdgar\\nAllan\\tPoe\\tpoems\\tgiven\\tonly\\tsequences\\tof\\tparts-of-speech\\ttags.\\n\\t\\n\\t\\n\\t\\nWe\\twere\\table\\tto\\tget\\taround\\t60-70%\\taccuracy\\ton\\tthe\\tvalidation\\tset\\tusing\\tHMMs,\\nand\\twe’ll\\ttry\\tto\\tdo\\tbetter\\twith\\tRNNs\\t(>\\t90%).\\n\\t\\n\\t\\n\\t\\nFirst,\\tsince\\tthe\\tdata\\tprocessing\\tpart\\tof\\tthis\\texercise\\tis\\ta\\tlittle\\tmore\\tcomplex\\tthan\\nusual,\\twe’ll\\tdiscuss\\tthat\\there.\\n\\t\\n\\t\\n\\t\\nI\\tmentioned\\tparts-of-speech\\ttags.\\tThese\\ttags\\tare\\tbasically\\tthings\\tlike\\tnoun,\\nadverb,\\tadjective,\\tand\\tso\\ton,\\tthat\\ttell\\tus\\twhat\\tthe\\trole\\tof\\teach\\tword\\tis\\tin\\ta\\nsentence.\\n\\t\\n\\t\\n\\t\\nSo\\tgiven\\ta\\tsentence,\\tor,\\ta\\tsequence\\tof\\twords,\\twe\\tcan\\tget\\ta\\tsequence\\tof\\tPOS\\ttags\\nof\\tthe\\tsame\\tlength.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3e72c1d4-c0bf-46aa-9a84-fedc48dcff46', embedding=None, metadata={'page_label': '76', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To\\tdo\\tthis\\twe’ll\\tuse\\ta\\tlibrary\\tcalled\\tNLTK,\\twhich\\tstands\\tfor\\tnatural\\tlanguage\\ntoolkit.\\n\\t\\n\\t\\n\\t\\nIf\\tyou’ve\\talready\\ttaken\\tmy\\tEasy\\tNLP\\tclass,\\tyou\\tshould\\talready\\thave\\tit\\tinstalled,\\nbut\\tjust\\tin\\tcase\\tyou\\tdon’t,\\tit’s\\tvery\\teasy,\\tjust\\tuse\\t“sudo\\tpip\\tinstall\\tnltk”.\\tThere\\nmay\\tbe\\tsome\\texternal\\tpackages\\tyou’ll\\tneed\\tto\\tinstall\\tthat\\tNLTK\\twill\\trequire,\\tbut\\nit\\twill\\tprompt\\tyou\\tto\\tdo\\tso\\tif\\tit\\tneeds\\tthem,\\tin\\taddition\\tto\\tgiving\\tyou\\tinstructions\\nfor\\thow\\tto\\tdo\\tit.\\tIt’s\\ta\\tvery\\tsimple\\tcommand,\\tjust\\tnltk.download()\\tinside\\tthe\\nIPython\\tconsole.\\n\\t\\n\\t\\n\\t\\nNext,\\twe\\tjust\\tdo\\tthe\\tsame\\tthing\\tas\\twe\\tdid\\tin\\tthe\\tgenerating\\tpoetry\\texercise\\t-\\tturn\\neach\\tsequence\\tof\\tPOS\\ttags\\tinto\\ta\\tsequence\\tof\\tindexes\\tthat\\trepresent\\tthose\\tPOS\\ntags.\\n\\t\\n\\t\\n\\t\\nI’ve\\tadded\\tsome\\tcaching\\tability\\tsince\\tNLTK’s\\tPOS\\ttagging\\tand\\tword\\ntokenization\\tis\\textremely\\tslow,\\tand\\tyou\\thave\\tthe\\tability\\tto\\tinput\\tthe\\tnumber\\tof\\ndesired\\tsamples\\tper\\tclass.\\tAll\\tit\\tdoes\\tis\\tsave\\tthe\\tdata,\\ttargets,\\tand\\tvocabulary\\nsize\\tto\\ta\\tnumpy\\tblob.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\twe\\tdon’t\\tactually\\tneed\\tthe\\tPOS\\ttag\\tto\\tindex\\tmapping\\tsince\\twe\\tdon’t\\ncare\\tabout\\twhat\\tthe\\tactual\\tPOS\\ttags\\tare,\\twe\\tjust\\tneed\\tto\\tbe\\table\\tto\\tdifferentiate\\nthem\\tin\\torder\\tto\\tdo\\tclassification.\\n\\t\\n\\t\\n\\t\\nfrom\\tnltk\\timport\\tpos_tag,\\tword_tokenize', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f4a96dac-5920-4a36-81e6-caaf91e21df0', embedding=None, metadata={'page_label': '77', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"from\\tnltk\\timport\\tpos_tag,\\tword_tokenize\\n\\t\\ndef\\tget_tags(s):\\n\\t\\ntuples\\t=\\tpos_tag(word_tokenize(s))\\n\\t\\nreturn\\t[y\\tfor\\tx,\\ty\\tin\\ttuples]\\n\\t\\n\\t\\n\\t\\ndef\\tget_poetry_classifier_data(samples_per_class,\\tload_cached=True,\\nsave_cached=True):\\tdatafile\\t=\\t'poetry_classifier_data.npz'\\n\\t\\nif\\tload_cached\\tand\\tos.path.exists(datafile):\\tnpz\\t=\\tnp.load(datafile)\\n\\t\\nX\\t=\\tnpz['arr_0']\\n\\t\\nY\\t=\\tnpz['arr_1']\\n\\t\\nV\\t=\\tint(npz['arr_2'])\\n\\t\\nreturn\\tX,\\tY,\\tV\\n\\t\\n\\t\\n\\t\\nword2idx\\t=\\t{}\\n\\t\\ncurrent_idx\\t=\\t0\\n\\t\\nX\\t=\\t[]\\n\\t\\nY\\t=\\t[]\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c1642322-68be-4516-955d-c39d25a9b1b4', embedding=None, metadata={'page_label': '78', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nfor\\tfn,\\tlabel\\tin\\tzip(('../hmm_class/edgar_allan_poe.txt',\\n'../hmm_class/robert_frost.txt'),\\t(0,\\t1)):\\tcount\\t=\\t0\\n\\t\\nfor\\tline\\tin\\topen(fn):\\n\\t\\nline\\t=\\tline.rstrip()\\n\\t\\nif\\tline:\\n\\t\\nprint\\tline\\n\\t\\n#\\ttokens\\t=\\tremove_punctuation(line.lower()).split()\\ttokens\\t=\\tget_tags(line)\\n\\t\\nif\\tlen(tokens)\\t>\\t1:\\n\\t\\n#\\tscan\\tdoesn't\\twork\\tnice\\there,\\ttechnically\\tcould\\tfix...\\n\\t\\nfor\\ttoken\\tin\\ttokens:\\n\\t\\nif\\ttoken\\tnot\\tin\\tword2idx:\\n\\t\\nword2idx[token]\\t=\\tcurrent_idx\\n\\t\\ncurrent_idx\\t+=\\t1\\n\\t\\nsequence\\t=\\tnp.array([word2idx[w]\\tfor\\tw\\tin\\ttokens])\\tX.append(sequence)\\n\\t\\nY.append(label)\\n\\t\\ncount\\t+=\\t1\\n\\t\\nprint\\tcount\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d4c32f42-0f0c-465b-ac2b-71fd3a48b916', embedding=None, metadata={'page_label': '79', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='print\\tcount\\n\\t\\n#\\tquit\\tearly\\tbecause\\tthe\\ttokenizer\\tis\\tvery\\tslow\\tif\\tcount\\t>=\\tsamples_per_class:\\n\\t\\nbreak\\n\\t\\nif\\tsave_cached:\\n\\t\\nnp.savez(datafile,\\tX,\\tY,\\tcurrent_idx)\\treturn\\tX,\\tY,\\tcurrent_idx\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nNext\\tlet’s\\tlook\\tat\\tthe\\tclassifier\\titself.\\tIt’s\\tagain\\tgoing\\tto\\tbe\\tslightly\\tdifferent\\tthan\\nthe\\tprevious\\tRNNs\\twe\\tbuilt.\\n\\t\\n\\t\\n\\t\\nEven\\tthough\\tthis\\tis\\tsort\\tof\\ta\\tlanguage\\tmodel,\\tit’s\\tnot\\tgoing\\tto\\thave\\tany\\tword\\nembeddings\\tsince\\twe’re\\tnot\\tusing\\twords.\\tThe\\tone-hot\\tvector\\trepresenting\\tthe\\nPOS\\ttag\\tgoes\\tstraight\\tto\\tthe\\trecurrent\\tunit.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\tthis\\tmeans\\tWx\\twill\\tnow\\tbe\\tVxM\\tinstead\\tof\\tDxM,\\tthere’s\\tno\\tD\\nanymore\\tsince\\tthere\\tare\\tno\\tword\\tembeddings.\\n\\t\\n\\t\\n\\t\\nThe\\tnumber\\tof\\toutput\\tclasses\\tis\\tnow\\t2\\tinstead\\tof\\tV\\tsince\\twe’re\\tnot\\tpredicting\\ta\\nword,\\twe’re\\tpredicting\\tthe\\tpoet.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4d26e690-4058-4c91-aafa-11e73da3b445', embedding=None, metadata={'page_label': '80', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nOf\\tour\\t3\\tways\\tof\\tdoing\\tprediction\\tusing\\tRNNs\\tyou\\tshould\\trecognize\\tthis\\tas\\tthe\\nfirst\\tone\\t-\\tone\\tclass\\tper\\tsequence.\\n\\t\\n\\t\\n\\t\\nThis\\tmeans\\twe\\tonly\\tcare\\tabout\\tthe\\tclassification\\tat\\tthe\\tend\\tof\\tthe\\tsequence,\\tor\\tin\\nother\\twords,\\tafter\\twe\\thave\\tseen\\tthe\\tentire\\tsequence.\\n\\t\\n\\t\\n\\t\\nOne\\tmore\\tsmall\\tnote\\t-\\tI’m\\tgoing\\tto\\tuse\\ta\\tvariable\\tlearning\\trate\\there\\tbecause\\tI\\nnoticed\\tthe\\tcost\\tjumping\\taround\\ta\\tlot\\tin\\tthe\\tlater\\tepochs.\\n\\t\\n\\t\\n\\t\\nTo\\tdo\\tthis\\tI’m\\tjust\\tgoing\\tto\\tmake\\tthe\\tlearning\\trate\\tsmaller\\tby\\ta\\tfactor\\tof\\t0.9999\\nafter\\tevery\\tepoch.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6b00cbdc-fc49-407f-944b-b66e717d6e88', embedding=None, metadata={'page_label': '81', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nClassifying\\tPoetry\\tCode\\n\\t\\n\\t\\n\\t\\nNote:\\tThe\\trelevant\\tfile\\tin\\tthe\\tclass\\trepo\\tis\\tpoetry_classifier.py\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\tmatplotlib.pyplot\\tas\\tplt\\n\\t\\n\\t\\n\\t\\nfrom\\tsklearn.utils\\timport\\tshuffle\\n\\t\\nfrom\\tutil\\timport\\tinit_weight,\\tget_poetry_classifier_data\\n\\t\\n\\t\\n\\t\\nclass\\tSimpleRNN:\\n\\t\\ndef\\t__init__(self,\\tM,\\tV):\\n\\t\\nself.M\\t=\\tM\\t#\\thidden\\tlayer\\tsize\\n\\t\\nself.V\\t=\\tV\\t#\\tvocabulary\\tsize\\n\\t\\n\\t\\n\\t\\ndef\\tfit(self,\\tX,\\tY,\\tlearning_rate=10e-1,\\tmu=0.99,\\treg=1.0,\\tactivation=T.tanh,\\tepochs=500,\\nshow_fig=False):\\tM\\t=\\tself.M', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='17b948f9-0590-4f10-91d2-126597cb34ec', embedding=None, metadata={'page_label': '82', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='show_fig=False):\\tM\\t=\\tself.M\\n\\t\\nV\\t=\\tself.V\\n\\t\\nK\\t=\\tlen(set(Y))\\n\\t\\n\\t\\n\\t\\nX,\\tY\\t=\\tshuffle(X,\\tY)\\n\\t\\nNvalid\\t=\\t10\\n\\t\\nXvalid,\\tYvalid\\t=\\tX[-Nvalid:],\\tY[-Nvalid:]\\n\\t\\nX,\\tY\\t=\\tX[:-Nvalid],\\tY[:-Nvalid]\\n\\t\\nN\\t=\\tlen(X)\\n\\t\\n\\t\\n\\t\\n#\\tinitial\\tweights\\n\\t\\nWx\\t=\\tinit_weight(V,\\tM)\\n\\t\\nWh\\t=\\tinit_weight(M,\\tM)\\n\\t\\nbh\\t=\\tnp.zeros(M)\\n\\t\\nh0\\t=\\tnp.zeros(M)\\n\\t\\nWo\\t=\\tinit_weight(M,\\tK)\\n\\t\\nbo\\t=\\tnp.zeros(K)\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='34ac84b7-9b05-4258-981b-84b024f157f3', embedding=None, metadata={'page_label': '83', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nthX,\\tthY,\\tpy_x,\\tprediction\\t=\\tself.set(Wx,\\tWh,\\tbh,\\th0,\\tWo,\\tbo,\\tactivation)\\n\\t\\ncost\\t=\\t-T.mean(T.log(py_x[thY]))\\n\\t\\ngrads\\t=\\tT.grad(cost,\\tself.params)\\n\\t\\ndparams\\t=\\t[theano.shared(p.get_value()*0)\\tfor\\tp\\tin\\tself.params]\\n\\t\\nlr\\t=\\tT.scalar('learning_rate')\\n\\t\\n\\t\\n\\t\\nupdates\\t=\\t[\\n\\t\\n(p,\\tp\\t+\\tmu*dp\\t-\\tlr*g)\\tfor\\tp,\\tdp,\\tg\\tin\\tzip(self.params,\\tdparams,\\tgrads)\\t]\\t+\\t[\\n\\t\\n(dp,\\tmu*dp\\t-\\tlr*g)\\tfor\\tdp,\\tg\\tin\\tzip(dparams,\\tgrads)\\t]\\n\\t\\n\\t\\n\\t\\nself.train_op\\t=\\ttheano.function(\\n\\t\\ninputs=[thX,\\tthY,\\tlr],\\n\\t\\noutputs=[cost,\\tprediction],\\n\\t\\nupdates=updates,\\n\\t\\nallow_input_downcast=True,\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ncosts\\t=\\t[]\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='51fe82a3-8022-46de-abe9-e568044e3234', embedding=None, metadata={'page_label': '84', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nfor\\ti\\tin\\txrange(epochs):\\n\\t\\nX,\\tY\\t=\\tshuffle(X,\\tY)\\n\\t\\nn_correct\\t=\\t0\\n\\t\\ncost\\t=\\t0\\n\\t\\nfor\\tj\\tin\\txrange(N):\\n\\t\\nc,\\tp\\t=\\tself.train_op(X[j],\\tY[j],\\tlearning_rate)\\tcost\\t+=\\tc\\n\\t\\nif\\tp\\t==\\tY[j]:\\n\\t\\nn_correct\\t+=\\t1\\n\\t\\n#\\tupdate\\tthe\\tlearning\\trate\\n\\t\\nlearning_rate\\t*=\\t0.9999\\n\\t\\n\\t\\n\\t\\n#\\tcalculate\\tvalidation\\taccuracy\\n\\t\\nn_correct_valid\\t=\\t0\\n\\t\\nfor\\tj\\tin\\txrange(Nvalid):\\n\\t\\np\\t=\\tself.predict_op(Xvalid[j])\\n\\t\\nif\\tp\\t==\\tYvalid[j]:\\n\\t\\nn_correct_valid\\t+=\\t1\\n\\t\\nprint\\t\"i:\",\\ti,\\t\"cost:\",\\tcost,\\t\"correct\\trate:\",\\t(float(n_correct)/N),\\tprint\\t\"validation\\tcorrect\\trate:\",', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='93063236-0fdb-449f-b6f5-fc9bb5040999', embedding=None, metadata={'page_label': '85', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='print\\t\"i:\",\\ti,\\t\"cost:\",\\tcost,\\t\"correct\\trate:\",\\t(float(n_correct)/N),\\tprint\\t\"validation\\tcorrect\\trate:\",\\n(float(n_correct_valid)/Nvalid)\\tcosts.append(cost)\\n\\t\\n\\t\\n\\t\\nif\\tshow_fig:\\n\\t\\nplt.plot(costs)\\n\\t\\nplt.show()\\n\\t\\n\\t\\n\\t\\ndef\\tsave(self,\\tfilename):\\n\\t\\nnp.savez(filename,\\t*[p.get_value()\\tfor\\tp\\tin\\tself.params])\\n\\t\\n@staticmethod\\n\\t\\ndef\\tload(filename,\\tactivation):\\n\\t\\n#\\tTODO:\\twould\\tprefer\\tto\\tsave\\tactivation\\tto\\tfile\\ttoo\\tnpz\\t=\\tnp.load(filename)\\n\\t\\nWx\\t=\\tnpz[\\'arr_0\\']\\n\\t\\nWh\\t=\\tnpz[\\'arr_1\\']\\n\\t\\nbh\\t=\\tnpz[\\'arr_2\\']\\n\\t\\nh0\\t=\\tnpz[\\'arr_3\\']\\n\\t\\nWo\\t=\\tnpz[\\'arr_4\\']\\n\\t\\nbo\\t=\\tnpz[\\'arr_5\\']\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ebd93a42-314e-46c4-a25a-47cc0b3e1052', embedding=None, metadata={'page_label': '86', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nV,\\tM\\t=\\tWx.shape\\n\\t\\nrnn\\t=\\tSimpleRNN(M,\\tV)\\n\\t\\nrnn.set(Wx,\\tWh,\\tbh,\\th0,\\tWo,\\tbo,\\tactivation)\\treturn\\trnn\\n\\t\\n\\t\\n\\t\\ndef\\tset(self,\\tWx,\\tWh,\\tbh,\\th0,\\tWo,\\tbo,\\tactivation):\\tself.f\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tredundant\\t-\\tsee\\thow\\tyou\\tcan\\timprove\\tit\\tself.Wx\\t=\\ttheano.shared(Wx)\\n\\t\\nself.Wh\\t=\\ttheano.shared(Wh)\\n\\t\\nself.bh\\t=\\ttheano.shared(bh)\\n\\t\\nself.h0\\t=\\ttheano.shared(h0)\\n\\t\\nself.Wo\\t=\\ttheano.shared(Wo)\\n\\t\\nself.bo\\t=\\ttheano.shared(bo)\\n\\t\\nself.params\\t=\\t[self.Wx,\\tself.Wh,\\tself.bh,\\tself.h0,\\tself.Wo,\\tself.bo]\\n\\t\\n\\t\\n\\t\\nthX\\t=\\tT.ivector('X')\\n\\t\\nthY\\t=\\tT.iscalar('Y')\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(x_t,\\th_t1):\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b153094d-2912-4d62-bf3a-41cea6538ad5', embedding=None, metadata={'page_label': '87', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n#\\treturns\\th(t),\\ty(t)\\n\\t\\nh_t\\t=\\tself.f(self.Wx[x_t]\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\ty_t\\t=\\tT.nnet.softmax(h_t.dot(self.Wo)\\t+\\tself.bo)\\nreturn\\th_t,\\ty_t\\n\\t\\n\\t\\n\\t\\n[h,\\ty],\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=recurrence,\\n\\t\\noutputs_info=[self.h0,\\tNone],\\n\\t\\nsequences=thX,\\n\\t\\nn_steps=thX.shape[0],\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\npy_x\\t=\\ty[-1,\\t0,\\t:]\\t#\\tonly\\tinterested\\tin\\tthe\\tfinal\\tclassification\\tof\\tthe\\tsequence\\tprediction\\t=\\tT.argmax(py_x)\\n\\t\\nself.predict_op\\t=\\ttheano.function(\\n\\t\\ninputs=[thX],\\n\\t\\noutputs=prediction,\\n\\t\\nallow_input_downcast=True,\\n\\t\\n)\\n\\t\\nreturn\\tthX,\\tthY,\\tpy_x,\\tprediction\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f00a2186-f7f8-46e5-9f75-0e43bf7daed9', embedding=None, metadata={'page_label': '88', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\n\\t\\n\\t\\n\\t\\n\\t\\ndef\\ttrain_poetry():\\n\\t\\nX,\\tY,\\tV\\t=\\tget_poetry_classifier_data(samples_per_class=500)\\trnn\\t=\\tSimpleRNN(30,\\tV)\\n\\t\\nrnn.fit(X,\\tY,\\tlearning_rate=10e-7,\\tshow_fig=True,\\tactivation=T.nnet.relu,\\tepochs=1000)\\n\\t\\nif\\t__name__\\t==\\t'__main__':\\n\\t\\ntrain_poetry()\\n\\t\\n\\t\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5b2feb01-b52f-4a8e-b453-ec1ad9fc29f5', embedding=None, metadata={'page_label': '89', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t5:\\tAdvanced\\tRNN\\tUnits\\t-\\tGRU\\tand\\tLSTM\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='440a40e5-9882-4214-aeee-e64a6f497113', embedding=None, metadata={'page_label': '90', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nRated\\tRecurrent\\tUnit\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\tI’m\\tgoing\\tto\\tshow\\tyou\\ta\\tvery\\tsimple\\tmodification\\tto\\tthe\\tsimple\\nrecurrent\\tunit.\\tIt\\twill\\thelp\\tus\\tbridge\\tthe\\tgap\\tto\\tGRU\\tand\\tLSTM.\\n\\t\\n\\t\\n\\t\\nThe\\tidea\\tis\\twe\\twant\\tto\\tweight\\t2\\tthings:\\n\\t\\n1)\\tf(x,\\th(t-1)),\\twhich\\twas\\tthe\\toutput\\twe\\twould’ve\\tgotten\\tin\\ta\\tsimple\\trecurrent\\nunit,\\tand\\n\\t\\n2)\\th(t-1),\\tthe\\tprevious\\tvalue\\tof\\tthe\\tstate.\\n\\t\\n\\t\\n\\t\\nh_hat(t)\\t=\\tf(x(t)W\\nx\\n\\t+\\th(t-1)W\\nh\\n\\t+\\tb\\nh\\n)\\n\\t\\nz(t)\\t=\\tsigmoid(x(t)W\\nxz\\n\\t+\\th(t-1)W\\nhz\\n\\t+\\tb\\nz\\n)\\n\\t\\nh(t)\\t=\\t(1\\t-\\tz(t))\\t\\n\\th(t-1)\\t+\\tz(t)\\t\\n\\th_hat(t)\\n\\t\\n\\t\\n\\t\\nz(t)\\tis\\tknown\\tas\\tthe\\trate.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='99291218-b28d-44eb-8b8b-89e3bbcdf4e9', embedding=None, metadata={'page_label': '91', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You\\tcan\\tinfer\\tthe\\tsizes\\tof\\tthe\\tnew\\tweights\\tby\\tthe\\tequations\\t(we\\tuse\\t*\\tto\\nrepresent\\telement-wise\\tmultiplication).\\n\\t\\n\\t\\n\\t\\nWxz\\tis\\tDxM\\n\\t\\nWhz\\tis\\tMxM\\n\\t\\nbz\\tis\\tsize\\tM\\n\\t\\n\\t\\n\\t\\nNotice\\thow\\tthis\\tlooks\\ta\\tlot\\tlike\\tthe\\tlow-pass\\tfilter\\twe\\tbuilt\\tin\\tthe\\tTheano\\tscan\\ntutorial\\t(free\\ton\\tYouTube,\\tbut\\talso\\tincluded\\tin\\tmy\\tcourse)\\t-\\tso\\tthat\\tshould\\tgive\\nyou\\tsome\\tintuition\\tabout\\tthe\\teffect\\tof\\tthis\\tunit.\\n\\t\\n\\t\\n\\t\\nIf\\tyou’ve\\tbeen\\tfollowing\\talong\\twith\\tthe\\tcode\\tyou\\tcan\\tprobably\\timagine\\tthat\\tthis\\nis\\ta\\tvery\\tsimple\\tchange\\t-\\tjust\\tadd\\tthe\\tnew\\tparams\\tand\\tinclude\\tthe\\tabove\\nequation\\tin\\tthe\\trecurrence\\tfunction.\\n\\t\\n\\t\\n\\t\\nRRNN\\tin\\tCode\\t(only\\tthe\\trecurrence\\tfunction\\tneeds\\tto\\tchange\\t+\\tobviously\\tthe\\nweights\\tneed\\tto\\tbe\\tinitialized):\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(x_t,\\th_t1):\\n\\t\\n#\\treturns\\th(t),\\ty(t)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7399e596-51c3-412f-acd2-766b39dae429', embedding=None, metadata={'page_label': '92', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='#\\treturns\\th(t),\\ty(t)\\n\\t\\nhhat_t\\t=\\tself.f(x_t.dot(self.Wx)\\t+\\th_t1.dot(self.Wh)\\t+\\tself.bh)\\n\\t\\nz_t\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxz)\\t+\\th_t1.dot(self.Whz)\\t+\\tself.bz)\\n\\t\\nh_t\\t=\\t(1\\t-\\tz_t)\\t*\\th_t1\\t+\\tz_t\\t*\\thhat_t\\n\\t\\ny_t\\t=\\tT.nnet.softmax(h_t.dot(self.Wo)\\t+\\tself.bo)\\n\\t\\nreturn\\th_t,\\ty_t\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b2b2c885-3839-488b-9043-78ccbf318c8a', embedding=None, metadata={'page_label': '93', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nGated\\tRecurrent\\tUnit\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\ttalk\\tabout\\ta\\tmore\\tpowerful\\trecurrent\\tunit\\tthan\\tthe\\nrated\\trecurrent\\tunit.\\tIt’s\\tcalled\\tthe\\tGRU\\tor\\tgated\\trecurrent\\tunit.\\n\\t\\n\\t\\n\\t\\nGated\\trecurrent\\tunits\\twere\\tintroduced\\tin\\t2014,\\twhereas\\tLSTMs\\twere\\tintroduced\\nin\\t1997.\\n\\t\\n\\t\\n\\t\\nUsually\\twhen\\tyou\\tlearn\\tabout\\trecurrent\\tnetworks\\tyou\\tlearn\\tabout\\tthe\\tLSTM\\tfirst\\n-\\twhich\\tcould\\tbe\\tdue\\tto\\tthe\\tfact\\tthat\\tit’s\\tway\\tmore\\tpopular,\\tor\\tpossibly\\tbecause\\nit\\twas\\tinvented\\tfirst.\\n\\t\\n\\t\\n\\t\\nYou\\tcan\\tthink\\tof\\tthe\\tGRU\\tas\\ta\\tsimpler\\tversion\\tof\\tthe\\tLSTM.\\tIt\\tincorporates\\ta\\nlot\\tof\\tthe\\tsame\\tconcepts,\\tbut\\tit\\thas\\ta\\tmuch\\tsmaller\\tnumber\\tof\\tparameters,\\tand\\tso\\nit\\tcan\\ttrain\\tfaster\\tat\\ta\\tconstant\\thidden\\tlayer\\tsize.\\tAt\\tthe\\tsame\\ttime,\\tit’s\\tmore\\ncomplex\\tthan\\tthe\\trated\\trecurrent\\tunit,\\twhich\\twe\\tjust\\tdiscussed.\\n\\t\\n\\t\\n\\t\\nSo\\tthese\\tlectures,\\tinstead\\tof\\tintroducing\\tthe\\tLSTM\\tunit\\tfirst,\\tjust\\tgo\\tin\\torder\\tof\\ncomplexity.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0eed4428-dcc6-483e-912e-e2a2065730c9', embedding=None, metadata={'page_label': '94', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nRecent\\tresearch\\thas\\talso\\tshown\\tthat\\tthe\\taccuracy\\tbetween\\tthe\\tLSTM\\tand\\tGRU\\nis\\tcomparable\\tand\\teven\\tbetter\\twith\\tthe\\tGRU\\tin\\tsome\\tcases.\\n\\t\\n\\t\\n\\t\\nSo\\tthere\\tisn’t\\tany\\thard\\trule\\tthat\\tyou\\tshould\\tchoose\\tone\\tover\\tthe\\tother.\\tIt’s\\tjust\\nlike\\thow\\tyou\\twould\\tchoose\\tthe\\tbest\\tnonlinearity\\tfor\\ta\\tregular\\tneural\\tnetwork\\t-\\nyou\\tjust\\thave\\tto\\ttry\\tand\\tsee\\twhat\\tworks\\tbetter\\tfor\\tyour\\tparticular\\tdata.\\n\\t\\n\\t\\n\\t\\nLet’s\\tdescribe\\tthe\\tarchitecture\\tof\\tthe\\tGRU.\\n\\t\\n\\t\\n\\t\\nThe\\tfirst\\tthing\\twe\\twant\\tto\\tdo\\tis\\ttake\\ta\\tcompartmental\\tpoint\\tof\\tview.\\n\\t\\n\\t\\n\\t\\nThink\\tof\\teverything\\tbetween\\tthe\\tprevious\\tlayer\\tand\\tthe\\tnext\\tlayer\\tas\\ta\\tblack\\nbox.\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tsimplest\\tfeedforward\\tneural\\tnetwork,\\tthis\\tblack\\tbox\\tjust\\tcontains\\tsome\\nnonlinear\\tfunction\\tlike\\ttanh\\tor\\trelu.\\n\\t\\n\\t\\n\\t\\nIn\\ta\\tsimple\\trecurrent\\tnetwork,\\twe\\tjust\\tconnect\\tthe\\toutput\\tof\\tthe\\tblack\\tbox\\tback\\nto\\titself,\\twith\\ta\\ttime\\tdelay\\tof\\tone.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6062d3a0-05de-445e-8555-0fa09cf38589', embedding=None, metadata={'page_label': '95', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\trated\\trecurrent\\tnetwork,\\twe\\tadd\\ta\\trating\\toperation\\tbetween\\twhat\\twould’ve\\nbeen\\tthe\\toutput\\tof\\tthe\\tsimple\\trecurrent\\tnetwork,\\tand\\tthe\\tprevious\\toutput\\tvalue.\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nYou\\tcan\\tthink\\tof\\tthis\\tnew\\toperation\\tas\\ta\\tgate.\\tSince\\tit\\thas\\tto\\ttake\\ton\\ta\\tvalue\\nbetween\\t0\\tand\\t1,\\tand\\tthe\\tother\\tgate\\thas\\tto\\ttake\\ton\\t1\\tminus\\tthat\\tvalue,\\tit’s\\ta\\tgate\\nthat\\tis\\tchoosing\\tbetween\\t2\\tthings\\t-\\ttaking\\ton\\tthe\\told\\tvalue,\\tor\\ttaking\\ton\\tthe\\tnew', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9a1c6f3d-abdc-4e98-a961-557ffaf8f61f', embedding=None, metadata={'page_label': '96', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='value.\\n\\t\\n\\t\\n\\t\\nThe\\tresult\\tis\\twe\\tget\\ta\\tmixture\\tof\\tboth.\\n\\t\\n\\t\\n\\t\\nNow\\tlet’s\\tgo\\tto\\tthe\\tgated\\trecurrent\\tunit\\tor\\tGRU.\\tThe\\tarchitecture\\tsimply\\nrequires\\tthat\\twe\\tadd\\tone\\tmore\\tgate.\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nFor\\tsome\\tit’s\\teasier\\tto\\tunderstand\\twith\\ta\\tpicture,\\tfor\\tothers\\tit’s\\teasier\\tto\\nunderstand\\twith\\ta\\tformula.\\tNotice\\tthere\\tis\\tno\\tnew\\tmath\\there,\\tjust\\tmore\\tof\\tthe\\nsame\\tstuff\\twe\\talready\\tknow\\tabout\\t-\\tweight\\tmatrices\\tmultiplied\\tby\\tinputs\\tand\\npassed\\tthrough\\tnonlinear\\tfunctions\\tand\\tgates.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d1e1dafc-32de-4258-b7b8-ebecc090e94c', embedding=None, metadata={'page_label': '97', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nNote\\tthat\\tthe\\tcircle\\twith\\tthe\\tdot\\tin\\tit\\tis\\tanother\\tway\\tof\\trepresenting\\telement-wise\\nmultiplication.\\n\\t\\n\\t\\n\\t\\nHere\\twe\\tsee\\tagain\\tthe\\t“update\\tgate”\\tz(t)\\tthat\\twe\\tsaw\\tin\\tthe\\trated\\trecurrent\\tunit.\\nIt\\tstill\\tbalances\\thow\\tmuch\\tof\\tthe\\tprevious\\thidden\\tvalue\\tand\\thow\\tmuch\\tof\\tthe\\nnew\\tcandidate\\thidden\\tvalue\\tcombines\\tto\\tget\\tthe\\tnew\\thidden\\tvalue.\\n\\t\\n\\t\\n\\t\\nThe\\textra\\tthing\\there\\tis\\tr(t),\\tor\\tthe\\t“reset\\tgate”.\\tIt\\thas\\tthe\\texact\\tsame\\tfunctional\\nform\\tas\\tthe\\t“update\\tgate”,\\tand\\tall\\tof\\tits\\tweights\\tare\\tthe\\tsame\\tsize,\\tbut\\tits\\nposition\\tin\\tthe\\tblack\\tbox\\tis\\tdifferent.\\n\\t\\n\\t\\n\\t\\nThe\\t“reset\\tgate”\\tis\\tmultiplied\\tby\\tthe\\tprevious\\thidden\\tstate\\tvalue\\t-\\tit\\tcontrols\\thow\\nmuch\\tof\\tthe\\tprevious\\thidden\\tstate\\twe\\twill\\tconsider\\twhen\\twe\\tcreate\\tthe\\tnew\\ncandidate\\thidden\\tvalue.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='09de5cfa-5891-43db-9299-8d6f5a50adf5', embedding=None, metadata={'page_label': '98', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nIn\\tother\\twords,\\tit\\thas\\tthe\\tability\\tto\\t“reset”\\tthe\\thidden\\tvalue.\\n\\t\\n\\t\\n\\t\\nIf\\tr(t)\\t=\\t0,\\tthen\\twe\\tget\\th_hat(t)\\t=\\tf(x(t)\\tWxh\\t+\\tbh),\\twhich\\twould\\tbe\\tas\\tif\\tx(t)\\nwere\\tthe\\tbeginning\\tof\\ta\\tnew\\tsequence.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\tthis\\tis\\tnot\\tthe\\tfull\\tpicture\\tsince\\th_hat(t)\\tis\\tonly\\ta\\tcandidate\\tfor\\tthe\\tnew\\nh(t),\\tsince\\th(t)\\twill\\tbe\\ta\\tcombination\\tof\\th_hat(t)\\tand\\th(t-1),\\tcontrolled\\tby\\tthe\\n“update\\tgate”\\tz(t).\\n\\t\\n\\t\\n\\t\\nIf\\tthis\\tall\\tsounds\\tvery\\tcomplicated,\\tanother\\tway\\tto\\tthink\\tof\\tthis\\tis\\tmore\\npragmatically.\\tWhat’s\\thappening\\tis\\twe’re\\tsimply\\tadding\\tmore\\tparameters\\tto\\tour\\nmodel\\tand\\tmaking\\tit\\tmore\\texpressive.\\n\\t\\n\\t\\n\\t\\nAdding\\tmore\\tparameters\\tallows\\tus\\tto\\tfit\\tmore\\tcomplex\\tpatterns.\\n\\t\\n\\t\\n\\t\\nIn\\tterms\\tof\\tcode,\\twriting\\ta\\tGRU\\tshould\\tbe\\trelatively\\ttrivial\\tif\\tyou\\talready\\nunderstand\\tthe\\tsimple\\trecurrent\\tunit\\tand\\tthe\\trated\\trecurrent\\tunit.\\tWe\\tjust\\thave\\tto\\nadd\\tmore\\tweights\\tand\\tmodify\\tthe\\trecurrence.\\n\\t\\n\\t\\n\\t\\nThe\\tnext\\tstep\\tto\\tmaking\\tour\\tcode\\tbetter\\tis\\tto\\tmodularize\\tit.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='58c4319d-71de-4cab-b8ae-297931ebd4a1', embedding=None, metadata={'page_label': '99', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nI\\ttalked\\tabout\\tthe\\tGRU\\tbeing\\ta\\tblack\\tbox.\\n\\t\\n\\t\\n\\t\\nIn\\tmy\\tprevious\\tclasses,\\twe\\tput\\tthe\\thidden\\tlayer\\tand\\tconvolutional\\tpooling\\tlayer\\nin\\ta\\tclass,\\tso\\tthat\\twe\\tcould\\tre-use\\tthem\\tand\\tstack\\tthem.\\n\\t\\n\\t\\n\\t\\nWith\\tthe\\tGRU\\twe\\twill\\tdo\\tthe\\tsame\\t-\\tmake\\tit\\tinto\\ta\\tclass,\\tso\\tthat\\tit\\tcan\\tbe\\n“abstracted\\taway”.\\n\\t\\n\\t\\n\\t\\nBy\\tdoing\\tthis,\\tyou’ll\\tbe\\table\\tto\\tstack\\tGRUs,\\tand\\tanywhere\\tyou\\tcould\\thave\\tput\\ta\\nvanilla\\thidden\\tlayer\\tyou\\tcan\\tnow\\tinsert\\ta\\tGRU.\\n\\t\\n\\t\\n\\t\\nIt’s\\tjust\\ta\\t“thing”\\tthat\\ttakes\\tan\\tinput\\tand\\tproduces\\tan\\toutput\\t-\\tthe\\tfact\\tthat\\tit\\ncontains\\ta\\tmemory\\tof\\tprevious\\tinputs\\tis\\tjust\\tan\\tinternal\\tdetail\\tof\\tthe\\tblack\\tbox.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9301838b-4a3d-4740-beca-0d570a5b2f1c', embedding=None, metadata={'page_label': '100', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nGRU\\tin\\tCode\\n\\t\\n\\t\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\n\\t\\n\\t\\nfrom\\tutil\\timport\\tinit_weight\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nclass\\tGRU:\\n\\t\\ndef\\t__init__(self,\\tMi,\\tMo,\\tactivation):\\n\\t\\nself.Mi\\t=\\tMi\\n\\t\\nself.Mo\\t=\\tMo\\n\\t\\nself.f\\t\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tnumpy\\tinit\\n\\t\\nWxr\\t=\\tinit_weight(Mi,\\tMo)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='89399088-0cce-4e2a-a407-fecd2a4ccbd8', embedding=None, metadata={'page_label': '101', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nWhr\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbr\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nWxz\\t=\\tinit_weight(Mi,\\tMo)\\n\\t\\nWhz\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbz\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nWxh\\t=\\tinit_weight(Mi,\\tMo)\\n\\t\\nWhh\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbh\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nh0\\t\\t=\\tnp.zeros(Mo)\\n\\t\\n\\t\\n\\t\\n#\\ttheano\\tvars\\n\\t\\nself.Wxr\\t=\\ttheano.shared(Wxr)\\n\\t\\nself.Whr\\t=\\ttheano.shared(Whr)\\n\\t\\nself.br\\t\\t=\\ttheano.shared(br)\\n\\t\\nself.Wxz\\t=\\ttheano.shared(Wxz)\\n\\t\\nself.Whz\\t=\\ttheano.shared(Whz)\\n\\t\\nself.bz\\t\\t=\\ttheano.shared(bz)\\n\\t\\nself.Wxh\\t=\\ttheano.shared(Wxh)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0def0c1e-0623-4394-9f96-0752c10d0832', embedding=None, metadata={'page_label': '102', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nself.Whh\\t=\\ttheano.shared(Whh)\\n\\t\\nself.bh\\t\\t=\\ttheano.shared(bh)\\n\\t\\nself.h0\\t\\t=\\ttheano.shared(h0)\\n\\t\\nself.params\\t=\\t[self.Wxr,\\tself.Whr,\\tself.br,\\tself.Wxz,\\tself.Whz,\\tself.bz,\\tself.Wxh,\\tself.Whh,\\tself.bh,\\tself.h0]\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(self,\\tx_t,\\th_t1):\\n\\t\\nr\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxr)\\t+\\th_t1.dot(self.Whr)\\t+\\tself.br)\\n\\t\\nz\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxz)\\t+\\th_t1.dot(self.Whz)\\t+\\tself.bz)\\n\\t\\nhhat\\t=\\tself.f(x_t.dot(self.Wxh)\\t+\\t(r\\t*\\th_t1).dot(self.Whh)\\t+\\tself.bh)\\n\\t\\nh\\t=\\t(1\\t-\\tz)\\t\\n\\th_t1\\t+\\tz\\t\\n\\thhat\\n\\t\\nreturn\\th\\n\\t\\n\\t\\n\\t\\ndef\\toutput(self,\\tx):\\n\\t\\n#\\tinput\\tX\\tshould\\tbe\\ta\\tmatrix\\t(2-D)\\n\\t\\n#\\trows\\tindex\\ttime\\n\\t\\nh,\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=self.recurrence,\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8a9d1b56-0ee6-46f8-98f5-e39568f75ef4', embedding=None, metadata={'page_label': '103', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='sequences=x,\\n\\t\\noutputs_info=[self.h0],\\n\\t\\nn_steps=x.shape[0],\\n\\t\\n)\\n\\t\\nreturn\\th\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5a04d108-9025-4ba7-90b0-eabcee133a53', embedding=None, metadata={'page_label': '104', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nLSTM\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tsection\\twe\\tare\\tgoing\\tto\\tgo\\tfrom\\tGRU\\tto\\tLSTM.\\tLSTM\\tstands\\tfor\\t“long\\nshort-term\\tmemory”\\tand\\tit’s\\ta\\ttype\\tof\\trecurrent\\tunit\\tthat\\thas\\tbecome\\tvery\\npopular\\tin\\trecent\\tyears\\tdue\\tto\\tits\\tsuperior\\tperformance\\tand\\tthe\\tfact\\tthat\\tit\\ndoesn’t\\tas\\teasily\\tsuccumb\\tto\\tthe\\tvanishing\\tgradient\\tproblem.\\tYou’ll\\tsee\\tthat\\tall\\nthe\\trecurrent\\tunits\\tyou’ve\\tlearned\\tabout\\tup\\tuntil\\tnow\\tgot\\tprogressively\\tmore\\ncomplex,\\tand\\teach\\tincorporated\\tconcepts\\tfrom\\tthe\\tprevious\\tunit.\\n\\t\\n\\t\\n\\t\\nThe\\tLSTM\\tis\\tthe\\tmost\\tcomplex\\trecurrent\\tunit\\tyou’ll\\tlearn\\tabout\\tin\\tthis\\tbook\\t-\\nhowever\\t-\\tthat\\t“complexity”\\tis\\tnot\\tin\\tthe\\tway\\tof\\ta\\thard-to-understand\\tidea\\tor\\nhard\\tmath,\\tit’s\\tjust\\tthat\\tthe\\tnumber\\tof\\tcomponents\\tis\\thigher.\\n\\t\\n\\t\\n\\t\\nThe\\tgist\\tof\\tit\\tis:\\twe\\twill\\tnow\\thave\\t3\\tdifferent\\tgates,\\tand\\twe’re\\tgoing\\tto\\tadd\\tyet\\nanother\\t“internal\\tunit”,\\tthat\\twill\\texist\\talongside\\tthe\\thidden\\tstate,\\twhich\\tis\\nsometimes\\tcalled\\ta\\t“memory\\tcell”\\tor\\tjust\\ta\\t“cell”.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ab23f9bb-6dd0-4c9f-96dd-c90064b2d856', embedding=None, metadata={'page_label': '105', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nThe\\t3\\tgates\\tare\\tcalled\\tthe\\tinput\\tgate,\\tthe\\toutput\\tgate,\\tand\\tthe\\tforget\\tgate.\\n\\t\\n\\t\\n\\t\\nAgain,\\tsome\\tmay\\tfind\\tlooking\\tat\\tthe\\tformulas\\teasier\\tto\\tunderstand:\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nOne\\tway\\tto\\tthink\\tof\\tthe\\tcell\\tis\\tthat\\tit\\ttakes\\tthe\\tplace\\tof\\twhat\\twe\\tcalled\\t“h_hat”\\nor\\tthe\\t“candidate\\thidden\\tvalue”\\twhen\\twe\\ttalked\\tabout\\tthe\\tGRU.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5daea9c6-dedf-44e4-ae8a-7941387a47b5', embedding=None, metadata={'page_label': '106', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nThe\\tinput\\tgate\\tand\\tforget\\tgate\\tshould\\tremind\\tyou\\tof\\tthe\\t“rate\\tgate”\\tor\\t“update\\ngate”\\tfrom\\tthe\\tGRU.\\n\\t\\n\\t\\n\\t\\nBefore\\twe\\tused\\tz(t)\\tand\\t1\\t-\\tz(t)\\tbut\\tnow\\twe\\tjust\\thave\\t2\\tseparate\\tgates.\\n\\t\\n\\t\\n\\t\\nThe\\tinput\\tgate\\tcontrols\\thow\\tmuch\\tof\\tthe\\tnew\\tvalue\\tgoes\\tinto\\tthe\\tcell,\\tand\\tthe\\nforget\\tgate\\tcontrols\\thow\\tmuch\\tof\\tthe\\tprevious\\tcell\\tvalue\\tgoes\\tinto\\tthe\\tcurrent\\ncell\\tvalue.\\n\\t\\n\\t\\n\\t\\nThe\\tcandidate\\tfor\\tthe\\tnew\\tcell\\tvalue\\tlooks\\ta\\tlot\\tlike\\twhat\\twould\\tbe\\tthe\\tsimple\\nrecurrent\\tunit’s\\tvalue,\\tright\\tbefore\\tit\\tgets\\tmultiplied\\tby\\tthe\\tinput\\tgate.\\n\\t\\n\\t\\n\\t\\nFinally,\\tthe\\toutput\\tgate\\ttakes\\tinto\\taccount\\teverything\\t-\\tthe\\tinput\\tat\\ttime\\tt,\\tthe\\nprevious\\thidden\\tstate,\\tand\\tthe\\tcurrent\\tcell\\tvalue.\\n\\t\\n\\t\\n\\t\\nThe\\tnew\\thidden\\tstate\\tis\\tjust\\tthe\\ttanh\\tof\\tthe\\tcell\\tvalue\\tmultiplied\\tby\\tthe\\toutput\\ngate.\\n\\t\\n\\t\\n\\t\\nIt\\tmight\\tbe\\tuseful\\tto\\tlook\\tat\\tall\\tthe\\tparameters\\tof\\tthis\\tmodel,\\tbecause\\tthere\\tare\\nmany.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f6e02568-34b3-4cbd-93a6-5c5380f60939', embedding=None, metadata={'page_label': '107', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nIt\\twill\\thelps\\tus\\torganize\\tthem\\tso\\tthat\\tit\\tdoesn’t\\tseem\\tso\\tcomplex.\\n\\t\\n\\t\\n\\t\\nFor\\tthe\\tinput\\tgate,\\twe\\thave\\tWxi,\\tWhi,\\tWci,\\tand\\tbi,\\twhich\\tmap\\tx(t),\\th(t-1)\\tand\\nc(t-1)\\tto\\tthe\\tinput\\tgate.\\n\\t\\n\\t\\n\\t\\nFor\\tthe\\tforget\\tgate,\\twe\\thave\\tmatching\\tparameters,\\tso\\twe\\thave\\tWxf,\\tWhf,\\tWcf,\\nand\\tbf,\\twhich\\tagain\\tmap\\tx(t),\\th(t-1),\\tand\\tc(t-1)\\tto\\tthe\\tforget\\tgate.\\n\\t\\n\\t\\n\\t\\nNext,\\tto\\tcalculate\\tthe\\tnew\\tcell\\tvalue,\\twe\\thave\\tWxc,\\tWhc,\\tand\\tbc,\\twhich\\tmap\\nx(t)\\tand\\th(t-1)\\tto\\tthe\\tnew\\tcandidate\\tcell\\tvalue.\\n\\t\\n\\t\\n\\t\\nFinally\\twe\\thave\\tthe\\toutput\\tgate,\\twhich\\tadds\\t4\\tmore\\tparameters,\\tWxo,\\tWho,\\nWco,\\tand\\tbo,\\twhich\\tmap\\tx(t),\\th(t-1),\\tand\\tc(t)\\tto\\tthe\\toutput\\tgate.\\n\\t\\n\\t\\n\\t\\nNotice\\tthat\\tthe\\toutput\\tgate\\tis\\tdifferent\\tfrom\\tthe\\tinput\\tand\\tforget\\tgates\\tin\\tthat\\tit\\ndepends\\ton\\tthe\\tcell\\tvalue\\tat\\ttime\\tt,\\tnot\\tt-1.\\n\\t\\n\\t\\n\\t\\nIn\\ttotal,\\tthat’s\\t15\\tnew\\tweights\\tand\\tbiases\\tfor\\tthis\\tblack\\tbox.\\tWe’ve\\tcome\\ta\\tlong', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='95440459-513c-416c-b8bc-840dbde83935', embedding=None, metadata={'page_label': '108', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='In\\ttotal,\\tthat’s\\t15\\tnew\\tweights\\tand\\tbiases\\tfor\\tthis\\tblack\\tbox.\\tWe’ve\\tcome\\ta\\tlong\\nway\\tfrom\\tthe\\tfeedforward\\tneural\\tnetwork,\\twhich\\thas\\tjust\\t1\\tweight\\tand\\t1\\tbias.\\n\\t\\n\\t\\n\\t\\nIf\\tyou\\tfind\\tthe\\tcomplexity\\toverwhelming,\\tI’ll\\tmention\\tagain\\tthat\\tanother\\tway\\tto\\nthink\\tof\\tthis\\tis\\tthat\\twe’re\\tsimply\\tjust\\tadding\\tmore\\tparameters,\\twhich\\tis\\tmaking\\nthe\\tmodel\\tmore\\texpressive.\\n\\t\\n\\t\\n\\t\\nSimilar\\tto\\tthe\\tGRU,\\twhat\\twe’re\\tgoing\\tto\\tdo\\tin\\tthe\\tcode\\tis\\tcreate\\ta\\tclass\\tand\\nmake\\tthe\\tLSTM\\tmodular.\\tI’ll\\tshow\\tyou\\tthat\\tsince\\tthey\\tare\\tboth\\tblack\\tboxes,\\nthey\\tcan\\tbe\\tplugged\\tin\\tand\\tout\\tinterchangeably\\tand\\tused\\tfor\\tthe\\tsame\\tproblems.\\n\\t\\n\\t\\n\\t\\nAt\\tthis\\tpoint,\\tyou\\tshould\\tbe\\tVERY\\tfamiliar\\twith\\thow\\tto\\tcreate\\tthese\\trecurrent\\nunits.\\tWe\\tare\\tagain\\tjust\\tadding\\tsome\\tmore\\tvariables\\tand\\tupdating\\tthe\\trecurrence\\nfunction.\\tIn\\tfact\\tyou\\tshould\\tbe\\table\\tto\\ttake\\tthe\\tformulas\\tI\\tpresented\\tand\\nimplement\\tthe\\tLSTM\\tclass\\tyourself.\\n\\t\\n\\t\\n\\t\\nI\\twould\\thighly\\trecommend\\tdoing\\tso\\tbefore\\tmoving\\ton\\tto\\tthe\\tcode.\\n\\t\\n\\t\\n\\t\\nIn\\tthe\\tnext\\tchapter\\twe\\twill\\tbuild\\ta\\tnew\\tmodel\\tfor\\tWikipedia\\tdata,\\ttreating\\tthe\\nGRU\\tand\\tLSTM\\tas\\t“plug\\tand\\tplay”\\tcomponents.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='67ba9d15-ce28-433a-9f5f-7050e9688526', embedding=None, metadata={'page_label': '109', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nLSTM\\tin\\tCode\\n\\t\\n\\t\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\n\\t\\n\\t\\nfrom\\tutil\\timport\\tinit_weight\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\nclass\\tLSTM:\\n\\t\\ndef\\t__init__(self,\\tMi,\\tMo,\\tactivation):\\n\\t\\nself.Mi\\t=\\tMi\\n\\t\\nself.Mo\\t=\\tMo\\n\\t\\nself.f\\t\\t=\\tactivation\\n\\t\\n\\t\\n\\t\\n#\\tnumpy\\tinit\\n\\t\\nWxi\\t=\\tinit_weight(Mi,\\tMo)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c01895cc-db69-4cec-acb0-f8a92e4ea749', embedding=None, metadata={'page_label': '110', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nWhi\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nWci\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbi\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nWxf\\t=\\tinit_weight(Mi,\\tMo)\\n\\t\\nWhf\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nWcf\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbf\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nWxc\\t=\\tinit_weight(Mi,\\tMo)\\n\\t\\nWhc\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbc\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nWxo\\t=\\tinit_weight(Mi,\\tMo)\\n\\t\\nWho\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nWco\\t=\\tinit_weight(Mo,\\tMo)\\n\\t\\nbo\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nc0\\t\\t=\\tnp.zeros(Mo)\\n\\t\\nh0\\t\\t=\\tnp.zeros(Mo)\\n\\t\\n\\t\\n\\t\\n#\\ttheano\\tvars', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='00fdfb5c-3675-4bc2-9771-9462f8ecf6a0', embedding=None, metadata={'page_label': '111', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nself.Wxi\\t=\\ttheano.shared(Wxi)\\n\\t\\nself.Whi\\t=\\ttheano.shared(Whi)\\n\\t\\nself.Wci\\t=\\ttheano.shared(Wci)\\n\\t\\nself.bi\\t\\t=\\ttheano.shared(bi)\\n\\t\\nself.Wxf\\t=\\ttheano.shared(Wxf)\\n\\t\\nself.Whf\\t=\\ttheano.shared(Whf)\\n\\t\\nself.Wcf\\t=\\ttheano.shared(Wcf)\\n\\t\\nself.bf\\t\\t=\\ttheano.shared(bf)\\n\\t\\nself.Wxc\\t=\\ttheano.shared(Wxc)\\n\\t\\nself.Whc\\t=\\ttheano.shared(Whc)\\n\\t\\nself.bc\\t\\t=\\ttheano.shared(bc)\\n\\t\\nself.Wxo\\t=\\ttheano.shared(Wxo)\\n\\t\\nself.Who\\t=\\ttheano.shared(Who)\\n\\t\\nself.Wco\\t=\\ttheano.shared(Wco)\\n\\t\\nself.bo\\t\\t=\\ttheano.shared(bo)\\n\\t\\nself.c0\\t\\t=\\ttheano.shared(c0)\\n\\t\\nself.h0\\t\\t=\\ttheano.shared(h0)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9b6179ae-a758-4c7e-b711-ecbd8c1b6f25', embedding=None, metadata={'page_label': '112', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='self.params\\t=\\t[\\n\\t\\nself.Wxi,\\n\\t\\nself.Whi,\\n\\t\\nself.Wci,\\n\\t\\nself.bi,\\n\\t\\nself.Wxf,\\n\\t\\nself.Whf,\\n\\t\\nself.Wcf,\\n\\t\\nself.bf,\\n\\t\\nself.Wxc,\\n\\t\\nself.Whc,\\n\\t\\nself.bc,\\n\\t\\nself.Wxo,\\n\\t\\nself.Who,\\n\\t\\nself.Wco,\\n\\t\\nself.bo,\\n\\t\\nself.c0,\\n\\t\\nself.h0,\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fd559928-4589-4d11-adea-a3e641f1eb70', embedding=None, metadata={'page_label': '113', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n]\\n\\t\\n\\t\\n\\t\\ndef\\trecurrence(self,\\tx_t,\\th_t1,\\tc_t1):\\n\\t\\ni_t\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxi)\\t+\\th_t1.dot(self.Whi)\\t+\\tc_t1.dot(self.Wci)\\t+\\tself.bi)\\n\\t\\nf_t\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxf)\\t+\\th_t1.dot(self.Whf)\\t+\\tc_t1.dot(self.Wcf)\\t+\\tself.bf)\\n\\t\\nc_t\\t=\\tf_t\\t\\n\\tc_t1\\t+\\ti_t\\t\\n\\tT.tanh(x_t.dot(self.Wxc)\\t+\\th_t1.dot(self.Whc)\\t+\\tself.bc)\\n\\t\\no_t\\t=\\tT.nnet.sigmoid(x_t.dot(self.Wxo)\\t+\\th_t1.dot(self.Who)\\t+\\tc_t.dot(self.Wco)\\t+\\tself.bo)\\n\\t\\nh_t\\t=\\to_t\\t*\\tT.tanh(c_t)\\n\\t\\nreturn\\th_t,\\tc_t\\n\\t\\n\\t\\n\\t\\ndef\\toutput(self,\\tx):\\n\\t\\n#\\tinput\\tX\\tshould\\tbe\\ta\\tmatrix\\t(2-D)\\n\\t\\n#\\trows\\tindex\\ttime\\n\\t\\n[h,\\tc],\\t_\\t=\\ttheano.scan(\\n\\t\\nfn=self.recurrence,\\n\\t\\nsequences=x,\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ccc39307-b501-48d5-b40e-6a91404d9ca3', embedding=None, metadata={'page_label': '114', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\noutputs_info=[self.h0,\\tself.c0],\\n\\t\\nn_steps=x.shape[0],\\n\\t\\n)\\n\\t\\nreturn\\th\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ca82c8b3-29af-4114-9a67-1ec8f5650b3d', embedding=None, metadata={'page_label': '115', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nChapter\\t6:\\tLearning\\tfrom\\tWikipedia\\tData\\n\\t\\n\\t\\n\\t\\nIn\\tthis\\tchapter\\twe\\tare\\tgoing\\tto\\ttalk\\tabout\\thow\\twe’re\\tgoing\\tto\\tcreate\\ta\\tmodel\\tfor\\nthe\\tWikipedia\\tdata\\tdumps.\\n\\t\\n\\t\\n\\t\\nThe\\tgoal\\tis\\tto\\tcreate\\ta\\tlanguage\\tmodel\\tjust\\tlike\\twe\\tdid\\tfor\\tpoetry,\\tboth\\tare\\tjust\\nsequences\\tof\\twords.\\tThe\\tdifference\\tis\\tthat\\tthe\\tWikipedia\\tdataset\\tis\\tmuch\\tlarger\\nand\\thas\\tmany\\tmore\\twords.\\n\\t\\n\\t\\n\\t\\nYou’ll\\tfind\\tthat\\tthere\\tis\\tnot\\tmuch\\tnew\\there\\tin\\tterms\\tof\\tRNN\\tconcepts,\\tjust\\tthat\\nwe’re\\tgoing\\tto\\tplug\\tthe\\tGRU\\tand\\tLSTM\\tinstead\\tof\\tusing\\ta\\tsimple\\trecurrent\\tunit\\nlike\\tbefore.\\n\\t\\n\\t\\n\\t\\nMost\\tof\\tthe\\twork\\twill\\tgo\\tinto\\tgetting\\tand\\tprocessing\\tthe\\tdata.\\tThen\\tit’s\\tjust\\ta\\nmatter\\tof\\tcreating\\ta\\tlanguage\\tmodel,\\twhich\\twe\\thave\\talready\\tdone.\\n\\t\\n\\t\\n\\t\\nFirst\\tlet’s\\ttalk\\tabout\\thow\\tyou’re\\tgoing\\tto\\tget\\tthe\\tdata.\\n\\t\\n\\t\\n\\t\\nYou’ll\\twant\\tto\\tgo\\tto\\thttps://dumps.wikimedia.org/\\tand\\tclick\\ton\\t“Database\\tData', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a98105bc-9843-4e26-95c8-d9f4dee2453e', embedding=None, metadata={'page_label': '116', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Dumps”.\\n\\t\\n\\t\\n\\t\\nLook\\tfor\\tenwiki\\tand\\twe’re\\tinterested\\tin\\tthe\\tfiles\\tthat\\thave\\t“pages-articles”\\tin\\tthe\\nname.\\tYou\\tcan\\tdownload\\tthe\\tentire\\tdata\\tdump\\twhich\\tis\\tabout\\t12\\tor\\t13\\tGB\\tor\\nyou\\tcan\\tdownload\\tit\\tin\\tparts.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\tyou\\twon’t\\tneed\\tALL\\tthe\\tparts\\tto\\tobtain\\ta\\tmeaningful\\tresult.\\tWhen\\tI\\nvisualize\\tthe\\tdata\\tlater\\tI’m\\tjust\\tgoing\\tto\\tbe\\tusing\\tthe\\tfirst\\tpart.\\n\\t\\n\\t\\n\\t\\nThe\\tdata\\tis\\tin\\tbz2\\tformat\\tbut\\tinstead\\tof\\tdirectly\\tunzipping\\tit\\twe’re\\tgoing\\tto\\tuse\\ta\\ntool\\tto\\textract\\tthe\\tdata\\tin\\ta\\tmore\\tconvenient\\tformat,\\tsince\\tit’s\\tcurrently\\tin\\tXML.\\n\\t\\n\\t\\n\\t\\nThe\\tnext\\tstep\\tis\\tto\\tget\\tthe\\tdata\\tinto\\ta\\tflat\\tfile\\tformat.\\tTo\\tdo\\tthis\\twe’re\\tgoing\\tto\\nuse\\ta\\ttool\\tcalled\\twp2txt.\\n\\t\\n\\t\\n\\t\\nJust\\tgo\\tto\\thttps://github.com/yohasebe/wp2txt\\tand\\tfollow\\tthe\\tinstructions.\\n\\t\\n\\t\\n\\t\\nTo\\tinstall\\tit,\\tyou’ll\\twant\\tto\\tuse\\tthe\\tcommand\\t“sudo\\tgem\\tinstall\\twp2txt”.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8ee65c51-e773-47f5-a3be-e0461260a94e', embedding=None, metadata={'page_label': '117', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nNext,\\tgo\\tto\\tthe\\tfolder\\t“large_files”,\\twhich\\tshould\\tbe\\tadjacent\\tto\\tthe\\trnn_class\\nfolder,\\tand\\tput\\tthe\\tbz2\\tfile\\tin\\tthere.\\tThen\\trun\\tthe\\tcommand\\t“wp2txt\\t-i\\t<the\\nfilename>”.\\n\\t\\n\\t\\n\\t\\nIt\\tshould\\toutput\\ttext\\tfiles\\tinto\\tthe\\tsame\\tfolder.\\n\\t\\n\\t\\n\\t\\nFinally\\tlet’s\\ttalk\\tabout\\thow\\twe\\tare\\tgoing\\tto\\ttake\\tthese\\ttext\\tfiles\\tand\\tget\\tit\\tinto\\nthe\\tright\\tformat\\tfor\\tour\\tneural\\tnetwork.\\n\\t\\n\\t\\n\\t\\nThis\\tcode\\tis\\tof\\tcourse\\tinside\\tutil.py,\\twhere\\tall\\tour\\tother\\tdata\\tgathering\\tcode\\tis.\\nThe\\trelevant\\tfunction\\tis\\tget_wikipedia_data.\\n\\t\\n\\t\\n\\t\\nThis\\tfunction\\ttakes\\tin\\t2\\tparameters:\\tn_files\\tand\\tn_vocab.\\n\\t\\n\\t\\n\\t\\nWe\\ttake\\tin\\tn_files\\tbecause\\tthere\\tare\\tgoing\\tto\\tbe\\ta\\tlot\\tof\\tinput\\tfiles\\t-\\ttoo\\tmany\\tto\\nfit\\tinto\\tmemory\\tif\\tyou\\tuse\\tall\\tthe\\tdata.\\tI’ve\\tbeen\\tlimiting\\tit\\tto\\t100\\tor\\tless,\\tand\\nthat\\tseems\\tto\\tbe\\tenough\\tto\\tlearn\\ta\\tmeaningful\\trepresentation\\tof\\tthe\\tdata.\\n\\t\\n\\t\\n\\t\\nYou\\talso\\twant\\tto\\tlimit\\tthe\\tvocabulary,\\twhich\\tis\\ta\\tlot\\tlarger\\tthan\\tthe\\tpoetry\\ndataset.\\tWe’re\\tgoing\\tto\\thave\\ton\\tthe\\torder\\tof\\t500,000\\tand\\tover\\t1\\tmillion\\twords.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f54a9512-f203-42db-b5d5-6147f0551342', embedding=None, metadata={'page_label': '118', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nRemember\\tthat\\tthe\\toutput\\ttarget\\tis\\tthe\\tnext\\tword,\\tso\\tthat’s\\t1\\tmillion\\toutput\\nclasses,\\twhich\\tis\\ta\\tlot\\tof\\toutput\\tclasses.\\tThis\\twill\\tmake\\tit\\thard\\tto\\tget\\tgood\\naccuracy,\\tand\\tit’s\\talso\\tgoing\\tto\\tmake\\tour\\toutput\\tweight\\thuge.\\n\\t\\n\\t\\n\\t\\nTo\\tremedy\\tthis,\\twe’ll\\trestrict\\tthe\\tvocabulary\\tsize\\tto\\tn_vocab.\\tUsually\\tthis\\tis\\tset\\nto\\taround\\t2000\\twords.\\n\\t\\n\\t\\n\\t\\nNote\\tthat\\tthe\\t2000\\twords\\twe\\twant\\tare\\tthe\\t2000\\tmost\\tcommon\\twords,\\tnot\\tjust\\n2000\\trandom\\twords.\\n\\t\\n\\t\\n\\t\\nBecause\\tof\\tthis,\\twe’ll\\tneed\\tto\\tdo\\ta\\tfew\\tthings.\\n\\t\\n\\t\\n\\t\\nFirst\\twe\\tneed\\tto\\tkeep\\ta\\tcount\\tof\\thow\\tmany\\ttimes\\ta\\tword\\thas\\tappeared\\tin\\ta\\ndictionary.\\n\\t\\n\\t\\n\\t\\nNext,\\twe\\tneed\\tto\\tsort\\tthat\\tdictionary\\tby\\tvalue\\tin\\treverse\\tso\\tthat\\tthe\\thighest\\tcount\\nword\\tcomes\\tfirst.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6acbf1bb-abfa-4d88-b89c-9144d2f6168d', embedding=None, metadata={'page_label': '119', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nThis\\twill\\tgive\\tus\\tthe\\tindexes\\tfor\\tthe\\ttop\\t2000\\twords.\\n\\t\\n\\t\\n\\t\\nHowever,\\twe’re\\tstill\\tnot\\tdone,\\tbecause\\tour\\tword\\tembedding\\tmatrix\\tneeds\\tto\\tbe\\nof\\tsize\\tV=2000,\\tand\\tso\\tthe\\tword\\tindexes\\tmust\\tbe\\tfrom\\t0..2000.\\n\\t\\n\\t\\n\\t\\nBut\\tthe\\tword\\tindexes\\twe\\tcurrently\\thave\\tare\\tjust\\t2000\\trandom\\tnumbers\\tfrom\\t0..1\\nmillion,\\tassuming\\tour\\ttotal\\tvocabulary\\tis\\t1\\tmillion\\twords.\\n\\t\\n\\t\\n\\t\\nTherefore,\\twe\\tneed\\tto\\tcreate\\ta\\tnew\\tmapping,\\tfrom\\told\\tword\\tindex\\tto\\tnew\\tword\\nindex,\\twhere\\tthe\\told\\tword\\tindex\\tis\\tany\\tnumber\\tfrom\\t0..\\t1\\tmillion\\tand\\tthe\\tnew\\nword\\tindex\\tis\\ta\\tnumber\\tfrom\\t0..2000.\\n\\t\\n\\t\\n\\t\\nThis\\talso\\tmeans\\twe\\tneed\\tto\\tcreate\\ta\\tnew\\tword2idx\\tdictionary\\tas\\twell.\\n\\t\\n\\t\\n\\t\\nIf\\tyou\\twant\\tto\\ttrain\\tyour\\tmodel\\ton\\tthe\\tentire\\tdataset,\\tyou’ll\\tprobably\\trun\\tout\\tof\\nmemory,\\tbut\\tthere\\tare\\tsome\\tways\\taround\\tthis.\\n\\t\\n\\t\\n\\t\\nFirst,\\tyou\\tdon’t\\twant\\tto\\tload\\tall\\tyour\\tdata\\tinto\\tmemory\\tall\\tat\\tthe\\tsame\\ttime.\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5dd04547-d994-472a-a90c-b5e3f5503310', embedding=None, metadata={'page_label': '120', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nOne\\tstrategy\\twould\\tbe\\tto\\ttrain\\ton\\teach\\tseparate\\ttext\\tfile\\tone\\tat\\ta\\ttime\\twithin\\neach\\tepoch.\\n\\t\\n\\t\\n\\t\\nSo\\twithin\\tthe\\tepoch,\\tyou\\tloop\\tthrough\\teach\\tfile,\\topen\\tthe\\tfile,\\tconvert\\tall\\tthe\\nsentences\\tinto\\tarrays\\tof\\tword\\tindexes,\\tand\\tthen\\ttrain\\ton\\tthose\\tsentences.\\tOpen\\nthe\\tnext\\tfile,\\tand\\tso\\ton.\\tYou\\twould\\thave\\tto\\tof\\tcourse\\tkeep\\ta\\tdictionary\\tof\\tthe\\nword\\tto\\tindex\\tmapping\\thandy\\tso\\tthat\\tit\\tremains\\tconsistent\\tbetween\\teach\\tfile.\\n\\t\\n\\t\\n\\t\\nYou\\tcan\\timagine\\tthat\\tre-processing\\tthe\\tsentences\\tafter\\teach\\tfile\\topen\\twould\\tbe\\nslow.\\tAnother\\toption\\twould\\tbe\\tto\\tconvert\\teach\\tsentences\\tinto\\tlists\\tof\\tword\\nindexes\\tbefore\\trunning\\tyour\\tneural\\tnetwork,\\tand\\tto\\tsave\\tyour\\tword\\tto\\tindex\\nmapping\\tto\\ta\\tfile\\tas\\twell.\\n\\t\\n\\t\\n\\t\\nThat\\tway,\\tyour\\tinput\\tdata\\tcan\\tbe\\tjust\\ta\\tbunch\\tof\\tarrays\\tof\\tword\\tindexes.\\tThis\\nwould\\tstill\\tneed\\tto\\tbe\\tin\\tmultiple\\tfiles\\tsince\\tit\\twould\\ttake\\tup\\ttoo\\tmuch\\tRAM,\\nbut\\tat\\tleast\\tyou\\twon’t\\thave\\tto\\tconvert\\tthe\\tdata\\teach\\ttime\\tyou\\topen\\tthe\\tfile.\\n\\t\\n\\t\\n\\t\\nYet\\tanother\\toption\\tmight\\tbe\\tto\\tuse\\ta\\tsimple\\tdatabase\\tlike\\tSQLite\\tto\\tstore\\tthe\\narrays\\tof\\tword\\tindexes.\\tThis\\tway,\\tyou\\tcan\\thave\\tthe\\tentire\\tdataset\\tin\\tthe\\ndatabase,\\tand\\tretrieve\\trows\\tat\\trandom\\twithout\\thaving\\tto\\tstore\\tit\\tin\\tmemory.\\n\\t\\n\\t\\n\\t\\nWe\\twon’t\\tdo\\tthis\\tsince\\tit’s\\tnot\\tthe\\tfocus\\tof\\tthis\\tcourse,\\tbut\\tif\\tyou’re\\tinterested\\tin', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b4379e5c-1a1e-4b6a-98b1-93db34b9721b', embedding=None, metadata={'page_label': '121', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"learning\\tabout\\ttechniques\\tlike\\tthis\\tthen\\tyou’ll\\twant\\tto\\tcheck\\tout\\tmy\\tcourse\\ton\\nSQL.\\n\\t\\n\\t\\n\\t\\nLet\\tus\\tnow\\tlook\\tat\\tthe\\tcode.\\n\\t\\n\\t\\n\\t\\nFirst,\\tfor\\tloading\\tthe\\tWiki\\tdata:\\n\\t\\n\\t\\n\\t\\ndef\\tget_wikipedia_data(n_files,\\tn_vocab,\\tby_paragraph=False):\\tprefix\\t=\\n'../large_files/'\\n\\t\\ninput_files\\t=\\t[f\\tfor\\tf\\tin\\tos.listdir(prefix)\\tif\\tf.startswith('enwiki')\\tand\\nf.endswith('txt')]\\n\\t\\n\\t\\n\\t\\n#\\treturn\\tvariables\\n\\t\\nsentences\\t=\\t[]\\n\\t\\nword2idx\\t=\\t{'START':\\t0,\\t'END':\\t1}\\n\\t\\nidx2word\\t=\\t['START',\\t'END']\\n\\t\\ncurrent_idx\\t=\\t2\\n\\t\\nword_idx_count\\t=\\t{0:\\tfloat('inf'),\\t1:\\tfloat('inf')}\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='eae2a8c7-1c94-45cf-b9ac-70de539bf04b', embedding=None, metadata={'page_label': '122', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nif\\tn_files\\tis\\tnot\\tNone:\\n\\t\\ninput_files\\t=\\tinput_files[:n_files]\\n\\t\\n\\t\\n\\t\\nfor\\tf\\tin\\tinput_files:\\n\\t\\nprint\\t\"reading:\",\\tf\\n\\t\\nfor\\tline\\tin\\topen(prefix\\t+\\tf):\\n\\t\\nline\\t=\\tline.strip()\\n\\t\\n#\\tdon\\'t\\tcount\\theaders,\\tstructured\\tdata,\\tlists,\\tetc...\\n\\t\\nif\\tline\\tand\\tline[0]\\tnot\\tin\\t(\\'[\\',\\t\\'*\\',\\t\\'-\\',\\t\\'|\\',\\t\\'=\\',\\t\\'{\\',\\t\\'}\\'):\\tif\\tby_paragraph:\\n\\t\\nsentence_lines\\t=\\t[line]\\n\\t\\nelse:\\n\\t\\nsentence_lines\\t=\\tline.split(\\'.\\t\\')\\n\\t\\nfor\\tsentence\\tin\\tsentence_lines:\\n\\t\\ntokens\\t=\\tmy_tokenizer(sentence)\\n\\t\\nfor\\tt\\tin\\ttokens:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f7d119e0-9ef1-4e95-ba96-4a939c13cb12', embedding=None, metadata={'page_label': '123', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='for\\tt\\tin\\ttokens:\\n\\t\\nif\\tt\\tnot\\tin\\tword2idx:\\n\\t\\nword2idx[t]\\t=\\tcurrent_idx\\n\\t\\nidx2word.append(t)\\n\\t\\ncurrent_idx\\t+=\\t1\\n\\t\\nidx\\t=\\tword2idx[t]\\n\\t\\nword_idx_count[idx]\\t=\\tword_idx_count.get(idx,\\t0)\\t+\\t1\\n\\t\\nsentence_by_idx\\t=\\t[word2idx[t]\\tfor\\tt\\tin\\ttokens]\\n\\t\\nsentences.append(sentence_by_idx)\\n\\t\\n\\t\\n\\t\\n#\\trestrict\\tvocab\\tsize\\n\\t\\nsorted_word_idx_count\\t=\\tsorted(word_idx_count.items(),\\nkey=operator.itemgetter(1),\\treverse=True)\\tword2idx_small\\t=\\t{}\\n\\t\\nnew_idx\\t=\\t0\\n\\t\\nidx_new_idx_map\\t=\\t{}\\n\\t\\nfor\\tidx,\\tcount\\tin\\tsorted_word_idx_count[:n_vocab]:\\tword\\t=\\tidx2word[idx]\\n\\t\\nprint\\tword,\\tcount\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='eef530c3-b72b-48aa-a160-58a3fc2be975', embedding=None, metadata={'page_label': '124', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nword2idx_small[word]\\t=\\tnew_idx\\n\\t\\nidx_new_idx_map[idx]\\t=\\tnew_idx\\n\\t\\nnew_idx\\t+=\\t1\\n\\t\\n#\\tlet\\t'unknown'\\tbe\\tthe\\tlast\\ttoken\\n\\t\\nword2idx_small['UNKNOWN']\\t=\\tnew_idx\\tunknown\\t=\\tnew_idx\\n\\t\\n\\t\\n\\t\\nassert('START'\\tin\\tword2idx_small)\\n\\t\\nassert('END'\\tin\\tword2idx_small)\\n\\t\\nassert('king'\\tin\\tword2idx_small)\\n\\t\\nassert('queen'\\tin\\tword2idx_small)\\n\\t\\nassert('man'\\tin\\tword2idx_small)\\n\\t\\nassert('woman'\\tin\\tword2idx_small)\\n\\t\\n\\t\\n\\t\\n#\\tmap\\told\\tidx\\tto\\tnew\\tidx\\n\\t\\nsentences_small\\t=\\t[]\\n\\t\\nfor\\tsentence\\tin\\tsentences:\\n\\t\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7baa13a0-021e-4143-a2f2-9ef9f47e123a', embedding=None, metadata={'page_label': '125', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nif\\tlen(sentence)\\t>\\t1:\\n\\t\\nnew_sentence\\t=\\t[idx_new_idx_map[idx]\\tif\\tidx\\tin\\tidx_new_idx_map\\telse\\nunknown\\tfor\\tidx\\tin\\tsentence]\\n\\t\\nsentences_small.append(new_sentence)\\n\\t\\nreturn\\tsentences_small,\\tword2idx_small\\n\\t\\nNext,\\tfor\\tfitting\\tthe\\tRNN\\tand\\tdoing\\tsome\\tword\\tanalogies\\t(notice\\thow\\twe\\tcan\\nplug\\tand\\tplay\\tLSTM\\tand\\tGRU):\\n\\t\\nimport\\tsys\\n\\t\\nimport\\ttheano\\n\\t\\nimport\\ttheano.tensor\\tas\\tT\\n\\t\\nimport\\tnumpy\\tas\\tnp\\n\\t\\nimport\\tmatplotlib.pyplot\\tas\\tplt\\n\\t\\nimport\\tjson\\n\\t\\n\\t\\n\\t\\nfrom\\tdatetime\\timport\\tdatetime\\n\\t\\nfrom\\tsklearn.utils\\timport\\tshuffle\\n\\t\\nfrom\\tgru\\timport\\tGRU\\n\\t\\nfrom\\tlstm\\timport\\tLSTM\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='67b0accf-01b1-4bf5-b5a9-0a40fd84402e', embedding=None, metadata={'page_label': '126', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nfrom\\tutil\\timport\\tinit_weight,\\tget_wikipedia_data\\n\\t\\n\\t\\n\\t\\nclass\\tRNN:\\n\\t\\ndef\\t__init__(self,\\tD,\\thidden_layer_sizes,\\tV):\\tself.hidden_layer_sizes\\t=\\thidden_layer_sizes\\tself.D\\t=\\tD\\n\\t\\nself.V\\t=\\tV\\n\\t\\n\\t\\n\\t\\ndef\\tfit(self,\\tX,\\tlearning_rate=10e-5,\\tmu=0.99,\\tepochs=10,\\tshow_fig=True,\\tactivation=T.nnet.relu,\\nRecurrentUnit=GRU,\\tnormalize=True):\\tD\\t=\\tself.D\\n\\t\\nV\\t=\\tself.V\\n\\t\\nN\\t=\\tlen(X)\\n\\t\\n\\t\\n\\t\\nWe\\t=\\tinit_weight(V,\\tD)\\n\\t\\nself.hidden_layers\\t=\\t[]\\n\\t\\nMi\\t=\\tD\\n\\t\\nfor\\tMo\\tin\\tself.hidden_layer_sizes:\\tru\\t=\\tRecurrentUnit(Mi,\\tMo,\\tactivation)\\tself.hidden_layers.append(ru)\\n\\t\\nMi\\t=\\tMo\\n\\t\\n\\t\\n\\t\\nWo\\t=\\tinit_weight(Mi,\\tV)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8641c702-af9e-4fb8-9285-38f8b6cb4288', embedding=None, metadata={'page_label': '127', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"\\t\\nbo\\t=\\tnp.zeros(V)\\n\\t\\n\\t\\n\\t\\nself.We\\t=\\ttheano.shared(We)\\n\\t\\nself.Wo\\t=\\ttheano.shared(Wo)\\n\\t\\nself.bo\\t=\\ttheano.shared(bo)\\n\\t\\nself.params\\t=\\t[self.Wo,\\tself.bo]\\n\\t\\nfor\\tru\\tin\\tself.hidden_layers:\\n\\t\\nself.params\\t+=\\tru.params\\n\\t\\n\\t\\n\\t\\nthX\\t=\\tT.ivector('X')\\n\\t\\nthY\\t=\\tT.ivector('Y')\\n\\t\\n\\t\\n\\t\\nZ\\t=\\tself.We[thX]\\n\\t\\nfor\\tru\\tin\\tself.hidden_layers:\\n\\t\\nZ\\t=\\tru.output(Z)\\n\\t\\npy_x\\t=\\tT.nnet.softmax(Z.dot(self.Wo)\\t+\\tself.bo)\\n\\t\\nprediction\\t=\\tT.argmax(py_x,\\taxis=1)\\t#\\tlet's\\treturn\\tpy_x\\ttoo\\tso\\twe\\tcan\\tdraw\\ta\\tsample\\tinstead\\tself.predict_op\\n=\\ttheano.function(\\n\\t\\ninputs=[thX],\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf45784a-5367-4c42-bc57-94758034c3b3', embedding=None, metadata={'page_label': '128', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='inputs=[thX],\\n\\t\\noutputs=[py_x,\\tprediction],\\n\\t\\nallow_input_downcast=True,\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ncost\\t=\\t-T.mean(T.log(py_x[T.arange(thY.shape[0]),\\tthY]))\\tgrads\\t=\\tT.grad(cost,\\tself.params)\\n\\t\\ndparams\\t=\\t[theano.shared(p.get_value()*0)\\tfor\\tp\\tin\\tself.params]\\n\\t\\n\\t\\n\\t\\ndWe\\t=\\ttheano.shared(self.We.get_value()*0)\\tgWe\\t=\\tT.grad(cost,\\tself.We)\\n\\t\\ndWe_update\\t=\\tmu*dWe\\t-\\tlearning_rate*gWe\\tWe_update\\t=\\tself.We\\t+\\tdWe_update\\n\\t\\nif\\tnormalize:\\n\\t\\nWe_update\\t/=\\tWe_update.norm(2)\\n\\t\\n\\t\\n\\t\\nupdates\\t=\\t[\\n\\t\\n(p,\\tp\\t+\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tp,\\tdp,\\tg\\tin\\tzip(self.params,\\tdparams,\\tgrads)\\t]\\t+\\t[\\n\\t\\n(dp,\\tmu*dp\\t-\\tlearning_rate*g)\\tfor\\tdp,\\tg\\tin\\tzip(dparams,\\tgrads)\\t]\\t+\\t[\\n\\t\\n(self.We,\\tWe_update),\\t(dWe,\\tdWe_update)\\t]\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='900dfa18-d6dc-43cc-aa2d-4011e665670f', embedding=None, metadata={'page_label': '129', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nself.train_op\\t=\\ttheano.function(\\n\\t\\ninputs=[thX,\\tthY],\\n\\t\\noutputs=[cost,\\tprediction],\\n\\t\\nupdates=updates\\n\\t\\n)\\n\\t\\n\\t\\n\\t\\ncosts\\t=\\t[]\\n\\t\\nfor\\ti\\tin\\txrange(epochs):\\n\\t\\nt0\\t=\\tdatetime.now()\\n\\t\\nX\\t=\\tshuffle(X)\\n\\t\\nn_correct\\t=\\t0\\n\\t\\nn_total\\t=\\t0\\n\\t\\ncost\\t=\\t0\\n\\t\\nfor\\tj\\tin\\txrange(N):\\n\\t\\nif\\tnp.random.random()\\t<\\t0.01\\tor\\tlen(X[j])\\t<=\\t1:\\tinput_sequence\\t=\\t[0]\\t+\\tX[j]\\n\\t\\noutput_sequence\\t=\\tX[j]\\t+\\t[1]\\n\\t\\nelse:\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f25cef3f-3f17-458e-99b4-a93fc5691d7a', embedding=None, metadata={'page_label': '130', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\ninput_sequence\\t=\\t[0]\\t+\\tX[j][:-1]\\n\\t\\noutput_sequence\\t=\\tX[j]\\n\\t\\nn_total\\t+=\\tlen(output_sequence)\\n\\t\\nc,\\tp\\t=\\tself.train_op(input_sequence,\\toutput_sequence)\\tcost\\t+=\\tc\\n\\t\\nfor\\tpj,\\txj\\tin\\tzip(p,\\toutput_sequence):\\tif\\tpj\\t==\\txj:\\n\\t\\nn_correct\\t+=\\t1\\n\\t\\nif\\tj\\t%\\t200\\t==\\t0:\\n\\t\\nsys.stdout.write(\"j/N:\\t%d/%d\\tcorrect\\trate\\tso\\tfar:\\t%f\\\\r\"\\t%\\t(j,\\tN,\\tfloat(n_correct)/n_total))\\tsys.stdout.flush()\\n\\t\\nprint\\t\"i:\",\\ti,\\t\"cost:\",\\tcost,\\t\"correct\\trate:\",\\t(float(n_correct)/n_total),\\t\"time\\tfor\\tepoch:\",\\t(datetime.now()\\t-\\tt0)\\ncosts.append(cost)\\n\\t\\n\\t\\n\\t\\nif\\tshow_fig:\\n\\t\\nplt.plot(costs)\\n\\t\\nplt.show()\\n\\t\\n\\t\\n\\t\\n\\t\\n\\t\\ndef\\ttrain_wikipedia(we_file=\\'word_embeddings.npy\\',\\tw2i_file=\\'wikipedia_word2idx.json\\',\\nRecurrentUnit=GRU):\\tsentences,\\tword2idx\\t=\\tget_wikipedia_data(n_files=50,\\tn_vocab=2000)\\tprint\\n\"finished\\tretrieving\\tdata\"\\n\\t\\nprint\\t\"vocab\\tsize:\",\\tlen(word2idx),\\t\"number\\tof\\tsentences:\",\\tlen(sentences)\\trnn\\t=\\tRNN(30,\\t[30],', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fa89ef31-617a-4afa-a807-ce994eae492d', embedding=None, metadata={'page_label': '131', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='print\\t\"vocab\\tsize:\",\\tlen(word2idx),\\t\"number\\tof\\tsentences:\",\\tlen(sentences)\\trnn\\t=\\tRNN(30,\\t[30],\\nlen(word2idx))\\trnn.fit(sentences,\\tlearning_rate=10e-6,\\tepochs=10,\\tshow_fig=True,\\tactivation=T.nnet.relu)\\n\\t\\nnp.save(we_file,\\trnn.We.get_value())\\twith\\topen(w2i_file,\\t\\'w\\')\\tas\\tf:\\n\\t\\njson.dump(word2idx,\\tf)\\n\\t\\n\\t\\n\\t\\ndef\\tfind_analogies(w1,\\tw2,\\tw3,\\twe_file=\\'word_embeddings.npy\\',\\tw2i_file=\\'wikipedia_word2idx.json\\'):\\tWe\\n=\\tnp.load(we_file)\\n\\t\\nwith\\topen(w2i_file)\\tas\\tf:\\n\\t\\nword2idx\\t=\\tjson.load(f)\\n\\t\\n\\t\\n\\t\\nking\\t=\\tWe[word2idx[w1]]\\n\\t\\nman\\t=\\tWe[word2idx[w2]]\\n\\t\\nwoman\\t=\\tWe[word2idx[w3]]\\n\\t\\nv0\\t=\\tking\\t-\\tman\\t+\\twoman\\n\\t\\n\\t\\n\\t\\ndef\\tdist1(a,\\tb):\\n\\t\\nreturn\\tnp.linalg.norm(a\\t-\\tb)\\n\\t\\ndef\\tdist2(a,\\tb):\\n\\t\\nreturn\\t1\\t-\\ta.dot(b)\\t/\\t(np.linalg.norm(a)\\t*\\tnp.linalg.norm(b))\\n\\t\\nfor\\tdist,\\tname\\tin\\t[(dist1,\\t\\'Euclidean\\'),\\t(dist2,\\t\\'cosine\\')]:\\tmin_dist\\t=\\tfloat(\\'inf\\')', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2269cb9a-4efa-4feb-bdc1-dbced066108d', embedding=None, metadata={'page_label': '132', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='for\\tdist,\\tname\\tin\\t[(dist1,\\t\\'Euclidean\\'),\\t(dist2,\\t\\'cosine\\')]:\\tmin_dist\\t=\\tfloat(\\'inf\\')\\n\\t\\nbest_word\\t=\\t\\'\\'\\n\\t\\nfor\\tword,\\tidx\\tin\\tword2idx.iteritems():\\tif\\tword\\tnot\\tin\\t(w1,\\tw2,\\tw3):\\n\\t\\nv1\\t=\\tWe[idx]\\n\\t\\nd\\t=\\tdist(v0,\\tv1)\\n\\t\\nif\\td\\t<\\tmin_dist:\\n\\t\\nmin_dist\\t=\\td\\n\\t\\nbest_word\\t=\\tword\\n\\t\\nprint\\t\"closest\\tmatch\\tby\",\\tname,\\t\"distance:\",\\tbest_word\\tprint\\tw1,\\t\"-\",\\tw2,\\t\"=\",\\tbest_word,\\t\"-\",\\tw3\\n\\t\\n\\t\\n\\t\\nif\\t__name__\\t==\\t\\'__main__\\':\\n\\t\\ntrain_wikipedia()\\n\\t\\nwe\\t=\\t\\'lstm_word_embeddings2.npy\\'\\n\\t\\nw2i\\t=\\t\\'lstm_wikipedia_word2idx2.json\\'\\n\\t\\ntrain_wikipedia(we,\\tw2i,\\tRecurrentUnit=LSTM)\\tfind_analogies(\\'king\\',\\t\\'man\\',\\t\\'woman\\',\\twe,\\tw2i)\\nfind_analogies(\\'france\\',\\t\\'paris\\',\\t\\'london\\',\\twe,\\tw2i)\\tfind_analogies(\\'france\\',\\t\\'paris\\',\\t\\'rome\\',\\twe,\\tw2i)\\nfind_analogies(\\'paris\\',\\t\\'france\\',\\t\\'italy\\',\\twe,\\tw2i)\\n\\t\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8440a457-cee3-4011-84e2-c14ddad918bb', embedding=None, metadata={'page_label': '133', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nConclusion\\n\\t\\n\\t\\n\\t\\nI\\treally\\thope\\tyou\\thad\\tas\\tmuch\\tfun\\treading\\tthis\\tbook\\tas\\tI\\tdid\\tmaking\\tit.\\n\\t\\n\\t\\n\\t\\nDid\\tyou\\tfind\\tanything\\tconfusing?\\tDo\\tyou\\thave\\tany\\tquestions?\\n\\t\\n\\t\\n\\t\\nI\\tam\\talways\\tavailable\\tto\\thelp.\\tJust\\temail\\tme\\tat:\\t\\ninfo@lazyprogrammer.me\\n\\t\\n\\t\\n\\t\\nI\\tdo\\t1:1\\tcoaching\\tand\\tconsulting\\tas\\twell.\\n\\t\\n\\t\\n\\t\\nDo\\tyou\\twant\\tto\\tlearn\\tmore\\tabout\\tdeep\\tlearning?\\tPerhaps\\tonline\\tcourses\\tare\\nmore\\tyour\\tstyle.\\tI\\thappen\\tto\\thave\\ta\\tfew\\tof\\tthem\\ton\\tUdemy.\\n\\t\\n\\t\\n\\t\\nMy\\tfirst\\tcourse\\tin\\tdeep\\tlearning\\tis\\ta\\tlot\\tlike\\tthe\\tbook,\\tbut\\tyou\\tget\\tto\\tsee\\tme\\nderive\\tthe\\tformulas\\tand\\twrite\\tthe\\tcode\\tlive:\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8492a978-2f2f-437a-8190-38622b737e2b', embedding=None, metadata={'page_label': '134', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Data\\tScience:\\tDeep\\tLearning\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://udemy.com/data-science-deep-learning-in-python\\n\\t\\n\\t\\n\\t\\nAre\\tyou\\tcomfortable\\twith\\tthis\\tmaterial,\\tand\\tyou\\twant\\tto\\ttake\\tyour\\tdeep\\tlearning\\nskillset\\tto\\tthe\\tnext\\tlevel?\\tThen\\tmy\\tfollow-up\\tUdemy\\tcourse\\ton\\tdeep\\tlearning\\tis\\nfor\\tyou.\\tSimilar\\tto\\tthis\\tbook,\\tI\\ttake\\tyou\\tthrough\\tthe\\tbasics\\tof\\tTheano\\tand\\nTensorFlow\\t-\\tcreating\\tfunctions,\\tvariables,\\tand\\texpressions,\\tand\\tbuild\\tup\\tneural\\nnetworks\\tfrom\\tscratch.\\tI\\tteach\\tyou\\tabout\\tways\\tto\\taccelerate\\tthe\\tlearning\\tprocess,\\nincluding\\tbatch\\tgradient\\tdescent,\\tmomentum,\\tand\\tadaptive\\tlearning\\trates.\\tI\\talso\\nshow\\tyou\\tlive\\thow\\tto\\tcreate\\ta\\tGPU\\tinstance\\ton\\tAmazon\\tAWS\\tEC2,\\tand\\tprove\\nto\\tyou\\tthat\\ttraining\\ta\\tneural\\tnetwork\\twith\\tGPU\\toptimization\\tcan\\tbe\\torders\\tof\\nmagnitude\\tfaster\\tthan\\ton\\tyour\\tCPU.\\n\\t\\n\\t\\n\\t\\nData\\tScience:\\tPractical\\tDeep\\tLearning\\tin\\tTheano\\tand\\tTensorFlow\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\\n\\t\\n\\t\\n\\t\\nWhen\\tyou’ve\\tgot\\tthe\\tbasics\\tof\\tdeep\\tlearning\\tdown,\\tyou’re\\tready\\tto\\texplore\\nalternative\\tarchitectures.\\tOne\\tvery\\tpopular\\talternative\\tis\\tthe\\tconvolutional\\tneural\\nnetwork,\\tcreated\\tspecifically\\tfor\\timage\\tclassification.\\tThese\\thave\\tpromising\\napplications\\tin\\tmedical\\timaging,\\tself-driving\\tvehicles,\\tand\\tmore.\\tIn\\tthis\\tcourse,\\tI\\nshow\\tyou\\thow\\tto\\tbuild\\tconvolutional\\tnets\\tin\\tTheano\\tand\\tTensorFlow.\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ad400de4-1ef5-416a-84fd-6ed1e06d02b5', embedding=None, metadata={'page_label': '135', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\n\\t\\nDeep\\tLearning:\\tConvolutional\\tNeural\\tNetworks\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/deep-learning-convolutional-neural-networks-theano-\\ntensorflow\\n\\t\\n\\t\\n\\t\\nIn\\tpart\\t4\\tof\\tmy\\tdeep\\tlearning\\tseries,\\tI\\ttake\\tyou\\tthrough\\tunsupervised\\tdeep\\nlearning\\tmethods\\t(that’s\\tthis\\tbook!).\\tWe\\tstudy\\tprincipal\\tcomponents\\tanalysis\\n(PCA),\\tt-SNE\\t(jointly\\tdeveloped\\tby\\tthe\\tgodfather\\tof\\tdeep\\tlearning,\\tGeoffrey\\nHinton),\\tdeep\\tautoencoders,\\tand\\trestricted\\tBoltzmann\\tmachines\\t(RBMs).\\tI\\ndemonstrate\\thow\\tunsupervised\\tpretraining\\ton\\ta\\tdeep\\tnetwork\\twith\\tautoencoders\\nand\\tRBMs\\tcan\\timprove\\tsupervised\\tlearning\\tperformance.\\n\\t\\n\\t\\n\\t\\nUnsupervised\\tDeep\\tLearning\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/unsupervised-deep-learning-in-python\\n\\t\\n\\t\\n\\t\\nWould\\tyou\\tlike\\tan\\tintroduction\\tto\\tthe\\tbasic\\tbuilding\\tblock\\tof\\tneural\\tnetworks\\t-\\nlogistic\\tregression?\\tIn\\tthis\\tcourse\\tI\\tteach\\tthe\\ttheory\\tof\\tlogistic\\tregression\\t(our\\ncomputational\\tmodel\\tof\\tthe\\tneuron),\\tand\\tgive\\tyou\\tan\\tin-depth\\tlook\\tat\\tbinary\\nclassification,\\tmanually\\tcreating\\tfeatures,\\tand\\tgradient\\tdescent.\\tYou\\tmight\\twant', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dd827a30-f3a1-47c9-8c61-95293b4aa3f5', embedding=None, metadata={'page_label': '136', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='to\\tcheck\\tthis\\tcourse\\tout\\tif\\tyou\\tfound\\tthe\\tmaterial\\tin\\tthis\\tbook\\ttoo\\tchallenging.\\n\\t\\n\\t\\n\\t\\nData\\tScience:\\tLogistic\\tRegression\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://udemy.com/data-science-logistic-regression-in-python\\n\\t\\n\\t\\n\\t\\nTo\\tget\\tan\\teven\\tsimpler\\tpicture\\tof\\tmachine\\tlearning\\tin\\tgeneral,\\twhere\\twe\\tdon’t\\neven\\tneed\\tgradient\\tdescent\\tand\\tcan\\tjust\\tsolve\\tfor\\tthe\\toptimal\\tmodel\\tparameters\\ndirectly\\tin\\t“closed-form”,\\tyou’ll\\twant\\tto\\tcheck\\tout\\tmy\\tfirst\\tUdemy\\tcourse\\ton\\tthe\\nclassical\\tstatistical\\tmethod\\t-\\tlinear\\tregression:\\n\\t\\n\\t\\n\\t\\nData\\tScience:\\tLinear\\tRegression\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/data-science-linear-regression-in-python\\n\\t\\n\\t\\n\\t\\nIf\\tyou\\tare\\tinterested\\tin\\tlearning\\tabout\\thow\\tmachine\\tlearning\\tcan\\tbe\\tapplied\\tto\\nlanguage,\\ttext,\\tand\\tspeech,\\tyou’ll\\twant\\tto\\tcheck\\tout\\tmy\\tcourse\\ton\\tNatural\\nLanguage\\tProcessing,\\tor\\tNLP:\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a5e5f616-8ecf-4ec1-8336-655bf3cdb178', embedding=None, metadata={'page_label': '137', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nData\\tScience:\\tNatural\\tLanguage\\tProcessing\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/data-science-natural-language-processing-in-python\\n\\t\\n\\t\\n\\t\\nIf\\tyou\\tare\\tinterested\\tin\\tlearning\\tSQL\\t-\\tstructured\\tquery\\tlanguage\\t-\\ta\\tlanguage\\nthat\\tcan\\tbe\\tapplied\\tto\\tdatabases\\tas\\tsmall\\tas\\tthe\\tones\\tsitting\\ton\\tyour\\tiPhone,\\tto\\ndatabases\\tas\\tlarge\\tas\\tthe\\tones\\tthat\\tspan\\tmultiple\\tcontinents\\t-\\tand\\tnot\\tonly\\tlearn\\nthe\\tmechanics\\tof\\tthe\\tlanguage\\tbut\\tknow\\thow\\tto\\tapply\\tit\\tto\\treal-world\\tdata\\nanalytics\\tand\\tmarketing\\tproblems?\\tCheck\\tout\\tmy\\tcourse\\there:\\n\\t\\n\\t\\n\\t\\nSQL\\tfor\\tMarketers:\\tDominate\\tdata\\tanalytics,\\tdata\\tscience,\\tand\\tbig\\tdata\\n\\t\\n\\t\\n\\t\\nhttps://www.udemy.com/sql-for-marketers-data-analytics-data-science-big-data\\n\\t\\n\\t\\n\\t\\nAre\\tyou\\tinterested\\tin\\tstock\\tprediction,\\ttime\\tseries,\\tand\\tsequences\\tin\\tgeneral?\\tMy\\nHidden\\tMarkov\\tModels\\tcourse\\tis\\twhere\\tyou\\twant\\tto\\tbe.\\tI\\tteach\\tyou\\tnot\\tonly\\tall\\nthe\\tclassical\\ttheory\\tof\\tHMMs,\\tbut\\tI\\talso\\tshow\\tyou\\thow\\tto\\twrite\\tthem\\tin\\tTheano\\nusing\\tgradient\\tdescent!\\tThis\\tis\\tgreat\\tpractice\\tfor\\twriting\\tdeep\\tlearning\\tmodels\\nand\\tit\\twill\\tprepare\\tyou\\twell\\tfor\\tits\\tsequel,\\tDeep\\tLearning\\tPart\\t5:\\tRecurrent\\nNeural\\tNetworks\\tin\\tPython.\\tYou\\tcan\\tget\\tthe\\tHMM\\tcourse\\there:\\n\\t\\n\\t\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a2c9e6ee-33be-49a1-8829-52b9e53db9ba', embedding=None, metadata={'page_label': '138', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\nUnsupervised\\tMachine\\tLearning:\\tHidden\\tMarkov\\tModels\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://udemy.com/unsupervised-machine-learning-hidden-markov-models-in-\\npython\\n\\t\\n\\t\\n\\t\\nRecurrent\\tNeural\\tNetworks,\\tthe\\ttopic\\tcovered\\tin\\tthis\\tbook,\\tis\\tcovered\\teven\\tmore\\nin-depth\\tin\\tthe\\tcorresponding\\tvideo\\tcourse:\\n\\t\\n\\t\\n\\t\\nDeep\\tLearning:\\tRecurrent\\tNeural\\tNetworks\\tin\\tPython\\n\\t\\n\\t\\n\\t\\nhttps://udemy.com/deep-learning-recurrent-neural-networks-in-python\\n\\t\\n\\t\\n\\t\\nFinally,\\tI\\tam\\t\\nalways\\n\\tgiving\\tout\\t\\ncoupons\\n\\tand\\tletting\\tyou\\tknow\\twhen\\tyou\\tcan\\tget\\nmy\\tstuff\\tfor\\t\\nfree\\n.\\tBut\\tyou\\tcan\\tonly\\tdo\\tthis\\tif\\tyou\\tare\\ta\\tcurrent\\tstudent\\tof\\tmine!\\nHere\\tare\\tsome\\tways\\tI\\tnotify\\tmy\\tstudents\\tabout\\tcoupons\\tand\\tfree\\tgiveaways:\\n\\t\\n\\t\\n\\t\\nMy\\tnewsletter,\\twhich\\tyou\\tcan\\tsign\\tup\\tfor\\tat\\t\\nhttp://lazyprogrammer.me\\n\\t(it\\tcomes\\nwith\\ta\\tfree\\t6-week\\tintro\\tto\\tmachine\\tlearning\\tcourse)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='db209855-500a-40f4-8861-56c39b403629', embedding=None, metadata={'page_label': '139', 'file_name': 'Deep Learning2.pdf', 'file_path': '/content/Documents/Documents/Deep Learning2.pdf', 'file_type': 'application/pdf', 'file_size': 695251, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\t\\n\\t\\nMy\\tTwitter,\\t\\nhttps://twitter.com/lazy_scientist\\n\\t\\n\\t\\n\\t\\nMy\\tFacebook\\tpage,\\t\\nhttps://facebook.com/lazyprogrammer.me\\n\\t(don’t\\tforget\\tto\\nhit\\t“like”!)\\n\\t', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8e7c1251-ab66-45d1-91d5-28d72096dae5', embedding=None, metadata={'page_label': 'i', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Foundations and TrendsR⃝ in Signal Processing\\nVol. 7, Nos. 3–4 (2013) 197–387\\nc⃝ 2014 L. Deng and D. Yu\\nDOI: 10.1561/2000000039\\nDeep Learning: Methods and Applications\\nLi Deng\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\ndeng@microsoft.com\\nDong Yu\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\nDong.Yu@microsoft.com', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b193acd0-e8da-4ed2-9ef9-c091af12e7fe', embedding=None, metadata={'page_label': 'ii', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents\\n1 Introduction 198\\n1 . 1 D e ﬁ n i t i o n s a n d b a c k g r o u n d.................198\\n1 . 2 O r g a n i z a t i o n o f t h i s m o n o g r a p h ..............202\\n2 Some Historical Context of Deep Learning 205\\n3 Three Classes of Deep Learning Networks 214\\n3 . 1 A t h r e e - w a y c a t e g o r i z a t i o n .................214\\n3.2 Deep networks for unsupervised or generative learning . . . 216\\n3 . 3 D e e p n e t w o r k s f o r s u p e r v i s e d l e a r n i n g ...........223\\n3 . 4 H y b r i d d e e p n e t w o r k s .................... 226\\n4 Deep Autoencoders — Unsupervised Learning 230\\n4 . 1 I n t r o d u c t i o n ......................... 230\\n4.2 Use of deep autoencoders to extract speech features . . . 231\\n4 . 3 S t a c k e d d e n o i s i n g a u t o e n c o d e r s ...............235\\n4 . 4 T r a n s f o r m i n g a u t o e n c o d e r s .................239\\n5 Pre-Trained Deep Neural Networks — A Hybrid 241\\n5 . 1 R e s t r i c t e d B o l t z m a n n m a c h i n e s ...............241\\n5 . 2 U n s u p e r v i s e d l a y e r - w i s e p r e - t r a i n i n g ............245\\n5 . 3 I n t e r f a c i n g D N N s w i t h H M M s ...............248\\nii', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c26bb918-4727-4871-874e-a044cefda844', embedding=None, metadata={'page_label': 'iii', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='iii\\n6 Deep Stacking Networks and Variants —\\nSupervised Learning 250\\n6 . 1 I n t r o d u c t i o n ......................... 250\\n6.2 A basic architecture of the deep stacking network . . . . . 252\\n6 . 3 A m e t h o d f o r l e a r n i n g t h e D S N w e i g h t s ..........254\\n6 . 4 T h e t e n s o r d e e p s t a c k i n g n e t w o r k ..............255\\n6 . 5 T h e K e r n e l i z e d d e e p s t a c k i n g n e t w o r k ...........257\\n7 Selected Applications in Sp eech and Audio Processing 262\\n7.1 Acoustic modeling for speech recognition . ......... 262\\n7.2 Speech synthesis . . . . ................... 286\\n7 . 3 A u d i o a n d m u s i c p r o c e s s i n g.................288\\n8 Selected Applications in Language\\nModeling and Natural Language Processing 292\\n8 . 1 L a n g u a g e m o d e l i n g ..................... 293\\n8 . 2 N a t u r a l l a n g u a g e p r o c e s s i n g .................299\\n9 Selected Applications in Information Retrieval 308\\n9 . 1 A b r i e f i n t r o d u c t i o n t o i n f o r m a t i o n r e t r i e v a l ........308\\n9 . 2 S H D A f o r d o c u m e n t i n d e x i n g a n d r e t r i e v a l .........310\\n9 . 3 D S S M f o r d o c u m e n t r e t r i e v a l................311\\n9.4 Use of deep stacking networks for information retrieval . . 317\\n10 Selected Applications in Object Recognition\\nand Computer Vision 320\\n10.1 Unsupervised or generative feature learning . . . . . . . . 321\\n1 0 . 2S u p e r v i s e d f e a t u r e l e a r n i n g a n d c l a s s i ﬁ c a t i o n ........324\\n11 Selected Applications in Multimodal\\nand Multi-task Learning 331\\n1 1 . 1M u l t i - m o d a l i t i e s : T e x t a n d i m a g e..............332\\n11.2 Multi-modalities: Speech and image . . . ......... 336\\n11.3 Multi-task learning within the speech, NLP or image . . . 339', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='10955309-715e-432f-9da7-1e0103e395e1', embedding=None, metadata={'page_label': 'iv', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='iv\\n12 Conclusion 343\\nReferences 349', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7b48e8ee-5c20-42c9-89a7-b03adfdfd40a', embedding=None, metadata={'page_label': '197', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Abstract\\nThis monogrph provides an overview of general deep learning method-\\nology and its applications to a variety of signal and information pro-\\ncessing tasks. The application areas are chosen with the following three\\ncriteria in mind: (1) expertise or knowledge of the authors; (2) the\\napplication areas that have already been transformed by the successful\\nuse of deep learning technology, such as speech recognition and com-\\nputer vision; and (3) the application areas that have the potential to be\\nimpacted signiﬁcantly by deep learning and that have been experienc-\\ning research growth, including natural language and text processing,\\ninformation retrieval, and multimodal information processing empow-\\nered by multi-task deep learning.\\nL. Deng and D. Yu. Deep Learning: Methods and Applications. Foundations and\\nTrendsR⃝ in Signal Processing, vol. 7, nos. 3–4, pp. 197–387, 2013.\\nDOI: 10.1561/2000000039.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a09938d1-c167-4bd6-8d26-6358d3c57de8', embedding=None, metadata={'page_label': '198', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1\\nIntroduction\\n1.1 Deﬁnitions and background\\nSince 2006, deep structured learning, or more commonly called deep\\nlearning or hierarchical learning, has emerged as a new area of machine\\nlearning research [20, 163]. During the past several years, the techniques\\ndeveloped from deep learning research have already been impacting\\na wide range of signal and information processing work within the\\ntraditional and the new, widened scopes including key aspects of\\nmachine learning and artiﬁcial intelligence; see overview articles in\\n[7, 20, 24, 77, 94, 161, 412], and also the media coverage of this progress\\nin [6, 237]. A series of workshops, tutorials, and special issues or con-\\nference special sessions in recent years have been devoted exclusively\\nto deep learning and its applications to various signal and information\\nprocessing areas. These include:\\n• 2008 NIPS Deep Learning Workshop;\\n• 2009 NIPS Workshop on Deep Learning for Speech Recognition\\nand Related Applications;\\n• 2009 ICML Workshop on Learning Feature Hierarchies;\\n198', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='def8efe2-d245-42e1-9514-f027d4579a74', embedding=None, metadata={'page_label': '199', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1. Deﬁnitions and background 199\\n• 2011 ICML Workshop on Learning Architectures, Representa-\\ntions, and Optimization for Speech and Visual Information Pro-\\ncessing;\\n• 2012 ICASSP Tutorial on Deep Learning for Signal and Informa-\\ntion Processing;\\n• 2012 ICML Workshop on Representation Learning;\\n• 2012 Special Section on Deep Learning for Speech and Language\\nProcessing in IEEE Transactions on Audio, Speech, and Lan-\\nguage Processing (T-ASLP, January);\\n• 2010, 2011, and 2012 NIPS Workshops on Deep Learning and\\nUnsupervised Feature Learning;\\n• 2013 NIPS Workshops on Deep Learning and on Output Repre-\\nsentation Learning;\\n• 2013 Special Issue on Learning Deep Architectures in IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(T-PAMI, September).\\n• 2013 International Conference on Learning Representations;\\n• 2013 ICML Workshop on Representation Learning Challenges;\\n• 2013 ICML Workshop on Deep Learning for Audio, Speech, and\\nLanguage Processing;\\n• 2013 ICASSP Special Session on New Types of Deep Neural Net-\\nwork Learning for Speech Recognition and Related Applications.\\nThe authors have been actively involved in deep learning research and\\nin organizing or providing several of the above events, tutorials, and\\neditorials. In particular, they gave tutorials and invited lectures on\\nthis topic at various places. Part of this monograph is based on their\\ntutorials and lecture material.\\nBefore embarking on describing details of deep learning, let’s pro-\\nvide necessary deﬁnitions. Deep learning has various closely related\\ndeﬁnitions or high-level descriptions:\\n• Deﬁnition 1 : A class of machine learning techniques that\\nexploit many layers of non-linear information processing for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ee4d06ad-fbf0-4606-a047-d22668c5e384', embedding=None, metadata={'page_label': '200', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='200 Introduction\\nsupervised or unsupervised feature extraction and transforma-\\ntion, and for pattern analysis and classiﬁcation.\\n• Deﬁnition 2: “A sub-ﬁeld within machine learning that is based\\non algorithms for learning multiple levels of representation in\\norder to model complex relationships among data. Higher-level\\nfeatures and concepts are thus deﬁned in terms of lower-level\\nones, and such a hierarchy of features is called a deep architec-\\nture. Most of these models are based on unsupervised learning of\\nrepresentations. ” (Wikipedia on “Deep Learning” around March\\n2012.)\\n• Deﬁnition 3 : “A sub-ﬁeld of machine learning that is based\\non learning several levels of representations, corresponding to a\\nhierarchy of features or factors or concepts, where higher-level\\nconcepts are deﬁned from lower-level ones, and the same lower-\\nlevel concepts can help to deﬁne many higher-level concepts. Deep\\nlearning is part of a broader family of machine learning methods\\nbased on learning representations. An observation (e.g., an image)\\ncan be represented in many ways (e.g., a vector of pixels), but\\nsome representations make it easier to learn tasks of interest (e.g.,\\nis this the image of a human face?) from examples, and research\\nin this area attempts to deﬁne what makes better representations\\nand how to learn them. ” (Wikipedia on “Deep Learning” around\\nFebruary 2013.)\\n• Deﬁnition 4 : “Deep learning is a set of algorithms in machine\\nlearning that attempt to learn in multiple levels, correspond-\\ning to diﬀerent levels of abstraction. It typically uses artiﬁcial\\nneural networks. The levels in these learned statistical models\\ncorrespond to distinct levels of concepts, where higher-level con-\\ncepts are deﬁned from lower-level ones, and the same lower-\\nlevel concepts can help to deﬁne many higher-level concepts. ”\\nSee Wikipedia http://en.wikipedia.org/wiki/Deep_learning on\\n“Deep Learning” as of this most recent update in October 2013.\\n• Deﬁnition 5: “Deep Learning is a new area of Machine Learning\\nresearch, which has been introduced with the objective of moving\\nMachine Learning closer to one of its original goals: Artiﬁcial', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c6cfcc71-b1d8-4153-a152-413b2d7f00af', embedding=None, metadata={'page_label': '201', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1. Deﬁnitions and background 201\\nIntelligence. Deep Learning is about learning multiple levels of\\nrepresentation and abstraction that help to make sense of data\\nsuch as images, sound, and text. ” See https: //github.com/lisa-\\nlab/DeepLearningTutorials\\nNote that the deep learning that we discuss in this monograph is\\nabout learning with deep architectures for signal and information pro-\\ncessing. It is not about deep understanding of the signal or infor-\\nmation, although in many cases they may be related. It should also\\nbe distinguished from the overloaded term in educational psychology:\\n“Deep learning describes an approach to learning that is character-\\nized by active engagement, intrinsic motivation, and a personal search\\nfor meaning. ” http://www.blackwellreference.com/public/tocnode?id=\\ng9781405161251_chunk_g97814051612516_ss1-1\\nCommon among the various high-level descriptions of deep learning\\nabove are two key aspects: (1) models consisting of multiple layers\\nor stages of nonlinear information processing; and (2) methods for\\nsupervised or unsupervised learning of feature representation at\\nsuccessively higher, more abstract layers. Deep learning is in the\\nintersections among the research areas of neural networks, artiﬁcial\\nintelligence, graphical modeling, optimization, pattern recognition,\\nand signal processing. Three important reasons for the popularity\\nof deep learning today are the drastically increased chip processing\\nabilities (e.g., general-purpose graphical processing units or GPGPUs),\\nthe signiﬁcantly increased size of data used for training, and the recent\\nadvances in machine learning and signal/information processing\\nresearch. These advances have enabled the deep learning methods\\nto eﬀectively exploit complex, compositional nonlinear functions, to\\nlearn distributed and hierarchical feature representations, and to make\\neﬀective use of both labeled and unlabeled data.\\nActive researchers in this area include those at University of\\nToronto, New York University, University of Montreal, Stanford\\nUniversity, Microsoft Research (since 2009), Google (since about\\n2011), IBM Research (since about 2011), Baidu (since 2012), Facebook\\n(since 2013), UC-Berkeley, UC-Irvine, IDIAP, IDSIA, University\\nCollege London, University of Michigan, Massachusetts Institute of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4d4cdd38-8e53-4149-b7f3-c40091bfbbc7', embedding=None, metadata={'page_label': '202', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='202 Introduction\\nTechnology, University of Washington, and numerous other places; see\\nhttp://deeplearning.net/deep-learning-research-groups-and-labs/ for\\na more detailed list. These researchers have demonstrated empirical\\nsuccesses of deep learning in diverse applications of computer vision,\\nphonetic recognition, voice search, conversational speech recognition,\\nspeech and image feature coding, semantic utterance classiﬁca-\\ntion, natural language understanding, hand-writing recognition, audio\\nprocessing, information retrieval, robotics, and even in the analysis of\\nmolecules that may lead to discovery of new drugs as reported recently\\nby [237].\\nIn addition to the reference list provided at the end of this mono-\\ngraph, which may be outdated not long after the publication of this\\nmonograph, there are a number of excellent and frequently updated\\nreading lists, tutorials, software, and video lectures online at:\\n• http://deeplearning.net/reading-list/\\n• http://uﬂdl.stanford.edu/wiki/index.php/\\nUFLDL_Recommended_Readings\\n• http://www.cs.toronto.edu/∼hinton/\\n• http://deeplearning.net/tutorial/\\n• http://uﬂdl.stanford.edu/wiki/index.php/UFLDL_Tutorial\\n1.2 Organization of this monograph\\nThe rest of the monograph is organized as follows:\\nIn Section 2, we provide a brief historical account of deep learning,\\nmainly from the perspective of how speech recognition technology has\\nbeen hugely impacted by deep learning, and how the revolution got\\nstarted and has gained and sustained immense momentum.\\nIn Section 3, a three-way categorization scheme for a majority of\\nthe work in deep learning is developed. They include unsupervised,\\nsupervised, and hybrid deep learning networks, where in the latter cat-\\negory unsupervised learning (or pre-training) is exploited to assist the\\nsubsequent stage of supervised learning when the ﬁnal tasks pertain to\\nclassiﬁcation. The supervised and hybrid deep networks often have the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1f53ca00-3037-4d24-9c75-b192eb586330', embedding=None, metadata={'page_label': '203', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2. Organization of this monograph 203\\nsame type of architectures or the structures in the deep networks, but\\nthe unsupervised deep networks tend to have diﬀerent architectures\\nfrom the others.\\nSections 4–6 are devoted, respectively, to three popular types of\\ndeep architectures, one from each of the classes in the three-way cat-\\negorization scheme reviewed in Section 3. In Section 4, we discuss\\nin detail deep autoencoders as a prominent example of the unsuper-\\nvised deep learning networks. No class labels are used in the learning,\\nalthough supervised learning methods such as back-propagation are\\ncleverly exploited when the input signal itself, instead of any label\\ninformation of interest to possible classiﬁcation tasks, is treated as the\\n“supervision” signal.\\nIn Section 5, as a major example in the hybrid deep network cate-\\ngory, we present in detail the deep neural networks with unsupervised\\nand largely generative pre-training to boost the eﬀectiveness of super-\\nvised training. This beneﬁt is found critical when the training data\\nare limited and no other appropriate regularization approaches (i.e.,\\ndropout) are exploited. The particular pre-training method based on\\nrestricted Boltzmann machines and the related deep belief networks\\ndescribed in this section has been historically signiﬁcant as it ignited\\nthe intense interest in the early applications of deep learning to speech\\nrecognition and other information processing tasks. In addition to this\\nretrospective review, subsequent development and diﬀerent paths from\\nthe more recent perspective are discussed.\\nIn Section 6, the basic deep stacking networks and their several\\nextensions are discussed in detail, which exemplify the discrimina-\\ntive, supervised deep learning networks in the three-way classiﬁcation\\nscheme. This group of deep networks operate in many ways that are\\ndistinct from the deep neural networks. Most notably, they use target\\nlabels in constructing each of many layers or modules in the overall\\ndeep networks. Assumptions made about part of the networks, such as\\nlinear output units in each of the modules, simplify the learning algo-\\nrithms and enable a much wider variety of network architectures to\\nbe constructed and learned than the networks discussed in Sections 4\\nand 5.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cf5c828d-0c42-476e-8ba1-505411a3d5e8', embedding=None, metadata={'page_label': '204', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='204 Introduction\\nIn Sections 7–11, we select a set of typical and successful applica-\\ntions of deep learning in diverse areas of signal and information process-\\ning. In Section 7, we review the applications of deep learning to speech\\nrecognition, speech synthesis, and audio processing. Subsections sur-\\nrounding the main subject of speech recognition are created based on\\nseveral prominent themes on the topic in the literature.\\nIn Section 8, we present recent results of applying deep learning to\\nlanguage modeling and natural language processing, where we highlight\\nthe key recent development in embedding symbolic entities such as\\nwords into low-dimensional, continuous-valued vectors.\\nSection 9 is devoted to selected applications of deep learning to\\ninformation retrieval including web search.\\nIn Section 10, we cover selected applications of deep learning to\\nimage object recognition in computer vision. The section is divided to\\ntwo main classes of deep learning approaches: (1) unsupervised feature\\nlearning, and (2) supervised learning for end-to-end and joint feature\\nlearning and classiﬁcation.\\nSelected applications to multi-modal processing and multi-task\\nlearning are reviewed in Section 11, divided into three categories\\naccording to the nature of the multi-modal data as inputs to the deep\\nlearning systems. For single-modality data of speech, text, or image,\\na number of recent multi-task learning studies based on deep learning\\nmethods are reviewed in the literature.\\nFinally, conclusions are given in Section 12 to summarize the mono-\\ngraph and to discuss future challenges and directions.\\nThis short monograph contains the material expanded from two\\ntutorials that the authors gave, one at APSIPA in October 2011 and\\nthe other at ICASSP in March 2012. Substantial updates have been\\nmade based on the literature up to January 2014 (including the mate-\\nrials presented at NIPS-2013 and at IEEE-ASRU-2013 both held in\\nDecember of 2013), focusing on practical aspects in the fast develop-\\nment of deep learning research and technology during the interim years.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='24901eb8-2e79-448f-aa14-3bb8962e2d27', embedding=None, metadata={'page_label': '205', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2\\nSome Historical Context of Deep Learning\\nUntil recently, most machine learning and signal processing techniques\\nhad exploited shallow-structured architectures. These architectures\\ntypically contain at most one or two layers of nonlinear feature transfor-\\nmations. Examples of the shallow architectures are Gaussian mixture\\nmodels (GMMs), linear or nonlinear dynamical systems, conditional\\nrandom ﬁelds (CRFs), maximum entropy (MaxEnt) models, support\\nvector machines (SVMs), logistic regression, kernel regression, multi-\\nlayer perceptrons (MLPs) with a single hidden layer including extreme\\nlearning machines (ELMs). For instance, SVMs use a shallow linear\\npattern separation model with one or zero feature transformation layer\\nwhen the kernel trick is used or otherwise. (Notable exceptions are the\\nrecent kernel methods that have been inspired by and integrated with\\ndeep learning; e.g. [9, 53, 102, 377]). Shallow architectures have been\\nshown eﬀective in solving many simple or well-constrained problems,\\nbut their limited modeling and representational power can cause dif-\\nﬁculties when dealing with more complicated real-world applications\\ninvolving natural signals such as human speech, natural sound and\\nlanguage, and natural image and visual scenes.\\n205', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d458b5aa-64d9-46aa-829c-f5524a3d0ce1', embedding=None, metadata={'page_label': '206', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='206 Some Historical Context of Deep Learning\\nHuman information processing mechanisms (e.g., vision and audi-\\ntion), however, suggest the need of deep architectures for extracting\\ncomplex structure and building internal representation from rich sen-\\nsory inputs. For example, human speech production and perception\\nsystems are both equipped with clearly layered hierarchical structures\\nin transforming the information from the waveform level to the linguis-\\ntic level [11, 12, 74, 75]. In a similar vein, the human visual system is\\nalso hierarchical in nature, mostly in the perception side but interest-\\ningly also in the “generation” side [43, 126, 287]). It is natural to believe\\nthat the state-of-the-art can be advanced in processing these types of\\nnatural signals if eﬃcient and eﬀective deep learning algorithms can be\\ndeveloped.\\nHistorically, the concept of deep learning originated from artiﬁ-\\ncial neural network research. (Hence, one may occasionally hear the\\ndiscussion of “new-generation neural networks. ”) Feed-forward neural\\nnetworks or MLPs with many hidden layers, which are often referred\\nto as deep neural networks (DNNs), are good examples of the models\\nwith a deep architecture. Back-propagation (BP), popularized in 1980s,\\nhas been a well-known algorithm for learning the parameters of these\\nnetworks. Unfortunately BP alone did not work well in practice then\\nfor learning networks with more than a small number of hidden layers\\n(see a review and analysis in [20, 129]. The pervasive presence of local\\noptima and other optimization challenges in the non-convex objective\\nfunction of the deep networks are the main source of diﬃculties in the\\nlearning. BP is based on local gradient information, and starts usu-\\nally at some random initial points. It often gets trapped in poor local\\noptima when the batch-mode or even stochastic gradient descent BP\\nalgorithm is used. The severity increases signiﬁcantly as the depth of\\nthe networks increases. This diﬃculty is partially responsible for steer-\\ning away most of the machine learning and signal processing research\\nfrom neural networks to shallow models that have convex loss func-\\ntions (e.g., SVMs, CRFs, and MaxEnt models), for which the global\\noptimum can be eﬃciently obtained at the cost of reduced modeling\\npower, although there had been continuing work on neural networks\\nwith limited scale and impact (e.g., [42, 45, 87, 168, 212, 263, 304].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e41191b9-17d4-4712-b4c4-28b520a9d426', embedding=None, metadata={'page_label': '207', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='207\\nThe optimization diﬃculty associated with the deep models was\\nempirically alleviated when a reasonably eﬃcient, unsupervised learn-\\ning algorithm was introduced in the two seminar papers [163, 164].\\nIn these papers, a class of deep generative models, called deep belief\\nnetwork (DBN), was introduced. A DBN is composed of a stack of\\nrestricted Boltzmann machines (RBMs). A core component of the\\nDBN is a greedy, layer-by-layer learning algorithm which optimizes\\nDBN weights at time complexity linear to the size and depth of the\\nnetworks. Separately and with some surprise, initializing the weights\\nof an MLP with a correspondingly conﬁgured DBN often produces\\nmuch better results than that with the random weights. As such,\\nMLPs with many hidden layers, or deep neural networks (DNN),\\nwhich are learned with unsupervised DBN pre-training followed by\\nback-propagation ﬁne-tuning is sometimes also called DBNs in the\\nliterature [67, 260, 258]. More recently, researchers have been more\\ncareful in distinguishing DNNs from DBNs [68, 161], and when DBN\\nis used to initialize the training of a DNN, the resulting network is\\nsometimes called the DBN–DNN [161].\\nIndependently of the RBM development, in 2006 two alternative,\\nnon-probabilistic, non-generative, unsupervised deep models were pub-\\nlished. One is an autoencoder variant with greedy layer-wise training\\nmuch like the DBN training [28]. Another is an energy-based model\\nwith unsupervised learning of sparse over-complete representations\\n[297]. They both can be eﬀectively used to pre-train a deep neural\\nnetwork, much like the DBN.\\nIn addition to the supply of good initialization points, the DBN\\ncomes with other attractive properties. First, the learning algorithm\\nmakes eﬀective use of unlabeled data. Second, it can be interpreted\\nas a probabilistic generative model. Third, the over-ﬁtting problem,\\nwhich is often observed in the models with millions of parameters such\\nas DBNs, and the under-ﬁtting problem, which occurs often in deep\\nnetworks, can be eﬀectively alleviated by the generative pre-training\\nstep. An insightful analysis on what kinds of speech information DBNs\\ncan capture is provided in [259].\\nUsing hidden layers with many neurons in a DNN signiﬁcantly\\nimproves the modeling power of the DNN and creates many closely', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3c263e62-5266-49db-a1a4-57f03a694596', embedding=None, metadata={'page_label': '208', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='208 Some Historical Context of Deep Learning\\noptimal conﬁgurations. Even if parameter learning is trapped into a\\nlocal optimum, the resulting DNN can still perform quite well since\\nthe chance of having a poor local optimum is lower than when a small\\nnumber of neurons are used in the network. Using deep and wide neu-\\nral networks, however, would cast great demand to the computational\\npower during the training process and this is one of the reasons why it\\nis not until recent years that researchers have started exploring both\\ndeep and wide neural networks in a serious manner.\\nBetter learning algorithms and diﬀerent nonlinearities also con-\\ntributed to the success of DNNs. Stochastic gradient descend (SGD)\\nalgorithms are the most eﬃcient algorithm when the training set is large\\nand redundant as is the case for most applications [39]. Recently, SGD is\\nshown to be eﬀective for parallelizing over many machines with an asyn-\\nchronous mode [69] or over multiple GPUs through pipelined BP [49].\\nFurther, SGD can often allow the training to jump out of local optima\\ndue to the noisy gradients estimated from a single or a small batch of\\nsamples. Other learning algorithms such as Hessian free [195, 238] or\\nKrylov subspace methods [378] have shown a similar ability.\\nFor the highly non-convex optimization problem of DNN learn-\\ning, it is obvious that better parameter initialization techniques will\\nlead to better models since optimization starts from these initial mod-\\nels. What was not obvious, however, is how to eﬃciently and eﬀec-\\ntively initialize DNN parameters and how the use of large amounts of\\ntraining data can alleviate the learning problem until more recently\\n[28, 20, 100, 64, 68, 163, 164, 161, 323, 376, 414]. The DNN parameter\\ninitialization technique that attracted the most attention is the unsu-\\npervised pretraining technique proposed in [163, 164] discussed earlier.\\nThe DBN pretraining procedure is not the only one that allows\\neﬀective initialization of DNNs. An alternative unsupervised approach\\nthat performs equally well is to pretrain DNNs layer by layer by con-\\nsidering each pair of layers as a de-noising autoencoder regularized by\\nsetting a random subset of the input nodes to zero [20, 376]. Another\\nalternative is to use contractive autoencoders for the same purpose by\\nfavoring representations that are more robust to the input variations,\\ni.e., penalizing the gradient of the activities of the hidden units with\\nrespect to the inputs [303]. Further, Ranzato et al. [294] developed the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='18756ca0-8779-4c9b-a30a-e70d5adbea66', embedding=None, metadata={'page_label': '209', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='209\\nsparse encoding symmetric machine (SESM), which has a very similar\\narchitecture to RBMs as building blocks of a DBN. The SESM may also\\nbe used to eﬀectively initialize the DNN training. In addition to unsu-\\npervised pretraining using greedy layer-wise procedures [28, 164, 295],\\nthe supervised pretraining, or sometimes called discriminative pretrain-\\ning, has also been shown to be eﬀective [28, 161, 324, 432] and in cases\\nwhere labeled training data are abundant performs better than the\\nunsupervised pretraining techniques. The idea of the discriminative\\npretraining is to start from a one-hidden-layer MLP trained with the\\nBP algorithm. Every time when we want to add a new hidden layer we\\nreplace the output layer with a randomly initialized new hidden and\\noutput layer and train the whole new MLP (or DNN) using the BP\\nalgorithm. Diﬀerent from the unsupervised pretraining techniques, the\\ndiscriminative pretraining technique requires labels.\\nResearchers who apply deep learning to speech and vision analyzed\\nwhat DNNs capture in speech and images. For example, [259] applied\\na dimensionality reduction method to visualize the relationship among\\nthe feature vectors learned by the DNN. They found that the DNN’s\\nhidden activity vectors preserve the similarity structure of the feature\\nvectors at multiple scales, and that this is especially true for the ﬁl-\\nterbank features. A more elaborated visualization method, based on\\na top-down generative process in the reverse direction of the classi-\\nﬁcation network, was recently developed by Zeiler and Fergus [436]\\nfor examining what features the deep convolutional networks capture\\nfrom the image data. The power of the deep networks is shown to\\nbe their ability to extract appropriate features and do discrimination\\njointly [210].\\nAs another way to concisely introduce the DNN, we can review the\\nhistory of artiﬁcial neural networks using a “hype cycle,” which is a\\ngraphic representation of the maturity, adoption and social applica-\\ntion of speciﬁc technologies. The 2012 version of the hype cycles graph\\ncompiled by Gartner is shown in Figure 2.1. It intends to show how\\na technology or application will evolve over time (according to ﬁve\\nphases: technology trigger, peak of inﬂated expectations, trough of dis-\\nillusionment, slope of enlightenment, and plateau of production), and\\nto provide a source of insight to manage its deployment.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='99f1d319-ad91-40f0-9d41-f486612e578e', embedding=None, metadata={'page_label': '210', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='210 Some Historical Context of Deep Learning\\nFigure 2.1: Gartner hyper cycle graph representing ﬁve phases of a technology\\n(http://en.wikipedia.org/wiki/Hype_cycle).\\nApplying the Gartner hyper cycle to the artiﬁcial neural network\\ndevelopment, we created Figure 2.2 to align diﬀerent generations of\\nthe neural network with the various phases designated in the hype\\ncycle. The peak activities (“expectations” or “media hype” on the ver-\\ntical axis) occurred in late 1980s and early 1990s, corresponding to the\\nheight of what is often referred to as the “second generation” of neu-\\nral networks. The deep belief network (DBN) and a fast algorithm for\\ntraining it were invented in 2006 [163, 164]. When the DBN was used\\nto initialize the DNN, the learning became highly eﬀective and this has\\ninspired the subsequent fast growing research (“enlightenment” phase\\nshown in Figure 2.2). Applications of the DBN and DNN to industry-\\nscale speech feature extraction and speech recognition started in 2009\\nwhen leading academic and industrial researchers with both deep learn-\\ning and speech expertise collaborated; see reviews in [89, 161]. This\\ncollaboration fast expanded the work of speech recognition using deep\\nlearning methods to increasingly larger successes [94, 161, 323, 414],', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1b7a37a4-14b2-45fb-937a-bdf6cfda39a8', embedding=None, metadata={'page_label': '211', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='211\\nFigure 2.2:Applying Gartner hyper cycle graph to analyzing the history of artiﬁcial\\nneural network technology (We thank our colleague John Platt during 2012 for\\nbringing this type of “Hyper Cycle” graph to our attention for concisely analyzing\\nthe neural network history).\\nmany of which will be covered in the remainder of this monograph.\\nThe height of the “plateau of productivity” phase, not yet reached in\\nour opinion, is expected to be higher than that in the stereotypical\\ncurve (circled with a question mark in Figure 2.2), and is marked by\\nthe dashed line that moves straight up.\\nWe show in Figure 2.3 the history of speech recognition, which\\nhas been compiled by NIST, organized by plotting the word error rate\\n(WER) as a function of time for a number of increasingly diﬃcult\\nspeech recognition tasks. Note all WER results were obtained using the\\nGMM–HMM technology. When one particularly diﬃcult task (Switch-\\nboard) is extracted from Figure 2.3, we see a ﬂat curve over many\\nyears using the GMM–HMM technology but after the DNN technology\\nis used the WER drops sharply (marked by the red star in Figure 2.4).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4951d55a-fdc9-47f2-abff-d0a683e99bcf', embedding=None, metadata={'page_label': '212', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='212 Some Historical Context of Deep Learning\\nFigure 2.3: The famous NIST plot showing the historical speech recognition error\\nrates achieved by the GMM-HMM approach for a number of increasingly diﬃcult\\nspeech recognition tasks. Data source: http://itl.nist.gov/iad/mig/publications/\\nASRhistory/index.html\\nFigure 2.4: Extracting WERs of one task from Figure 2.3 and adding the signiﬁ-\\ncantly lower WER (marked by the star) achieved by the DNN technology.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='347c04e5-aaa4-4ec6-9d88-80ce4c7efaf4', embedding=None, metadata={'page_label': '213', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='213\\nIn the next section, an overview is provided on the various architec-\\ntures of deep learning, followed by more detailed expositions of a few\\nwidely studied architectures and methods and by selected applications\\nin signal and information processing including speech and audio, natu-\\nral language, information retrieval, vision, and multi-modal processing.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1a81d0c8-2c32-448b-a024-5e22e697a48d', embedding=None, metadata={'page_label': '214', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3\\nThree Classes of Deep Learning Networks\\n3.1 A three-way categorization\\nAs described earlier, deep learning refers to a rather wide class of\\nmachine learning techniques and architectures, with the hallmark\\nof using many layers of non-linear information processing that are\\nhierarchical in nature. Depending on how the architectures and tech-\\nniques are intended for use, e.g., synthesis/generation or recognition/\\nclassiﬁcation, one can broadly categorize most of the work in this area\\ninto three major classes:\\n1. Deep networks for unsupervised or generative learn-\\ning, which are intended to capture high-order correlation of the\\nobserved or visible data for pattern analysis or synthesis purposes\\nwhen no information about target class labels is available. Unsu-\\npervised feature or representation learning in the literature refers\\nto this category of the deep networks. When used in the genera-\\ntive mode, may also be intended to characterize joint statistical\\ndistributions of the visible data and their associated classes when\\navailable and being treated as part of the visible data. In the\\n214', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='25e28546-bf30-4b77-ad11-1a4a35d796d8', embedding=None, metadata={'page_label': '215', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. A three-way categorization 215\\nlatter case, the use of Bayes rule can turn this type of generative\\nnetworks into a discriminative one for learning.\\n2. Deep networks for supervised learning, which are intended\\nto directly provide discriminative power for pattern classiﬁca-\\ntion purposes, often by characterizing the posterior distributions\\nof classes conditioned on the visible data. Target label data are\\nalways available in direct or indirect forms for such supervised\\nlearning. They are also called discriminative deep networks.\\n3. Hybrid deep networks, where the goal is discrimination which\\nis assisted, often in a signiﬁcant way, with the outcomes of genera-\\ntive or unsupervised deep networks. This can be accomplished by\\nbetter optimization or/and regularization of the deep networks\\nin category (2). The goal can also be accomplished when discrim-\\ninative criteria for supervised learning are used to estimate the\\nparameters in any of the deep generative or unsupervised deep\\nnetworks in category (1) above.\\nNote the use of “hybrid” in (3) above is diﬀerent from that used\\nsometimes in the literature, which refers to the hybrid systems for\\nspeech recognition feeding the output probabilities of a neural network\\ninto an HMM [17, 25, 42, 261].\\nBy the commonly adopted machine learning tradition (e.g.,\\nChapter 28 in [264], and Reference [95], it may be natural to just clas-\\nsify deep learning techniques into deep discriminative models (e.g., deep\\nneural networks or DNNs, recurrent neural networks or RNNs, convo-\\nlutional neural networks or CNNs, etc.) and generative/unsupervised\\nmodels (e.g., restricted Boltzmann machine or RBMs, deep belief\\nnetworks or DBNs, deep Boltzmann machines (DBMs), regularized\\nautoencoders, etc.). This two-way classiﬁcation scheme, however,\\nmisses a key insight gained in deep learning research about how gener-\\native or unsupervised-learning models can greatly improve the training\\nof DNNs and other deep discriminative or supervised-learning mod-\\nels via better regularization or optimization. Also, deep networks for\\nunsupervised learning may not necessarily need to be probabilistic or be\\nable to meaningfully sample from the model (e.g., traditional autoen-\\ncoders, sparse coding networks, etc.). We note here that more recent', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e78c325b-ca39-4623-bab4-9226f84a054e', embedding=None, metadata={'page_label': '216', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='216 Three Classes of Deep Learning Networks\\nstudies have generalized the traditional denoising autoencoders so that\\nthey can be eﬃciently sampled from and thus have become genera-\\ntive models [5, 24, 30]. Nevertheless, the traditional two-way classiﬁ-\\ncation indeed points to several key diﬀerences between deep networks\\nfor unsupervised and supervised learning. Compared between the two,\\ndeep supervised-learning models such as DNNs are usually more eﬃ-\\ncient to train and test, more ﬂexible to construct, and more suitable for\\nend-to-end learning of complex systems (e.g., no approximate inference\\nand learning such as loopy belief propagation). On the other hand, the\\ndeep unsupervised-learning models, especially the probabilistic gener-\\native ones, are easier to interpret, easier to embed domain knowledge,\\neasier to compose, and easier to handle uncertainty, but they are typi-\\ncally intractable in inference and learning for complex systems. These\\ndistinctions are retained also in the proposed three-way classiﬁcation\\nwhich is hence adopted throughout this monograph.\\nBelow we review representative work in each of the above three\\ncategories, where several basic deﬁnitions are summarized in Table 3.1.\\nApplications of these deep architectures, with varied ways of learn-\\ning including supervised, unsupervised, or hybrid, are deferred to Sec-\\ntions 7–11.\\n3.2 Deep networks for unsupervised or generative learning\\nUnsupervised learning refers to no use of task speciﬁc supervision infor-\\nmation (e.g., target class labels) in the learning process. Many deep net-\\nworks in this category can be used to meaningfully generate samples by\\nsampling from the networks, with examples of RBMs, DBNs, DBMs,\\nand generalized denoising autoencoders [23], and are thus generative\\nmodels. Some networks in this category, however, cannot be easily sam-\\npled, with examples of sparse coding networks and the original forms\\nof deep autoencoders, and are thus not generative in nature.\\nAmong the various subclasses of generative or unsupervised deep\\nnetworks, the energy-based deep models are the most common [28, 20,\\n213, 268]. The original form of the deep autoencoder [28, 100, 164],\\nwhich we will give more detail about in Section 4, is a typical example', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='928579f4-d4a5-4c5a-b3a8-1ba0961f8ffb', embedding=None, metadata={'page_label': '217', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Deep networks for unsupervised or generative learning 217\\nTable 3.1:Basic deep learning terminologies.\\nDeep Learning:a class of machine learning techniques, where many\\nlayers of information processing stages in hierarchical supervised\\narchitectures are exploited for unsupervised feature learning and for\\npattern analysis/classiﬁcation. The essence of deep learning is to\\ncompute hierarchical features or representations of the observational\\ndata, where the higher-level features or factors are deﬁned from\\nlower-level ones. The family of deep learning methods have been\\ngrowing increasingly richer, encompassing those of neural networks,\\nhierarchical probabilistic models, and a variety of unsupervised and\\nsupervised feature learning algorithms.\\nDeep belief network (DBN): probabilistic generative models\\ncomposed of multiple layers of stochastic, hidden variables. The top\\ntwo layers have undirected, symmetric connections between them.\\nThe lower layers receive top-down, directed connections from the\\nlayer above.\\nBoltzmann machine (BM): a network of symmetrically connected,\\nneuron-like units that make stochastic decisions about whether to be\\non or oﬀ.\\nRestricted Boltzmann machine (RBM): a special type of BM\\nconsisting of a layer of visible units and a layer of hidden units with\\nno visible-visible or hidden-hidden connections.\\nDeep neural network (DNN): a multilayer perceptron with many\\nhidden layers, whose weights are fully connected and are often\\n(although not always) initialized using either an unsupervised or a\\nsupervised pretraining technique. (In the literature prior to 2012, a\\nDBN was often used incorrectly to mean a DNN.)\\nDeep autoencoder: a “discriminative” DNN whose output targets\\nare the data input itself rather than class labels; hence an\\nunsupervised learning model. When trained with a denoising\\ncriterion, a deep autoencoder is also a generative model and can be\\nsampled from.\\n(Continued)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a02bce63-ff93-4d4b-ab15-7c9b464a5e90', embedding=None, metadata={'page_label': '218', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='218 Three Classes of Deep Learning Networks\\nTable 3.1:(Continued)\\nDistributed representation:a ni n t e r n a lr e p r e s e n t a t i o no ft h e\\nobserved data in such a way that they are modeled as being explained\\nby the interactions of many hidden factors. A particular factor\\nlearned from conﬁgurations of other factors can often generalize well\\nto new conﬁgurations. Distributed representations naturally occur in\\na “connectionist” neural network, where a concept is represented by a\\npattern of activity across a number of units and where at the same\\ntime a unit typically contributes to many concepts. One key\\nadvantage of such many-to-many correspondence is that they provide\\nrobustness in representing the internal structure of the data in terms\\nof graceful degradation and damage resistance. Another key\\nadvantage is that they facilitate generalizations of concepts and\\nrelations, thus enabling reasoning abilities.\\nof this unsupervised model category. Most other forms of deep autoen-\\ncoders are also unsupervised in nature, but with quite diﬀerent prop-\\nerties and implementations. Examples are transforming autoencoders\\n[160], predictive sparse coders and their stacked version, and de-noising\\nautoencoders and their stacked versions [376].\\nSpeciﬁcally, in de-noising autoencoders, the input vectors are ﬁrst\\ncorrupted by, for example, randomly selecting a percentage of the\\ninputs and setting them to zeros or adding Gaussian noise to them.\\nThen the parameters are adjusted for the hidden encoding nodes to\\nreconstruct the original, uncorrupted input data using criteria such as\\nmean square reconstruction error and KL divergence between the orig-\\ninal inputs and the reconstructed inputs. The encoded representations\\ntransformed from the uncorrupted data are used as the inputs to the\\nnext level of the stacked de-noising autoencoder.\\nAnother prominent type of deep unsupervised models with genera-\\ntive capability is the deep Boltzmann machine or DBM [131, 315, 316,\\n348]. A DBM contains many layers of hidden variables, and has no con-\\nnections between the variables within the same layer. This is a special\\ncase of the general Boltzmann machine (BM), which is a network of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9a53ba91-975a-4ff1-a4e0-db1698088cf5', embedding=None, metadata={'page_label': '219', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Deep networks for unsupervised or generative learning 219\\nsymmetrically connected units that are on or oﬀ based on a stochastic\\nmechanism. While having a simple learning algorithm, the general BMs\\nare very complex to study and very slow to train. In a DBM, each layer\\ncaptures complicated, higher-order correlations between the activities\\nof hidden features in the layer below. DBMs have the potential of learn-\\ning internal representations that become increasingly complex, highly\\ndesirable for solving object and speech recognition problems. Further,\\nthe high-level representations can be built from a large supply of unla-\\nbeled sensory inputs and very limited labeled data can then be used to\\nonly slightly ﬁne-tune the model for a speciﬁc task at hand.\\nWhen the number of hidden layers of DBM is reduced to one, we\\nhave restricted Boltzmann machine (RBM). Like DBM, there are no\\nhidden-to-hidden and no visible-to-visible connections in the RBM. The\\nmain virtue of RBM is that via composing many RBMs, many hidden\\nlayers can be learned eﬃciently using the feature activations of one\\nRBM as the training data for the next. Such composition leads to deep\\nbelief network (DBN), which we will describe in more detail, together\\nwith RBMs, in Section 5.\\nThe standard DBN has been extended to the factored higher-order\\nBoltzmann machine in its bottom layer, with strong results obtained\\nfor phone recognition [64] and for computer vision [296]. This model,\\ncalled the mean-covariance RBM or mcRBM, recognizes the limitation\\nof the standard RBM in its ability to represent the covariance structure\\nof the data. However, it is diﬃcult to train mcRBMs and to use them\\nat the higher levels of the deep architecture. Further, the strong results\\npublished are not easy to reproduce. In the architecture described by\\nDahl et al. [64], the mcRBM parameters in the full DBN are not ﬁne-\\ntuned using the discriminative information, which is used for ﬁne tuning\\nthe higher layers of RBMs, due to the high computational cost. Subse-\\nquent work showed that when speaker adapted features are used, which\\nremove more variability in the features, mcRBM was not helpful [259].\\nAnother representative deep generative network that can be used\\nfor unsupervised (as well as supervised) learning is the sum–product\\nnetwork or SPN [125, 289]. An SPN is a directed acyclic graph with\\nthe observed variables as leaves, and with sum and product operations\\nas internal nodes in the deep network. The “sum” nodes give mixture', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8aa2e3c0-8127-40c2-89c9-71d4b357ee25', embedding=None, metadata={'page_label': '220', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='220 Three Classes of Deep Learning Networks\\nmodels, and the “product” nodes build up the feature hierarchy. Prop-\\nerties of “completeness” and “consistency” constrain the SPN in a desir-\\nable way. The learning of SPNs is carried out using the EM algorithm\\ntogether with back-propagation. The learning procedure starts with a\\ndense SPN. It then ﬁnds an SPN structure by learning its weights,\\nwhere zero weights indicate removed connections. The main diﬃculty\\nin learning SPNs is that the learning signal (i.e., the gradient) quickly\\ndilutes when it propagates to deep layers. Empirical solutions have been\\nfound to mitigate this diﬃculty as reported in [289]. It was pointed\\nout in that early paper that despite the many desirable generative\\nproperties in the SPN, it is diﬃcult to ﬁne tune the parameters using\\nthe discriminative information, limiting its eﬀectiveness in classiﬁca-\\ntion tasks. However, this diﬃculty has been overcome in the subse-\\nquent work reported in [125], where an eﬃcient BP-style discriminative\\ntraining algorithm for SPN was presented. Importantly, the standard\\ngradient descent, based on the derivative of the conditional likelihood,\\nsuﬀers from the same gradient diﬀusion problem well known in the\\nregular DNNs. The trick to alleviate this problem in learning SPNs\\nis to replace the marginal inference with the most probable state of\\nthe hidden variables and to propagate gradients through this “hard”\\nalignment only. Excellent results on small-scale image recognition tasks\\nwere reported by Gens and Domingo [125].\\nRecurrent neural networks (RNNs) can be considered as another\\nclass of deep networks for unsupervised (as well as supervised) learning,\\nwhere the depth can be as large as the length of the input data sequence.\\nIn the unsupervised learning mode, the RNN is used to predict the data\\nsequence in the future using the previous data samples, and no addi-\\ntional class information is used for learning. The RNN is very powerful\\nfor modeling sequence data (e.g., speech or text), but until recently\\nthey had not been widely used partly because they are diﬃcult to train\\nto capture long-term dependencies, giving rise to gradient vanishing or\\ngradient explosion problems which were known in early 1990s [29, 167].\\nThese problems can now be dealt with more easily [24, 48, 85, 280].\\nRecent advances in Hessian-free optimization [238] have also partially\\novercome this diﬃculty using approximated second-order information\\nor stochastic curvature estimates. In the more recent work [239], RNNs', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f3b6db90-f58b-4bf1-b6ce-790b41cba703', embedding=None, metadata={'page_label': '221', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Deep networks for unsupervised or generative learning 221\\nthat are trained with Hessian-free optimization are used as a genera-\\ntive deep network in the character-level language modeling tasks, where\\ngated connections are introduced to allow the current input characters\\nto predict the transition from one latent state vector to the next. Such\\ngenerative RNN models are demonstrated to be well capable of gener-\\nating sequential text characters. More recently, Bengio et al. [22] and\\nSutskever [356] have explored variations of stochastic gradient descent\\noptimization algorithms in training generative RNNs and shown that\\nthese algorithms can outperform Hessian-free optimization methods.\\nMikolov et al. [248] have reported excellent results on using RNNs for\\nlanguage modeling. Most recently, Mesnil et al. [242] and Yao et al.\\n[403] reported the success of RNNs in spoken language understanding.\\nWe will review this set of work in Section 8.\\nThere has been a long history in speech recognition research\\nwhere human speech production mechanisms are exploited to con-\\nstruct dynamic and deep structure in probabilistic generative models;\\nfor a comprehensive review, see the monograph by Deng [76]. Specif-\\nically, the early work described in [71, 72, 83, 84, 99, 274] generalized\\nand extended the conventional shallow and conditionally independent\\nHMM structure by imposing dynamic constraints, in the form of poly-\\nnomial trajectory, on the HMM parameters. A variant of this approach\\nhas been more recently developed using diﬀerent learning techniques\\nfor time-varying HMM parameters and with the applications extended\\nto speech recognition robustness [431, 416]. Similar trajectory HMMs\\nalso form the basis for parametric speech synthesis [228, 326, 439, 438].\\nSubsequent work added a new hidden layer into the dynamic model to\\nexplicitly account for the target-directed, articulatory-like properties in\\nhuman speech generation [45, 73, 74, 83, 96, 75, 90, 231, 232, 233, 251,\\n282]. More eﬃcient implementation of this deep architecture with hid-\\nden dynamics is achieved with non-recursive or ﬁnite impulse response\\n(FIR) ﬁlters in more recent studies [76, 107, 105]. The above deep-\\nstructured generative models of speech can be shown as special cases\\nof the more general dynamic network model and even more general\\ndynamic graphical models [35, 34]. The graphical models can comprise\\nmany hidden layers to characterize the complex relationship between\\nthe variables in speech generation. Armed with powerful graphical', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8c3bbbc8-1cfb-4674-ac5e-391a05015b53', embedding=None, metadata={'page_label': '222', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='222 Three Classes of Deep Learning Networks\\nmodeling tool, the deep architecture of speech has more recently been\\nsuccessfully applied to solve the very diﬃcult problem of single-channel,\\nmulti-talker speech recognition, where the mixed speech is the visible\\nvariable while the un-mixed speech becomes represented in a new hid-\\nden layer in the deep generative architecture [301, 391]. Deep generative\\ngraphical models are indeed a powerful tool in many applications due\\nto their capability of embedding domain knowledge. However, they are\\noften used with inappropriate approximations in inference, learning,\\nprediction, and topology design, all arising from inherent intractability\\nin these tasks for most real-world applications. This problem has been\\naddressed in the recent work of Stoyanov et al. [352], which provides\\nan interesting direction for making deep generative graphical models\\npotentially more useful in practice in the future. An even more drastic\\nway to deal with this intractability was proposed recently by Bengio\\net al. [30], where the need to marginalize latent variables is avoided\\naltogether.\\nThe standard statistical methods used for large-scale speech recog-\\nnition and understanding combine (shallow) hidden Markov models\\nfor speech acoustics with higher layers of structure representing dif-\\nferent levels of natural language hierarchy. This combined hierarchical\\nmodel can be suitably regarded as a deep generative architecture, whose\\nmotivation and some technical detail may be found in Section 7 of the\\nrecent monograph [200] on “Hierarchical HMM” or HHMM. Related\\nmodels with greater technical depth and mathematical treatment can\\nbe found in [116] for HHMM and [271] for Layered HMM. These early\\ndeep models were formulated as directed graphical models, missing the\\nkey aspect of “distributed representation” embodied in the more recent\\ndeep generative networks of the DBN and DBM discussed earlier in this\\nchapter. Filling in this missing aspect would help improve these gener-\\native models.\\nFinally, dynamic or temporally recursive generative models based\\non neural network architectures can be found in [361] for human motion\\nmodeling, and in [344, 339] for natural language and natural scene pars-\\ning. The latter model is particularly interesting because the learning\\nalgorithms are capable of automatically determining the optimal model\\nstructure. This contrasts with other deep architectures such as DBN', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9b696830-6f6e-4beb-b6f2-8cb4f0086a89', embedding=None, metadata={'page_label': '223', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3. Deep networks for supervised learning 223\\nwhere only the parameters are learned while the architectures need to\\nbe pre-deﬁned. Speciﬁcally, as reported in [344], the recursive struc-\\nture commonly found in natural scene images and in natural language\\nsentences can be discovered using a max-margin structure prediction\\narchitecture. It is shown that the units contained in the images or sen-\\ntences are identiﬁed, and the way in which these units interact with\\neach other to form the whole is also identiﬁed.\\n3.3 Deep networks for supervised learning\\nMany of the discriminative techniques for supervised learning in signal\\nand information processing are shallow architectures such as HMMs\\n[52, 127, 147, 186, 188, 290, 394, 418] and conditional random ﬁelds\\n(CRFs) [151, 155, 281, 400, 429, 446]. A CRF is intrinsically a shal-\\nlow discriminative architecture, characterized by the linear relationship\\nbetween the input features and the transition features. The shallow\\nnature of the CRF is made most clear by the equivalence established\\nbetween the CRF and the discriminatively trained Gaussian models\\nand HMMs [148]. More recently, deep-structured CRFs have been devel-\\noped by stacking the output in each lower layer of the CRF, together\\nwith the original input data, onto its higher layer [428]. Various ver-\\nsions of deep-structured CRFs are successfully applied to phone recog-\\nnition [410], spoken language identiﬁcation [428], and natural language\\nprocessing [428]. However, at least for the phone recognition task, the\\nperformance of deep-structured CRFs, which are purely discrimina-\\ntive (non-generative), has not been able to match that of the hybrid\\napproach involving DBN, which we will take on shortly.\\nMorgan [261] gives an excellent review on other major existing\\ndiscriminative models in speech recognition based mainly on the tra-\\nditional neural network or MLP architecture using back-propagation\\nlearning with random initialization. It argues for the importance of\\nboth the increased width of each layer of the neural networks and the\\nincreased depth. In particular, a class of deep neural network models\\nforms the basis of the popular “tandem” approach [262], where the out-\\nput of the discriminatively learned neural network is treated as part', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='32ea98c7-bfc3-4238-bed0-526a8829cb94', embedding=None, metadata={'page_label': '224', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='224 Three Classes of Deep Learning Networks\\nof the observation variable in HMMs. For some representative recent\\nwork in this area, see [193, 283].\\nIn more recent work of [106, 110, 218, 366, 377], a new deep learning\\narchitecture, sometimes called deep stacking network (DSN), together\\nwith its tensor variant [180, 181] and its kernel version [102], are\\ndeveloped that all focus on discrimination with scalable, parallelizable,\\nblock-wise learning relying on little or no generative component. We\\nwill describe this type of discriminative deep architecture in detail in\\nSection 6.\\nAs discussed in the preceding section, recurrent neural networks\\n(RNNs) have been used as a generative model; see also the neural pre-\\ndictive model [87] with a similar “generative” mechanism. RNNs can\\nalso be used as a discriminative model where the output is a label\\nsequence associated with the input data sequence. Note that such dis-\\ncriminative RNNs or sequence models were applied to speech a long\\ntime ago with limited success. In [17], an HMM was trained jointly with\\nthe neural networks, with a discriminative probabilistic training crite-\\nrion. In [304], a separate HMM was used to segment the sequence during\\ntraining, and the HMM was also used to transform the RNN classiﬁ-\\ncation results into label sequences. However, the use of the HMM for\\nthese purposes does not take advantage of the full potential of RNNs.\\nA set of new models and methods were proposed more recently\\nin [133, 134, 135, 136] that enable the RNNs themselves to perform\\nsequence classiﬁcation while embedding the long-short-term memory\\ninto the model, removing the need for pre-segmenting the training data\\nand for post-processing the outputs. Underlying this method is the idea\\nof interpreting RNN outputs as the conditional distributions over all\\npossible label sequences given the input sequences. Then, a diﬀeren-\\ntiable objective function can be derived to optimize these conditional\\ndistributions over the correct label sequences, where the segmentation\\nof the data is performed automatically by the algorithm. The eﬀective-\\nness of this method has been demonstrated in handwriting recognition\\ntasks and in a small speech task [135, 136] to be discussed in more\\ndetail in Section 7 of this monograph.\\nAnother type of discriminative deep architecture is the convo-\\nlutional neural network (CNN), in which each module consists of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='950ba710-1b1a-4291-84eb-4399210d5e84', embedding=None, metadata={'page_label': '225', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3. Deep networks for supervised learning 225\\na convolutional layer and a pooling layer. These modules are often\\nstacked up with one on top of another, or with a DNN on top of it, to\\nform a deep model [212]. The convolutional layer shares many weights,\\nand the pooling layer subsamples the output of the convolutional layer\\nand reduces the data rate from the layer below. The weight sharing\\nin the convolutional layer, together with appropriately chosen pool-\\ning schemes, endows the CNN with some “invariance” properties (e.g.,\\ntranslation invariance). It has been argued that such limited “invari-\\nance” or equi-variance is not adequate for complex pattern recognition\\ntasks and more principled ways of handling a wider range of invariance\\nmay be needed [160]. Nevertheless, CNNs have been found highly eﬀec-\\ntive and been commonly used in computer vision and image recognition\\n[54, 55, 56, 57, 69, 198, 209, 212, 434]. More recently, with appropri-\\nate changes from the CNN designed for image analysis to that taking\\ninto account speech-speciﬁc properties, the CNN is also found eﬀec-\\ntive for speech recognition [1, 2, 3, 81, 94, 312]. We will discuss such\\napplications in more detail in Section 7 of this monograph.\\nIt is useful to point out that the time-delay neural network (TDNN)\\n[202, 382] developed for early speech recognition is a special case and\\npredecessor of the CNN when weight sharing is limited to one of the\\ntwo dimensions, i.e., time dimension, and there is no pooling layer. It\\nwas not until recently that researchers have discovered that the time-\\ndimension invariance is less important than the frequency-dimension\\ninvariance for speech recognition [1, 3, 81]. A careful analysis on the\\nunderlying reasons is described in [81], together with a new strategy for\\ndesigning the CNN’s pooling layer demonstrated to be more eﬀective\\nthan all previous CNNs in phone recognition.\\nIt is also useful to point out that the model of hierarchical tempo-\\nral memory (HTM) [126, 143, 142] is another variant and extension of\\nthe CNN. The extension includes the following aspects: (1) Time or\\ntemporal dimension is introduced to serve as the “supervision” infor-\\nmation for discrimination (even for static images); (2) Both bottom-up\\nand top-down information ﬂows are used, instead of just bottom-up in\\nthe CNN; and (3) A Bayesian probabilistic formalism is used for fusing\\ninformation and for decision making.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ab67e1b8-1ead-44c1-817c-3909854ca4a8', embedding=None, metadata={'page_label': '226', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='226 Three Classes of Deep Learning Networks\\nFinally, the learning architecture developed for bottom-up,\\ndetection-based speech recognition proposed in [214] and developed\\nfurther since 2004, notably in [330, 332, 427] using the DBN–DNN\\ntechnique, can also be categorized in the discriminative or supervised-\\nlearning deep architecture category. There is no intent and mecha-\\nnism in this architecture to characterize the joint probability of data\\nand recognition targets of speech attributes and of the higher-level\\nphone and words. The most current implementation of this approach\\nis based on the DNN, or neural networks with many layers using back-\\npropagation learning. One intermediate neural network layer in the\\nimplementation of this detection-based framework explicitly represents\\nthe speech attributes, which are simpliﬁed entities from the “atomic”\\nunits of speech developed in the early work of [101, 355]. The simpli-\\nﬁcation lies in the removal of the temporally overlapping properties\\nof the speech attributes or articulatory-like features. Embedding such\\nmore realistic properties in the future work is expected to improve the\\naccuracy of speech recognition further.\\n3.4 Hybrid deep networks\\nThe term “hybrid” for this third category refers to the deep architecture\\nthat either comprises or makes use of both generative and discrimina-\\ntive model components. In the existing hybrid architectures published\\nin the literature, the generative component is mostly exploited to help\\nwith discrimination, which is the ﬁnal goal of the hybrid architecture.\\nHow and why generative modeling can help with discrimination can be\\nexamined from two viewpoints [114]:\\n• The optimization viewpoint where generative models trained in\\nan unsupervised fashion can provide excellent initialization points\\nin highly nonlinear parameter estimation problems (The com-\\nmonly used term of “pre-training” in deep learning has been intro-\\nduced for this reason); and/or\\n• The regularization perspective where the unsupervised-learning\\nmodels can eﬀectively provide a prior on the set of functions\\nrepresentable by the model.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='122ce8d1-4384-48b2-b06d-1d7c323698bb', embedding=None, metadata={'page_label': '227', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Hybrid deep networks 227\\nThe study reported in [114] provided an insightful analysis and exper-\\nimental evidence supporting both of the viewpoints above.\\nThe DBN, a generative, deep network for unsupervised learning dis-\\ncussed in Section 3.2, can be converted to and used as the initial model\\nof a DNN for supervised learning with the same network structure,\\nwhich is further discriminatively trained or ﬁne-tuned using the target\\nlabels provided. When the DBN is used in this way we consider this\\nDBN–DNN model as a hybrid deep model, where the model trained\\nusing unsupervised data helps to make the discriminative model eﬀec-\\ntive for supervised learning. We will review details of the discriminative\\nDNN for supervised learning in the context of RBM/DBN generative,\\nunsupervised pre-training in Section 5.\\nAnother example of the hybrid deep network is developed in [260],\\nwhere the DNN weights are also initialized from a generative DBN\\nbut are further ﬁne-tuned with a sequence-level discriminative crite-\\nrion, which is the conditional probability of the label sequence given\\nthe input feature sequence, instead of the frame-level criterion of cross-\\nentropy commonly used. This can be viewed as a combination of the\\nstatic DNN with the shallow discriminative architecture of CRF. It can\\nbe shown that such a DNN–CRF is equivalent to a hybrid deep architec-\\nture of DNN and HMM whose parameters are learned jointly using the\\nfull-sequence maximum mutual information (MMI) criterion between\\nthe entire label sequence and the input feature sequence. A closely\\nrelated full-sequence training method designed and implemented for\\nmuch larger tasks is carried out more recently with success for a shallow\\nneural network [194] and for a deep one [195, 353, 374]. We note that\\nthe origin of the idea for joint training of the sequence model (e.g., the\\nHMM) and of the neural network came from the early work of [17, 25],\\nwhere shallow neural networks were trained with small amounts of\\ntraining data and with no generative pre-training.\\nHere, it is useful to point out a connection between the above\\npretraining/ﬁne-tuning strategy associated with hybrid deep networks\\nand the highly popular minimum phone error (MPE) training technique\\nfor the HMM (see [147, 290] for an overview). To make MPE training\\neﬀective, the parameters need to be initialized using an algorithm (e.g.,\\nBaum-Welch algorithm) that optimizes a generative criterion (e.g.,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1551fce8-3299-40cf-9e73-17b5108eae49', embedding=None, metadata={'page_label': '228', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='228 Three Classes of Deep Learning Networks\\nmaximum likelihood). This type of methods, which uses maximum-\\nlikelihood trained parameters to assist in the discriminative HMM\\ntraining can be viewed as a “hybrid” approach to train the shallow\\nHMM model.\\nAlong the line of using discriminative criteria to train parameters in\\ngenerative models as in the above HMM training example, we here dis-\\ncuss the same method applied to learning other hybrid deep networks.\\nIn [203], the generative model of RBM is learned using the discrimina-\\ntive criterion of posterior class-label probabilities. Here the label vector\\nis concatenated with the input data vector to form the combined vis-\\nible layer in the RBM. In this way, RBM can serve as a stand-alone\\nsolution to classiﬁcation problems and the authors derived a discrim-\\ninative learning algorithm for RBM as a shallow generative model. In\\nthe more recent work by Ranzato et al. [298], the deep generative model\\nof DBN with gated Markov random ﬁeld (MRF) at the lowest level is\\nlearned for feature extraction and then for recognition of diﬃcult image\\nclasses including occlusions. The generative ability of the DBN facil-\\nitates the discovery of what information is captured and what is lost\\nat each level of representation in the deep model, as demonstrated in\\n[298]. A related study on using the discriminative criterion of empirical\\nrisk to train deep graphical models can be found in [352].\\nA further example of hybrid deep networks is the use of generative\\nmodels of DBNs to pre-train deep convolutional neural networks (deep\\nCNNs) [215, 216, 217]. Like the fully connected DNN discussed ear-\\nlier, pre-training also helps to improve the performance of deep CNNs\\nover random initialization. Pre-training DNNs or CNNs using a set of\\nregularized deep autoencoders [24], including denoising autoencoders,\\ncontractive autoencoders, and sparse autoencoders, is also a similar\\nexample of the category of hybrid deep networks.\\nThe ﬁnal example given here for hybrid deep networks is based\\non the idea and work of [144, 267], where one task of discrimination\\n(e.g., speech recognition) produces the output (text) that serves\\nas the input to the second task of discrimination (e.g., machine\\ntranslation). The overall system, giving the functionality of speech\\ntranslation — translating speech in one language into text in another\\nlanguage — is a two-stage deep architecture consisting of both', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='45905a66-3157-4d23-bb2e-610047184d74', embedding=None, metadata={'page_label': '229', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Hybrid deep networks 229\\ngenerative and discriminative elements. Both models of speech\\nrecognition (e.g., HMM) and of machine translation (e.g., phrasal\\nmapping and non-monotonic alignment) are generative in nature, but\\ntheir parameters are all learned for discrimination of the ultimate\\ntranslated text given the speech data. The framework described in\\n[144] enables end-to-end performance optimization in the overall deep\\narchitecture using the uniﬁed learning framework initially published\\nin [147]. This hybrid deep learning approach can be applied to not\\nonly speech translation but also all speech-centric and possibly other\\ninformation processing tasks such as speech information retrieval,\\nspeech understanding, cross-lingual speech/text understanding and\\nretrieval, etc. (e.g., [88, 94, 145, 146, 366, 398]).\\nIn the next three chapters, we will elaborate on three prominent\\ntypes of models for deep learning, one from each of the three classes\\nreviewed in this chapter. These are chosen to serve the tutorial purpose,\\ngiven their simplicity of the architectural and mathematical descrip-\\ntions. The three architectures described in the following three chapters\\nmay not be interpreted as the most representative and inﬂuential work\\nin each of the three classes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='572fac26-fcc5-4116-a062-136f5aff1e5b', embedding=None, metadata={'page_label': '230', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4\\nDeep Autoencoders — Unsupervised Learning\\nThis section and the next two will each select one prominent example\\ndeep network for each of the three categories outlined in Section 3.\\nHere we begin with the category of the deep models designed mainly\\nfor unsupervised learning.\\n4.1 Introduction\\nThe deep autoencoder is a special type of the DNN (with no class\\nlabels), whose output vectors have the same dimensionality as the input\\nvectors. It is often used for learning a representation or eﬀective encod-\\ning of the original data, in the form of input vectors, at hidden layers.\\nNote that the autoencoder is a nonlinear feature extraction method\\nwithout using class labels. As such, the features extracted aim at con-\\nserving and better representing information instead of performing clas-\\nsiﬁcation tasks, although sometimes these two goals are correlated.\\nAn autoencoder typically has an input layer which represents the\\noriginal data or input feature vectors (e.g., pixels in image or spec-\\ntra in speech), one or more hidden layers that represent the trans-\\nformed feature, and an output layer which matches the input layer for\\n230', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='eb154ab2-2740-47e9-b6d8-ecf1443dfb66', embedding=None, metadata={'page_label': '231', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Use of deep autoencoders to extract speech features 231\\nreconstruction. When the number of hidden layers is greater than one,\\nthe autoencoder is considered to be deep. The dimension of the hidden\\nlayers can be either smaller (when the goal is feature compression) or\\nlarger (when the goal is mapping the feature to a higher-dimensional\\nspace) than the input dimension.\\nAn autoencoder is often trained using one of the many back-\\npropagation variants, typically the stochastic gradient descent method.\\nThough often reasonably eﬀective, there are fundamental problems\\nwhen using back-propagation to train networks with many hidden\\nlayers. Once the errors get back-propagated to the ﬁrst few layers,\\nthey become minuscule, and training becomes quite ineﬀective. Though\\nmore advanced back-propagation methods help with this problem to\\nsome degree, it still results in slow learning and poor solutions, espe-\\ncially with limited amounts of training data. As mentioned in the pre-\\nvious chapters, the problem can be alleviated by pre-training each layer\\nas a simple autoencoder [28, 163]. This strategy has been applied to\\nconstruct a deep autoencoder to map images to short binary code for\\nfast, content-based image retrieval, to encode documents (called seman-\\ntic hashing), and to encode spectrogram-like speech features which we\\nreview below.\\n4.2 Use of deep autoencoders to extract speech features\\nHere we review a set of work, some of which was published in [100],\\nin developing an autoencoder for extracting binary speech codes from\\nthe raw speech spectrogram data in an unsupervised manner (i.e., no\\nspeech class labels). The discrete representations in terms of a binary\\ncode extracted by this model can be used in speech information retrieval\\nor as bottleneck features for speech recognition.\\nA deep generative model of patches of spectrograms that con-\\ntain 256 frequency bins and 1, 3, 9, or 13 frames is illustrated in\\nFigure 4.1. An undirected graphical model called a Gaussian-Bernoulli\\nRBM is built that has one visible layer of linear variables with\\nGaussian noise and one hidden layer of 500 to 3000 binary latent\\nvariables. After learning the Gaussian-Bernoulli RBM, the activation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a34e32b9-8edf-4134-af45-dacaec3ba357', embedding=None, metadata={'page_label': '232', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='232 Deep Autoencoders — Unsupervised Learning\\nFigure 4.1: The architecture of the deep autoencoder used in [100] for extracting\\nbinary speech codes from high-resolution spectrograms. [after [100], @Elsevier].\\nprobabilities of its hidden units are treated as the data for training\\nanother Bernoulli-Bernoulli RBM. These two RBM’s can then be com-\\nposed to form a deep belief net (DBN) in which it is easy to infer the\\nstates of the second layer of binary hidden units from the input in a\\nsingle forward pass. The DBN used in this work is illustrated on the left\\nside of Figure 4.1, where the two RBMs are shown in separate boxes.\\n(See more detailed discussions on the RBM and DBN in Section 5).\\nThe deep autoencoder with three hidden layers is formed by\\n“unrolling” the DBN using its weight matrices. The lower layers of\\nthis deep autoencoder use the matrices to encode the input and the\\nupper layers use the matrices in reverse order to decode the input.\\nThis deep autoencoder is then ﬁne-tuned using error back-propagation\\nto minimize the reconstruction error, as shown on the right side of Fig-\\nure 4.1. After learning is complete, any variable-length spectrogram', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cec7a2c3-82ab-46d5-a304-19b1b5e98644', embedding=None, metadata={'page_label': '233', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Use of deep autoencoders to extract speech features 233\\ncan be encoded and reconstructed as foll ows. First, N consecutive\\noverlapping frames of 256-point log power spectra are each normalized\\nto zero-mean and unit-variance across samples per feature to provide\\nthe input to the deep autoencoder. The ﬁrst hidden layer then uses the\\nlogistic function to compute real-valued activations. These real values\\nare fed to the next, coding layer to compute “codes. ” The real-valued\\nactivations of hidden units in the coding layer are quantized to be\\neither zero or one with 0.5 as the threshold. These binary codes are\\nthen used to reconstruct the original spectrogram, where individual\\nﬁxed-frame patches are reconstructed ﬁrst using the two upper layers\\nof network weights. Finally, the standard overlap-and-add technique in\\nsignal processing is used to reconstruct the full-length speech spectro-\\ngram from the outputs produced by applying the deep autoencoder to\\nevery possible window of N consecutive frames. We show some illus-\\ntrative encoding and reconstruction examples below.\\nFigure 4.2: Top to Bottom: The ordinal spectrogram; reconstructions using input\\nwindow sized of N =1 , 3, 9, and 13 while forcing the coding units to take values of\\nzero one (i.e., a binary code) . [after [100], @Elsevier].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='73ccc9e0-1af9-4495-91e6-0b44d0b0fd82', embedding=None, metadata={'page_label': '234', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='234 Deep Autoencoders — Unsupervised Learning\\nAt the top of Figure 4.2 is the original, un-coded speech, followed\\nby the speech utterances reconstructed from the binary codes (zero\\nor one) at the 312 unit bottleneck code layer with encoding window\\nlengths of N =1 , 3, 9, and 13, respectively. The lower reconstruction\\nerrors for N =9a n d N = 13 are clearly seen.\\nEncoding error of the deep autoencoder is qualitatively examined\\nin comparison with the more traditional codes via vector quantization\\n(VQ). Figure 4.3 shows various aspects of the encoding errors. At the\\ntop is the original speech utterance’s spectrogram. The next two spec-\\ntrograms are the blurry reconstruction from the 312-bit VQ and the\\nmuch more faithful reconstruction from the 312-bit deep autoencoder.\\nCoding errors from both coders, plotted as a function of time, are\\nFigure 4.3:Top to bottom: The original spectrogram from the test set; reconstruc-\\ntion from the 312-bit VQ coder; reconstruction from the 312-bit autoencoder; coding\\nerrors as a function of time for the VQ coder (blue) and autoencoder (red); spec-\\ntrogram of the VQ coder residual; spectrogram of the deep autoencoder’s residual.\\n[after [100], @ Elsevier].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0d989f22-20ea-4521-b2a6-6fd0b513d486', embedding=None, metadata={'page_label': '235', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Stacked denoising autoencoders 235\\nFigure 4.4: The original speech spectrogram and the reconstructed counterpart.\\nA total of 312 binary codes are with one for each single frame.\\nshown below the spectrograms, demonstrating that the autoencoder\\n(red curve) is producing lower errors than the VQ coder (blue curve)\\nthroughout the entire span of the utterance. The ﬁnal two spectrograms\\nshow detailed coding error distributions over both time and frequency\\nbins.\\nFigures 4.4 to 4.10 show additional examples (unpublished) for the\\noriginal un-coded speech spectrograms and their reconstructions using\\nthe deep autoencoder. They give a diverse number of binary codes for\\neither a single or three consecutive frames in the spectrogram samples.\\n4.3 Stacked denoising autoencoders\\nIn early years of autoencoder research, the encoding layer had smaller\\ndimensions than the input layer. However, in some applications, it is\\ndesirable that the encoding layer is wider than the input layer, in which\\ncase techniques are needed to prevent the neural network from learning\\nthe trivial identity mapping function. One of the reasons for using a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2a435d5f-244c-4f02-812f-a0e11b1fca39', embedding=None, metadata={'page_label': '236', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='236 Deep Autoencoders — Unsupervised Learning\\nFigure 4.5: Same as Figure 4.4 but with a diﬀerent TIMIT speech utterance.\\nFigure 4.6: The original speech spectrogram and the reconstructed counterpart.\\nA total of 936 binary codes are used for three adjacent frames.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0e55ff75-37e6-485f-9f63-7fe6f2fbe1c6', embedding=None, metadata={'page_label': '237', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Stacked denoising autoencoders 237\\nFigure 4.7: Same as Figure 4.6 but with a diﬀerent TIMIT speech utterance.\\nFigure 4.8: Same as Figure 4.6 but with yet another TIMIT speech utterance.\\nhigher dimension in the hidden or encoding layers than the input layer\\nis that it allows the autoencoder to capture a rich input distribution.\\nThe trivial mapping problem discussed above can be prevented by\\nmethods such as using sparseness constraints, or using the “dropout”\\ntrick by randomly forcing certain values to be zero and thus introducing\\ndistortions at the input data [376, 375] or at the hidden layers [166]. For', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2b17d106-5646-40ab-9db6-7e0c74ca363a', embedding=None, metadata={'page_label': '238', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='238 Deep Autoencoders — Unsupervised Learning\\nFigure 4.9: The original speech spectrogram and the reconstructed counterpart.\\nA total of 2000 binary codes with one for each single frame.\\nFigure 4.10: Same as Figure 4.9 but with a diﬀerent TIMIT speech utterance.\\nexample, in the stacked denoising autoencoder detailed in [376], random\\nnoises are added to the input data. This serves several purposes. First,\\nby forcing the output to match the original undistorted input data the\\nmodel can avoid learning the trivial identity solution. Second, since\\nthe noises are added randomly, the model learned would be robust to\\nthe same kind of distortions in the test data. Third, since each distorted', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f0fc90c6-b6fb-44a8-98b5-c46a2a11bdb2', embedding=None, metadata={'page_label': '239', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4. Transforming autoencoders 239\\ninput sample is diﬀerent, it greatly increases the training set size and\\nthus can alleviate the overﬁtting problem.\\nIt is interesting to note that when the encoding and decoding\\nweights are forced to be the transpose of each other, such denoising\\nautoencoder with a single sigmoidal hidden layer is strictly equiva-\\nlent to a particular Gaussian RBM, but instead of training it by the\\ntechnique of contrastive divergence (CD) or persistent CD, it is trained\\nby a score matching principle, where the score is deﬁned as the deriva-\\ntive of the log-density with respect to the input [375]. Furthermore,\\nAlain and Bengio [5] generalized this result to any parameterization\\nof the encoder and decoder with squared reconstruction error and\\nGaussian corruption noise. They show that as the amount of noise\\napproaches zero, such models estimate the true score of the underly-\\ning data generating distribution. Finally, Bengio et al. [30] show that\\nany denoising autoencoder is a consistent estimator of the underly-\\ning data generating distribution within some family of distributions.\\nThis is true for any parameterization of the autoencoder, for any\\ntype of information-destroying corruption process with no constraint\\non the noise level except being positive, and for any reconstruction\\nloss expressed as a conditional log-likelihood. The consistency of the\\nestimator is achieved by associating the denoising autoencoder with\\na Markov chain whose stationary distribution is the distribution esti-\\nmated by the model, and this Markov chain can be used to sample\\nfrom the denoising autoencoder.\\n4.4 Transforming autoencoders\\nThe deep autoencoder described above can extract faithful codes for\\nfeature vectors due to many layers of nonlinear processing. However, the\\ncode extracted in this way is transformation-variant. In other words,\\nthe extracted code would change in ways chosen by the learner when the\\ninput feature vector is transformed. Sometimes, it is desirable to have\\nthe code change predictably to reﬂect the underlying transformation-\\ninvariant property of the perceived content. This is the goal of the\\ntransforming autoencoder proposed in [162] for image recognition.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1bdfc02a-131c-40ed-bfb9-1214e303d018', embedding=None, metadata={'page_label': '240', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='240 Deep Autoencoders — Unsupervised Learning\\nThe building block of the transforming autoencoder is a “capsule,”\\nwhich is an independent sub-network that extracts a single parameter-\\nized feature representing a single entity, be it visual or audio. A trans-\\nforming autoencoder receives both an input vector and a target output\\nvector, which is transformed from the input vector through a simple\\nglobal transformation mechanism; e.g., translation of an image and\\nfrequency shift of speech (the latter due to the vocal tract length\\ndiﬀerence). An explicit representation of the global transformation is\\nassumed known. The coding layer of the transforming autoencoder con-\\nsists of the outputs of several capsules.\\nDuring the training phase, the diﬀerent capsules learn to extract\\ndiﬀerent entities in order to minimize the error between the ﬁnal output\\nand the target.\\nIn addition to the deep autoencoder architectures described here,\\nthere are many other types of generative architectures in the literature,\\nall characterized by the use of data alone (i.e., free of classiﬁcation\\nlabels) to automatically derive higher-level features.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a166caa7-f36a-47f3-b1be-0da2567ce19e', embedding=None, metadata={'page_label': '241', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5\\nPre-Trained Deep Neural Networks — A Hybrid\\nIn this section, we present the most widely used hybrid deep archi-\\ntecture — the pre-trained deep neural network (DNN), and discuss\\nthe related techniques and building blocks including the RBM and\\nDBN. We discuss the DNN example here in the category of hybrid\\ndeep networks before the examples in the category of deep networks for\\nsupervised learning (Section 6). This is partly due to the natural ﬂow\\nfrom the unsupervised learning models to the DNN as a hybrid model.\\nThe discriminative nature of artiﬁcial neural networks for supervised\\nlearning has been widely known, and thus would not be required for\\nunderstanding the hybrid nature of the DNN that uses unsupervised\\npre-training to facilitate the subsequent discriminative ﬁne tuning.\\nPart of the review in this chapter is based on recent publications in\\n[68, 161, 412].\\n5.1 Restricted Boltzmann machines\\nAn RBM is a special type of Markov random ﬁeld that has one layer of\\n(typically Bernoulli) stochastic hidden units and one layer of (typically\\nBernoulli or Gaussian) stochastic visible or observable units. RBMs can\\n241', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='874a2f04-241c-4a14-8e69-cf533e3ce1d6', embedding=None, metadata={'page_label': '242', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='242 Pre-Trained Deep Neural Networks — A Hybrid\\nbe represented as bipartite graphs, where all visible units are connected\\nto all hidden units, and there are no visible–visible or hidden–hidden\\nconnections.\\nIn an RBM, the joint distribution p(v, h; θ) over the visible units v\\nand hidden units h, given the model parameters θ, is deﬁned in terms\\nof an energy function E(v, h; θ)o f\\np(v, h; θ)= exp(−E(v, h; θ))\\nZ ,\\nwhere Z = ∑\\nv\\n∑\\nh exp(−E(v, h; θ)) is a normalization factor or parti-\\ntion function, and the marginal probability that the model assigns to\\na visible vector v is\\np(v; θ)=\\n∑\\nh exp(−E(v, h; θ))\\nZ\\nFor a Bernoulli (visible)-Bernoulli (hidden) RBM, the energy function\\nis deﬁned as\\nE(v, h; θ)= −\\nI∑\\ni=1\\nJ∑\\nj=1\\nwijvihj −\\nI∑\\ni=1\\nbivi −\\nJ∑\\nj=1\\najhj.\\nwhere wij represents the symmetric interaction term between visible\\nunit vi and hidden unit hj, bi and aj the bias terms, and I and J are\\nthe numbers of visible and hidden units. The conditional probabilities\\ncan be eﬃciently calculated as\\np(hj =1 |v; θ)= σ\\n( I∑\\ni=1\\nwijvi + aj\\n)\\n,\\np(vi =1 |h; θ)= σ\\n\\uf8eb\\n\\uf8ed\\nJ∑\\nj=1\\nwijhj+bi\\n\\uf8f6\\n\\uf8f8,\\nwhere σ(x)=1 /(1 + exp(−x)).\\nSimilarly, for a Gaussian (visible)-Bernoulli (hidden) RBM, the\\nenergy is\\nE(v, h; θ)= −\\nI∑\\ni=1\\nJ∑\\nj=1\\nwijvihj − 1\\n2\\nI∑\\ni=1\\n(vi − bi)2 −\\nJ∑\\nj=1\\najhj,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5999fb19-74e8-4b68-be1c-a4481834560c', embedding=None, metadata={'page_label': '243', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1. Restricted Boltzmann machines 243\\nThe corresponding conditional probabilities become\\np(hj =1 |v; θ)= σ\\n( I∑\\ni=1\\nwijvi+aj\\n)\\n,\\np(vi|h; θ)= N\\n\\uf8eb\\n\\uf8ed\\nJ∑\\nj=1\\nwijhj + bi, 1\\n\\uf8f6\\n\\uf8f8,\\nwhere vi takes real values and follows a Gaussian distribution with\\nmean ∑J\\nj=1 wijhj + bi and variance one. Gaussian-Bernoulli RBMs can\\nbe used to convert real-valued stochastic variables to binary stochastic\\nvariables, which can then be further processed using the Bernoulli-\\nBernoulli RBMs.\\nThe above discussion used two of the most common conditional\\ndistributions for the visible data in the RBM — Gaussian (for\\ncontinuous-valued data) and binomial (for binary data). More general\\ntypes of distributions in the RBM can also be used. See [386] for the\\nuse of general exponential-family distributions for this purpose.\\nTaking the gradient of the log likelihood log p(v; θ)w ec a nd e r i v e\\nthe update rule for the RBM weights as:\\n∆wij = Edata(vihj) − Emodel(vihj),\\nwhere Edata(vihj) is the expectation observed in the training set (with\\nhj sampled given vi according to the model), and Emodel(vihj)i st h a t\\nsame expectation under the distribution deﬁned by the model. Unfor-\\ntunately, Emodel(vihj) is intractable to compute. The contrastive diver-\\ngence (CD) approximation to the gradient was the ﬁrst eﬃcient method\\nproposed to approximate this expected value, where Emodel(vihj)i s\\nreplaced by running the Gibbs sampler initialized at the data for one\\nor more steps. The steps in approximatingEmodel(vihj) is summarized\\nas follows:\\n• Initialize v0 at data\\n• Sample h0 ∼ p(h|v0)\\n• Sample v1 ∼ p(v|h0)\\n• Sample h1 ∼ p(h|v1)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0af7063e-7977-404e-8082-ff9395102960', embedding=None, metadata={'page_label': '244', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='244 Pre-Trained Deep Neural Networks — A Hybrid\\nFigure 5.1: A pictorial view of sampling from a RBM during RBM learning (cour-\\ntesy of Geoﬀ Hinton).\\nHere, (v1, h1)i sas a m p l ef r o mt h em o d e l ,a sav e r yr o u g he s t i m a t e\\nof Emodel(vihj). The use of (v1, h1) to approximate Emodel(vihj)g i v e s\\nrise to the algorithm of CD-1. The sampling process can be pictorially\\ndepicted in Figure 5.1.\\nNote that CD-k generalizes this to more steps of the Markov chain.\\nThere are other techniques for estimating the log-likelihood gradient of\\nRBMs, in particular the stochastic maximum likelihood or persistent\\ncontrastive divergence (PCD) [363, 406]. Both work better than CD\\nwhen using the RBM as a generative model.\\nCareful training of RBMs is essential to the success of applying\\nRBM and related deep learning techniques to solve practical problems.\\nSee Technical Report [159] for a very useful practical guide for training\\nRBMs.\\nThe RBM discussed above is both a generative and an unsupervised\\nmodel, which characterizes the input data distribution using hidden\\nvariables and there is no label information involved. However, when\\nthe label information is available, it can be used together with the\\ndata to form the concatenated “data” set. Then the same CD learn-\\ning can be applied to optimize the approximate “generative” objective\\nfunction related to data likelihood. Further, and more interestingly, a\\n“discriminative” objective function can be deﬁned in terms of condi-\\ntional likelihood of labels. This discriminative RBM can be used to\\n“ﬁne tune” RBM for classiﬁcation tasks [203].\\nRanzato et al. [297, 295] proposed an unsupervised learning algo-\\nrithm called sparse encoding symmetric machine (SESM), which is\\nquite similar to RBM. They both have a symmetric encoder and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='583e2311-f401-48b4-99b5-6c5bc9605d72', embedding=None, metadata={'page_label': '245', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2. Unsupervised layer-wise pre-training 245\\ndecoder, and a logistic nonlinearity on the top of the encoder. The main\\ndiﬀerence is that whereas the RBM is trained using (very approximate)\\nmaximum likelihood, SESM is trained by simply minimizing the aver-\\nage energy plus an additional code sparsity term. SESM relies on the\\nsparsity term to prevent ﬂat energy surfaces, while RBM relies on an\\nexplicit contrastive term in the loss, an approximation of the log par-\\ntition function. Another diﬀerence is in the coding strategy in that the\\ncode units are “noisy” and binary in the RBM, while they are quasi-\\nbinary and sparse in SESM. The use of SESM in pre-training DNNs\\nfor speech recognition can be found in [284].\\n5.2 Unsupervised layer-wise pre-training\\nHere we describe how to stack up RBMs just described to form a\\nDBN as the basis for DNN’s pre-training. Before delving into details,\\nwe ﬁrst note that this procedure, proposed by Hinton and Salakhut-\\ndinov [163] is a more general technique of unsupervised layer-wise\\npretraining. That is, not only RBMs can be stacked to form deep gen-\\nerative (or discriminative) networks, but other types of networks can\\nalso do the same, such as autoencoder variants as proposed by Bengio\\net al. [28].\\nStacking a number of the RBMs learned layer by layer from bottom\\nup gives rise to a DBN, an example of which is shown in Figure 5.2. The\\nstacking procedure is as follows. After learning a Gaussian-Bernoulli\\nRBM (for applications with continuous features such as speech) or\\nBernoulli-Bernoulli RBM (for applications with nominal or binary fea-\\ntures such as black-white image or coded text), we treat the activation\\nprobabilities of its hidden units as the data for training the Bernoulli-\\nBernoulli RBM one layer up. The activation probabilities of the second-\\nlayer Bernoulli-Bernoulli RBM are then used as the visible data input\\nfor the third-layer Bernoulli-Bernoulli RBM, and so on. Some theoret-\\nical justiﬁcation of this eﬃcient layer-by-layer greedy learning strat-\\negy is given in [163], where it is shown that thestacking procedure\\nabove improves a variational lower bound on the likelihood of the train-\\ning data under the composite model. That is, the greedy procedure', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f99ceb5f-33c9-4a6d-ab58-70fffcddc2e4', embedding=None, metadata={'page_label': '246', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='246 Pre-Trained Deep Neural Networks — A Hybrid\\nFigure 5.2: An illustration of the DBN-DNN architecture.\\nabove achieves approximate maximum likelihood learning. Note that\\nthis learning procedure is unsupervised and requires no class label.\\nWhen applied to classiﬁcation tasks, the generative pre-training\\ncan be followed by or combined with other, typically discriminative,\\nlearning procedures that ﬁne-tune all of the weights jointly to improve\\nthe performance of the network. This discriminative ﬁne-tuning is per-\\nformed by adding a ﬁnal layer of variables that represent the desired\\noutputs or labels provided in the training data. Then, the back-\\npropagation algorithm can be used to adjust or ﬁne-tune the network\\nweights in the same way as for the standard feed-forward neural net-\\nwork. What goes to the top, label layer of this DNN depends on the\\napplication. For speech recognition applications, the top layer, denoted\\nby “l1,l 2,...,l j,...,l L,” in Figure 5.2, can represent either syllables,\\nphones, sub-phones, phone states, or other speech units used in the\\nHMM-based speech recognition system.\\nThe generative pre-training described above has produced better\\nphone and speech recognition results than random initialization on\\na wide variety of tasks, which will be surveyed in Section 7. Fur-\\nther research has also shown the eﬀectiveness of other pre-training\\nstrategies. As an example, greedy layer-by-layer training may be carried', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='74975c7b-dd6b-4cc3-a76c-deb8cb38e145', embedding=None, metadata={'page_label': '247', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2. Unsupervised layer-wise pre-training 247\\nout with an additional discriminative term to the generative cost func-\\ntion at each level. And without generative pre-training, purely discrim-\\ninative training of DNNs from random initial weights using the tradi-\\ntional stochastic gradient decent method has been shown to work very\\nwell when the scales of the initial weights are set carefully and the mini-\\nbatch sizes, which trade oﬀ noisy gradients with convergence speed,\\nused in stochastic gradient decent are adapted prudently (e.g., with\\nan increasing size over training epochs). Also, randomization order in\\ncreating mini-batches needs to be judiciously determined. Importantly,\\nit was found eﬀective to learn a DNN by starting with a shallow neural\\nnetwork with a single hidden layer. Once this has been trained discrimi-\\nnatively (using early stops to avoid overﬁtting), a second hidden layer is\\ninserted between the ﬁrst hidden layer and the labeled softmax output\\nunits and the expanded deeper network is again trained discrimina-\\ntively. This can be continued until the desired number of hidden layers\\nis reached, after which a full backpropagation “ﬁne tuning” is applied.\\nThis discriminative “pre-training” procedure is found to work well in\\npractice [324, 419], especially with a reasonably large amount of train-\\ning data. When the amount of training data is increased even more,\\nthen some carefully designed random initialization methods can work\\nwell also without using the above pre-training schemes.\\nIn any case, pre-training based on the use of RBMs to stack up in\\nforming the DBN has been found to work well in most cases, regardless\\nof a large or small amount of training data. It is useful to point out\\nthat there are other ways to perform pre-training in addition to the\\nuse of RBMs and DBNs. For example, denoising autoencoders have\\nnow been shown to be consistent estimators of the data generating\\ndistribution [30]. Like RBMs, they are also shown to be generative\\nmodels from which one can sample. Unlike RBMs, however, an\\nunbiased estimator of the gradient of the training objective function\\ncan be obtained by the denoising autoencoders, avoiding the need for\\nMCMC or variational approximations in the inner loop of training.\\nTherefore, the greedy layer-wise pre-training may be performed as\\neﬀectively by stacking the denoising autoencoders as by stacking the\\nRBMs each as a single-layer learner.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e7dfef22-b5e5-4541-b23a-496d7cbddb6e', embedding=None, metadata={'page_label': '248', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='248 Pre-Trained Deep Neural Networks — A Hybrid\\nFurther, a general framework for layer-wise pre-training can be\\nfound in many deep learning papers; e.g., Section 2 of [21]. This\\nincludes, as a special case, the use of RBMs as the single-layer build-\\ning block as discussed in this section. The more general framework\\ncan cover the RBM/DBN as well as any other unsupervised feature\\nextractor. It can also cover the case of unsupervised pre-training of the\\nrepresentation only followed by a separate stage of learning a classiﬁer\\non top of the unsupervised, pre-trained features [215, 216, 217].\\n5.3 Interfacing DNNs with HMMs\\nThe pre-trained DNN as a prominent example of the hybrid deep\\nnetworks discussed so far in this chapter is a static classiﬁer with\\ninput vectors having a ﬁxed dimensionality. However, many practi-\\ncal pattern recognition and information processing problems, including\\nspeech recognition, machine translation, natural language understand-\\ning, video processing and bio-information processing, require sequence\\nrecognition. In sequence recognition, sometimes called classiﬁcation\\nwith structured input/output, the dimensionality of both inputs and\\noutputs are variable.\\nThe HMM, based on dynamic programing operations, is a con-\\nvenient tool to help port the strength of a static classiﬁer to han-\\ndle dynamic or sequential patterns. Thus, it is natural to combine\\nfeed-forward neural networks and HMMs to bridge the gap between\\nthe static and sequence pattern recognition, as was done in the early\\ndays of neural networks for speech recognition [17, 25, 42]. A popu-\\nlar architecture to fulﬁll this role with the use of the DNN is shown\\nin Figure 5.3. This architecture has been successfully used in speech\\nrecognition experiments as reported in [67, 68].\\nIt is important to note that the unique elasticity of temporal dynam-\\nics of speech as elaborated in [45, 73, 76, 83] would require temporally', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d44819b1-e29d-4373-89c6-770a714cf210', embedding=None, metadata={'page_label': '249', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.3. Interfacing DNNs with HMMs 249\\nFigure 5.3: Interface between DBN/DNN and HMM to form a DNN–HMM. This\\narchitecture, developed at Microsoft, has been successfully used in speech recognition\\nexperiments reported in [67, 68]. [after [67, 68], @IEEE].\\ncorrelated models more powerful than HMMs for the ultimate success\\nof speech recognition. Integrating such dynamic models that have real-\\nistic co-articulatory properties with the DNN and possibly other deep\\nlearning models to form the coherent dynamic deep architecture is a\\nchallenging new research direction.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5715f711-bd99-41c1-9d43-b521b74b3ab0', embedding=None, metadata={'page_label': '250', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6\\nDeep Stacking Networks and Variants —\\nSupervised Learning\\n6.1 Introduction\\nWhile the DNN just reviewed has been shown to be extremely power-\\nful in connection with performing recognition and classiﬁcation tasks\\nincluding speech recognition and image classiﬁcation, training a DNN\\nhas proven to be diﬃcult computationally. In particular, conventional\\ntechniques for training DNNs at the ﬁne tuning phase involve the uti-\\nlization of a stochastic gradient descent learning algorithm, which is\\ndiﬃcult to parallelize across machines. This makes learning at large\\nscale nontrivial. For example, it has been possible to use one single,\\nvery powerful GPU machine to train DNN-based speech recognizers\\nwith dozens to a few hundreds or thousands of hours of speech training\\ndata with remarkable results. It is less clear, however, how to scale up\\nthis success with much more training data. See [69] for recent work in\\nthis direction.\\nHere we describe a new deep learning architecture, the deep stacking\\nnetwork (DSN), which was originally designed with the learning scal-\\nability problem in mind. This chapter is based in part on the recent\\npublications of [106, 110, 180, 181] with expanded discussions.\\n250', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6577953a-4bfd-48c1-8092-eb14f93fe032', embedding=None, metadata={'page_label': '251', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1. Introduction 251\\nThe central idea of the DSN design relates to the concept of stack-\\ning, as proposed and explored in [28, 44, 392], where simple modules of\\nfunctions or classiﬁers are composed ﬁrst and then they are “stacked”\\non top of each other in order to learn complex functions or classiﬁers.\\nVarious ways of implementing stacking operations have been developed\\nin the past, typically making use of supervised information in the sim-\\nple modules. The new features for the stacked classiﬁer at a higher\\nlevel of the stacking architecture often come from concatenation of the\\nclassiﬁer output of a lower module and the raw input features. In [60],\\nthe simple module used for stacking was a conditional random ﬁeld\\n(CRF). This type of deep architecture was further developed with hid-\\nden states added for successful natural language and speech recognition\\napplications where segmentation information is unknown in the train-\\ning data [429]. Convolutional neural networks, as in [185], can also be\\nconsidered as a stacking architecture but the supervision information\\nis typically not used until in the ﬁnal stacking module.\\nThe DSN architecture was originally presented in [106] and was\\nreferred as deep convex network or DCN to emphasize the convex\\nnature of a major portion of the algorithm used for learning the net-\\nwork. The DSN makes use of supervision information for stacking each\\nof the basic modules, which takes the simpliﬁed form of multilayer per-\\nceptron. In the basic module, the output units are linear and the hidden\\nunits are sigmoidal nonlinear. The linearity in the output units permits\\nhighly eﬃcient, parallelizable, and closed-form estimation (a result of\\nconvex optimization) for the output network weights given the hidden\\nunits’ activities. Due to the closed-form constraints between the input\\nand output weights, the input weights can also be elegantly estimated in\\nan eﬃcient, parallelizable, batch-mode manner, which we will describe\\nin some detail in Section 6.3.\\nThe name “convex” used in [106] accentuates the role of convex\\noptimization in learning the output network weights given the hidden\\nunits’ activities in each basic module. It also points to the importance\\nof the closed-form constraints, derived from the convexity, between the\\ninput and output weights. Such constraints make the learning of the\\nremaining network parameters (i.e., the input network weights) much\\neasier than otherwise, enabling batch-mode learning of the DSN that', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='18003f46-7774-45d3-a25b-66865c22f809', embedding=None, metadata={'page_label': '252', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='252 Deep Stacking Networks and Variants — Supervised Learning\\ncan be distributed over CPU clusters. And in more recent publications,\\nthe DSN was used when the key operation of stacking is emphasized.\\n6.2 A basic architecture of the deep stacking network\\nA DSN, as shown in Figure 6.1, includes a variable number of layered\\nmodules, wherein each module is a specialized neural network con-\\nsisting of a single hidden layer and two trainable sets of weights. In\\nFigure 6.1, only four such modules are illustrated, where each module\\nis shown with a separate color. In practice, up to a few hundreds of\\nmodules have been eﬃciently trained and used in image and speech\\nclassiﬁcation experiments.\\nThe lowest module in the DSN comprises a linear layer with a set of\\nlinear input units, a hidden nonlinear layer with a set of nonlinear units,\\nand a second linear layer with a set of linear output units. A sigmoidal\\nnonlinearity is typically used in the hidden layer. However, other non-\\nlinearities can also be used. If the DSN is utilized in connection with rec-\\nognizing an image, the input units can correspond to a number of pixels\\n(or extracted features) in the image, and can be assigned values based at\\nleast in part upon intensity values, RGB values, or the like correspond-\\ning to the respective pixels. If the DSN is utilized in connection with\\nspeech recognition, the set of input units may correspond to samples\\nof speech waveform, or the extracted features from speech waveforms,\\nsuch as power spectra or cepstral coeﬃcients. The output units in the\\nlinear output layer represent the targets of classiﬁcation. For instance,\\nif the DSN is conﬁgured to perform digit recognition, then the output\\nunits may be representative of the values 0, 1, 2, 3, and so forth up to 9\\nwith a 0–1 coding scheme. If the DSN is conﬁgured to perform speech\\nrecognition, then the output units may be representative of phones,\\nHMM states of phones, or context-dependent HMM states of phones.\\nThe lower-layer weight matrix, which we denote by W , connects\\nthe linear input layer and the hidden nonlinear layer. The upper-layer\\nweight matrix, which we denote by U, connects the nonlinear hid-\\nden layer with the linear output layer. The weight matrix U can be\\ndetermined through a closed-form solution given the weight matrix W\\nwhen the mean square error training criterion is used.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='025d4c05-1137-4627-b285-0c580b3aad57', embedding=None, metadata={'page_label': '253', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2. A basic architecture of the deep stacking network 253\\nFigure 6.1: A DSN architecture using input–output stacking. Four modules are\\nillustrated, each with a distinct color. Dashed lines denote copying layers. [after\\n[366], @IEEE].\\nAs indicated above, the DSN includes a set of serially connected,\\noverlapping, and layered modules, wherein each module has the same\\narchitecture — a linear input layer followed by a nonlinear hidden\\nlayer, which is connected to a linear output layer. Note that the output\\nunits of a lower module are a subset of the input units of an adjacent\\nhigher module in the DSN. More speciﬁcally, in a second module that\\nis directly above the lowest module in the DSN, the input units can\\ninclude the output units of the lowest module and optionally the raw\\ninput feature.\\nThis pattern of including output units in a lower module as a\\nportion of the input units in an adjacent higher module and thereafter', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='186adafc-dfc0-496b-9fa7-4550a112545b', embedding=None, metadata={'page_label': '254', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='254 Deep Stacking Networks and Variants — Supervised Learning\\nlearning a weight matrix that describes connection weights between\\nhidden units and linear output units via convex optimization can con-\\ntinue for many modules. A resultant learned DSN may then be deployed\\nin connection with an automatic classiﬁcation task such as frame-level\\nspeech phone or state classiﬁcation. Connecting the DSN’s output to an\\nHMM or any dynamic programming device enables continuous speech\\nrecognition and other forms of sequential pattern recognition.\\n6.3 A method for learning the DSN weights\\nHere, we provide some technical details on how the use of linear out-\\nput units in the DSN facilitates the learning of the DSN weights. A\\nsingle module is used to illustrate the advantage for simplicity rea-\\nsons. First, it is clear that the upper layer weight matrix U can\\nbe eﬃciently learned once the activity matrix H over all training\\nsamples in the hidden layer is known. Let’s denote the training vec-\\ntors by X =[ x1,..., xi,..., xN ], in which each vector is denoted by\\nxi =[ x1i,...,x ji,...,x Di]T where D is the dimension of the input vec-\\ntor, which is a function of the block, and N is the total number of\\ntraining samples. Denote by L the number of hidden units and by C\\nthe dimension of the output vector. Then the output of a DSN block is\\nyi = UT hi where hi = σ(W T xi) is the hidden-layer vector for sample\\ni, U is an L × C weight matrix at the upper layer of a block. W is a\\nD ×L weight matrix at the lower layer of a block, and σ(·) is a sigmoid\\nfunction. Bias terms are implicitly represented in the above formulation\\nif xi and hi are augmented with ones.\\nGiven target vectors in the full training set with a total of\\nN samples, T =[ t1,..., ti,..., tN ], where each vector is ti =\\n[t1i, ··· ,t ji,...,t Ci]T, the parameters U and W are learned so as to\\nminimize the average of the total square error below:\\nE = 1\\n2\\n∑\\ni\\n∥ yi − ti∥ 2 = 1\\n2Tr[(Y − T)(Y − T)T]\\nwhere the output of the network is\\nyi = U Thi = UTσ(W Txi)= Gi(UW )', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='04fae5b0-3a6e-44ad-872f-b0a8a4943544', embedding=None, metadata={'page_label': '255', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4. The tensor deep stacking network 255\\nwhich depends on both weight matrices, as in the standard neural net.\\nAssuming H =[ h1,..., hi,..., hN ]i sk n o w n ,o re q u i v a l e n t l y ,W is\\nknown. Then, setting the error derivative with respective to U to zero\\ngives\\nU =( HH T)−1HTT =F (W ), where hi = σ(W Txi).\\nThis provides an explicit constraint between U and W which were\\ntreated independently in the conventional backpropagation algorithm.\\nNow, given the equality constraintU = F(W ), let’s use Lagrangian\\nmultiplier method to solve the optimization problem in learning W\\nOptimizing the Lagrangian:\\nE = 1\\n2\\n∑\\ni\\n∥ Gi(U , W ) − ti∥ 2 + λ∥ U − F(W )∥\\nwe can derive batch-mode gradient descent learning algorithm where\\nthe gradient takes the following form [106, 413]:\\n∂E\\n∂W = 2X[H T ◦ (1 − H )T ◦ [H †(HTT)(TH†) − TT(TH †)]],\\nwhere H† = H T(HH T)−1 is pseudo-inverse of H and symbol ◦\\ndenotes element-wise multiplication.\\nCompared with conventional backpropagation, the above method\\nhas less noise in gradient computation due to the exploitation of the\\nexplicit constraint U = F(W ). As such, it was found experimentally\\nthat, unlike backpropagation, batch training is eﬀective, which aids\\nparallel learning of the DSN.\\n6.4 The tensor deep stacking network\\nThe above DSN architecture has recently been generalized to its ten-\\nsorized version, which we call the tensor DSN (TDSN) [180, 181]. It\\nhas the same scalability as the DSN in terms of parallelizability in\\nlearning, but it generalizes the DSN by providing higher-order feature\\ninteractions missing in the DSN.\\nThe architecture of the TDSN is similar to that of the DSN in the\\nway that stacking operation is carried out. That is, modules of the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9ac16c23-72d9-46bd-a6be-b1330796f397', embedding=None, metadata={'page_label': '256', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='256 Deep Stacking Networks and Variants — Supervised Learning\\nFigure 6.2: Comparisons of a single module of a DSN (left) and that of a tensor\\nDSN (TDSN). Two equivalent forms of a TDSN module are shown to the right.\\n[after [180], @IEEE].\\nTDSN are stacked up in a similar way to form a deep architecture.\\nThe diﬀerences between the TDSN and the DSN lie mainly in how\\neach module is constructed. In the DSN, we have one set of hidden\\nunits forming a hidden layer, as denoted at the left panel of Figure 6.2.\\nIn contrast, each module of a TDSN contains two independent hidden\\nlayers, denoted as “Hidden 1” and “Hidden 2” in the middle and right\\npanels of Figure 6.2. As a result of this diﬀerence, the upper-layer\\nweights, denoted by “ U” in Figure 6.2, changes from a matrix (a two\\ndimensional array) in the DSN to a tensor (a three dimensional array)\\nin the TDSN, shown as a cube labeled by “ U” in the middle panel.\\nThe tensor U has a three-way connection, one to the prediction\\nlayer and the remaining to the two separate hidden layers. An equiva-\\nlent form of this TDSN module is shown in the right panel of Figure 6.2,\\nwhere the implicit hidden layer is formed by expanding the two sepa-\\nrate hidden layers into their outer product. The resulting large vector\\ncontains all possible pair-wise products for the two sets of hidden-layer\\nvectors. This turns tensor U into a matrix again whose dimensions are\\n(1) size of the prediction layer; and (2) product of the two hidden lay-\\ners’ sizes. Such equivalence enables the same convex optimization for\\nlearning U developed for the DSN to be applied to learning tensor U.\\nImportantly, higher-order hidden feature interactions are enabled in\\nthe TDSN via the outer product construction for the large, implicit\\nhidden layer.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8015d2c8-5500-4ceb-9bae-57e734ef3628', embedding=None, metadata={'page_label': '257', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5. The Kernelized deep stacking network 257\\nFigure 6.3: Stacking of TDSN modules by concatenating prediction vector with\\ninput vector. [after [180], @IEEE].\\nStacking the TDSN modules to form a deep architecture pursues in\\na similar way to the DSN by concatenating various vectors. Two exam-\\nples are shown in Figures 6.3 and 6.4. Note stacking by concatenating\\nhidden layers with input (Figure 6.4) would be diﬃcult for the DSN\\nsince its hidden layer tends to be too large for practical purposes.\\n6.5 The Kernelized deep stacking network\\nThe DSN architecture has also recently been generalized to its ker-\\nnelized version, which we call the kernel-DSN (K-DSN) [102, 171]. The\\nmotivation of the extension is to increase the size of the hidden units in\\neach DSN module, yet without increasing the size of the free parameters\\nto learn. This goal can be easily accomplished using the kernel trick,\\nresulting in the K-DSN which we describe below.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='abbb3d16-90a0-4084-8bea-874da85e468d', embedding=None, metadata={'page_label': '258', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='258 Deep Stacking Networks and Variants — Supervised Learning\\nFigure 6.4:Stacking of TDSN modules by concatenating two hidden-layers’ vectors\\nwith the input vector.\\nIn the DSN architecture reviewed above optimizing the weight\\nmatrix U given the hidden layers’ outputs in each module is a con-\\nvex optimization problem. However, the problem of optimizing weight\\nmatrixW and thus the whole network is nonconvex. In a recent exten-\\nsion of DSN, a tensor structure was imposed, shifting most of the\\nnonconvex learning burden for W to the convex optimization of U\\n[180, 181]. In the new K-DSN extension, we completely eliminate non-\\nconvex learning for W using the kernel trick.\\nTo derive the K-DSN architecture and the associated learning algo-\\nrithm, we ﬁrst take the bottom module of DSN as an example and\\ngeneralize the sigmoidal hidden layer hi = σ(W Txi)i nt h eD S Nm o d -\\nule into a generic nonlinear mapping function G(X) from the raw\\ninput feature X, with high dimensionality in G(X) (possibly inﬁnite)\\ndetermined only implicitly by a kernel function to be chosen. Second,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='911c9154-b467-4f34-bff6-d4dcf269983b', embedding=None, metadata={'page_label': '259', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5. The Kernelized deep stacking network 259\\nwe formulate the constrained optimization problem of\\nminimize 1\\n2Tr[EET]+ C\\n2 U TU\\nsubject to T − U TG(X)= E.\\nThird, we make use of dual representations of the above constrained\\noptimization problem to obtain U = GTa, where vector a takes the\\nfollowing form:\\na =( CI + K)−1T\\nand K = G(X)GT(X) is a symmetric kernel matrix with elements\\nKnm = gT(xn)g(xm).\\nFinally, for each new input vectorx in the test or dev set, we obtain\\nthe K-DSN (bottom) module’s prediction as\\ny(x)= UTg(x)= aTG(X)g(x)= kT(x)(CI + K)−1T,\\nwhere the kernel vector k(x) is so deﬁned that its elements have values\\nof kn(x)= k(xn, x)i nw h i c hxn is a training sample and x is the\\ncurrent test sample.\\nFor lth module in K-DCN where l ≥ 2, the kernel matrix is\\nmodiﬁed to\\nK = G([X|Y (l−1)| Y (l−2)| ... Y (1)])GT([X|Y (l−1)|Y (l−2)| ... Y (1)]).\\nThe key advantages of K-DSN can be analyzed as follows. First,\\nunlike DSN which needs to compute hidden units’ output, the K-DSN\\ndoes not need to explicitly compute hidden units’ outputG(X)o r\\nG([X|Y (l−1)|Y (l−2)| ... Y (1)]). When Gaussian kernels are used, ker-\\nnel trick equivalently gives us an inﬁnite number of hidden units with-\\nout the need to compute them explicitly. Further, we no longer need\\nto learn the lower-layer weight matrix W in DSN as described in [102]\\nand the kernel parameter (e.g., the single variance parameter σ in the\\nGaussian kernel) makes K-DSN much less subject to overﬁtting than\\nDSN. Figure 6.5 illustrates the basic architecture of a K-DSN using the\\nGaussian kernel and using three modules.\\nThe entire K-DSN with Gaussian kernels is characterized by two\\nsets of module-dependent hyper-parameters: σ(l) and C(l) the kernel', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4773c905-d82a-4e94-89d0-3efd9cf36157', embedding=None, metadata={'page_label': '260', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='260 Deep Stacking Networks and Variants — Supervised Learning\\nFigure 6.5: An example architecture of the K-DSN with three modules each of\\nwhich uses a Gaussian kernel with diﬀerent kernel parameters. [after [102], @IEEE].\\nsmoothing parameter and regularization parameter, respectively. While\\nboth parameters are intuitive and their tuning (via line search or leave-\\none-out cross validation) is straightforward for a single bottom mod-\\nule, tuning the full network with all the modules is more diﬃcult. For\\nexample, if the bottom module is tuned too well, then adding more\\nmodules would not beneﬁt much. In contrast, when the lower modules\\nare loosely tuned (i.e., relaxed from the results obtained from straight-\\nforward methods), the overall K-DSN often performs much better. The\\nexperimental results reported by Deng et al. [102] are obtained using a\\nset of empirically determined tuning schedules to adaptively regularize\\nthe K-DSN from bottom to top modules.\\nThe K-DSN described here has a set of highly desirable proper-\\nties from the machine learning and pattern recognition perspectives. It\\ncombines the power of deep learning and kernel learning in a principled', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c8095f67-4b05-493e-9c69-1690bfd028ac', embedding=None, metadata={'page_label': '261', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5. The Kernelized deep stacking network 261\\nway and unlike the basic DSN there is no longer nonconvex optimiza-\\ntion problem involved in training the K-DSN. The computation steps\\nmake the K-DSN easier to scale up for parallel computing in distributed\\nservers than the DSN and tensor-DSN. There are many fewer param-\\neters in the K-DSN to tune than in the DSN, T-DSN, and DNN, and\\nthere is no need for pre-training. It is found in the study of [102] that\\nregularization plays a much more important role in the K-DSN than\\nin the basic DSN and Tensor-DSN. Further, eﬀective regularization\\nschedules developed for learning the K-DSN weights can be motivated\\nby intuitive insight from useful optimization tricks such as the heuristic\\nin Rprop or resilient backpropagation algorithm [302].\\nHowever, as inherent in any kernel method, the scalability becomes\\nan issue also for the K-DSN as the training and testing samples become\\nvery large. A solution is provided in the study by Huang et al. [171],\\nbased on the use of random Fourier features, which possess the strong\\ntheoretical property of approximating the Gaussian kernel while render-\\ning eﬃcient computation in both training and evaluation of the K-DSN\\nwith large training samples. It is empirically demonstrated that just like\\nthe conventional K-DSN exploiting rigorous Gaussian kernels, the use\\nof random Fourier features also enables successful stacking of kernel\\nmodules to form a deep architecture.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='373730dc-2a2c-44b8-9c37-ac83c610cf72', embedding=None, metadata={'page_label': '262', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7\\nSelected Applications in Speech\\nand Audio Processing\\n7.1 Acoustic modeling for speech recognition\\nAs discussed in Section 2, speech recognition is the very ﬁrst success-\\nful application of deep learning methods at an industry scale. This\\nsuccess is a result of close academic-i ndustrial collaboration, initiated\\nat Microsoft Research, with the involved researchers identifying and\\nacutely attending to the industrial need for large-scale deployment\\n[68, 89, 109, 161, 323, 414]. It is also a result of carefully exploiting\\nthe strengths of the deep learning and the then-state-of-the-art speech\\nrecognition technology, including notably the highly eﬃcient decoding\\ntechniques.\\nSpeech recognition has long been dominated by the GMM–HMM\\nmethod, with an underlying shallow or ﬂat generative model of context-\\ndependent GMMs and HMMs (e.g., [92, 93, 187, 293]). Neural networks\\nonce were a popular approach but had not been competitive with the\\nGMM–HMM [42, 87, 261, 382]. Generative models with deep hidden\\ndynamics likewise have also not been clearly competitive (e.g., [45, 73,\\n108, 282]).\\nDeep learning and the DNN started making their impact in speech\\nrecognition in 2010, after close collaborations between academic and\\n262', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='13dadc12-7923-4d18-b588-fae2caa74019', embedding=None, metadata={'page_label': '263', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 263\\nindustrial researchers; see reviews in [89, 161]. The collaborative work\\nstarted in phone recognition tasks [89, 100, 135, 136, 257, 260, 258,\\n309, 311, 334], demonstrating the power of hybrid DNN architec-\\ntures discussed in Section 5 and of subsequent new architectures\\nwith convolutional and recurrent structure. The work also showed\\nthe importance of raw speech features of spectrogram — back from\\nthe long-popular MFCC features toward but not yet reaching the\\nraw speech-waveform level [183, 327]. The collaboration continued to\\nlarge vocabulary tasks with more convincing, highly positive results\\n[67, 68, 94, 89, 161, 199, 195, 223, 323, 353, 399, 414]. The success in\\nlarge vocabulary speech recognition is in large part attributed to the\\nuse of a very large DNN output layer structured in the same way as\\nthe GMM–HMM speech units (senones), motivated partially by the\\nspeech researchers’ desires to take advantage of the context-dependent\\nphone modeling techniques that have been proven to work well in the\\nGMM–HMM framework, and to keep the change of the already highly\\neﬃcient decoder software’s infrastructure developed for the GMM–\\nHMM systems to a minimum. In the meantime, this body of work\\nalso demonstrated the possibility to reduce the need for the DBN-\\nlike pre-training in eﬀective learning of DNNs when a large amount\\nof labeled data is available. A combination of three factors helped\\nto quickly spread the success of deep learning in speech recognition\\nto the entire speech industry and academia: (1) signiﬁcantly lowered\\nerrors compared with the then-state-of-the-art GMM-HMM systems;\\n(2) minimal decoder changes required to deploy the new DNN-based\\nspeech recognizer due to the use of senones as the DNN output; and\\n(3) reduced system complexity empowered by the DNN’s strong mod-\\neling power. By the ICASSP-2013 timeframe, at least 15 major speech\\nrecognition groups worldwide conﬁrmed experimentally the success of\\nD N N sw i t hv e r yl a r g et a s k sa n dw i t ht h eu s eo fr a ws p e e c hs p e c t r a l\\nfeatures other than MFCCs. The most notable groups include major\\nindustrial speech labs worldwide: Microsoft [49, 89, 94, 324, 399, 430],\\nIBM [195, 309, 311, 307, 317], Google [69, 150, 184, 223], iFlyTek, and\\nBaidu. Their results represent a new state-of-the-art in speech recog-\\nnition widely deployed in these companies’ voice products and services\\nwith extensive media coverage in recent years.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='81da5712-d036-4edc-b95a-d67238cd7f8d', embedding=None, metadata={'page_label': '264', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='264 Selected Applications in Speech and Audio Processing\\nIn the remainder of this chapter, we review a wide range of speech\\nrecognition work based on deep learning methods according to several\\nmajor themes expressed in the section titles.\\n7.1.1 Back to primitive spectral features of speech\\nDeep learning, also referred as representation learning or (unsuper-\\nvised) feature learning, sets an important goal of automatic discovery\\nof powerful features from raw input data independent of application\\ndomains. For speech feature learning and for speech recognition, this\\ngoal is condensed to the use of primitive spectral or possibly wave-\\nform features. Over the past 30 years or so, largely “hand-crafted”\\ntransformations of speech spectrogram have led to signiﬁcant accuracy\\nimprovements in the GMM-based HMM systems, despite the known\\nloss of information from the raw speech data. The most successful\\ntransformation is the non-adaptive cosine transform, which gave rise to\\nMel-frequency cepstral coeﬃcients (MFCC) features. The cosine trans-\\nform approximately de-correlates feature components, which is impor-\\ntant for the use of GMMs with diagonal covariance matrices. However,\\nwhen GMMs are replaced by deep learning models such as DNNs, deep\\nbelief nets (DBNs), or deep autoencoders, such de-correlation becomes\\nirrelevant due to the very strength of the deep learning methods in\\nmodeling data correlation. As discussed in detail in Section 4, early\\nwork of [100] demonstrated this strength and in particular the beneﬁt\\nof spectrograms over MFCCs in eﬀective coding of bottleneck speech\\nfeatures using autoencoders in an unsupervised manner.\\nThe pipeline from speech waveforms (raw speech features) to\\nMFCCs and their temporal diﬀerences goes through intermediate\\nstages of log-spectra and then (Mel-warped) ﬁlter-banks, with learned\\nparameters based on the data. An important character of deep learn-\\ning is to move away from separate design of feature representations and\\nof classiﬁers. This idea of jointly learning classiﬁer and feature trans-\\nformation for speech recognition was already explored in early studies\\non the GMM–HMM based systems; e.g., [33, 50, 51, 299]. However,\\ngreater speech recognition performance gain is obtained only recently', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d5283f63-27ff-4688-b8f5-4edb88c1decc', embedding=None, metadata={'page_label': '265', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 265\\nin the recognizers empowered by deep learning methods. For example,\\nMohamed et al. [259], Li et al. [221], and Deng et al. [94] showed signif-\\nicantly lowered speech recognition errors using large-scale DNNs when\\nmoving from the MFCC features back to more primitive (Mel-scaled)\\nﬁlter-bank features. These results indicate that DNNs can learn a bet-\\nter transformation than the original ﬁxed cosine transform from the\\nMel-scaled ﬁlter-bank features.\\nCompared with MFCCs, “raw” spectral features not only retain\\nmore information, but also enable the use of convolution and pool-\\ning operations to represent and handle some typical speech variabil-\\nity — e.g., vocal tract length diﬀerences across speakers, distinct speak-\\ning styles causing formant undershoot or overshoot, etc. — expressed\\nexplicitly in the frequency domain. For example, the convolutional neu-\\nral network (CNN) can only be meaningfully and eﬀectively applied to\\nspeech recognition [1, 2, 3, 94] when spectral features, instead of MFCC\\nfeatures, are used.\\nMore recently, Sainath et al. [307] went one step further toward\\nraw features by learning the parameters that deﬁne the ﬁlter-banks\\non power spectra. That is, rather than using Mel-warped ﬁlter-bank\\nfeatures as the input features as in [1, 3, 50, 221], the weights corre-\\nsponding to the Mel-scale ﬁlters are only used to initialize the param-\\neters, which are subsequently learned together with the rest of the\\ndeep network as the classiﬁer. The overall architecture of the jointly\\nlearned feature generator and classiﬁer is shown in Figure 7.1. Substan-\\ntial speech recognition error reduction is reported in [307].\\nIt has been shown that not only learning the spectral aspect of\\nthe features are beneﬁcial for speech recognition, learning the tempo-\\nral aspect of the features is also helpful [332]. Further, Yu et al. [426]\\ncarefully analyzed the properties of diﬀerent layers in the DNN as the\\nlayer-wise extracted features starting from the lower raw ﬁlter-bank\\nfeatures. They found that the improved speech recognition accuracy\\nachieved by the DNNs partially attributes to DNN’s ability to extract\\ndiscriminative internal representations that are robust to the many\\nsources of variability in speech signals. They also show that these rep-\\nresentations become increasingly insensitive to small perturbations in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7bb0f1d5-04e2-45e9-bcb7-880939fc696a', embedding=None, metadata={'page_label': '266', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='266 Selected Applications in Speech and Audio Processing\\nFigure 7.1:Illustration of the joint learning of ﬁlter parameters and the rest of the\\ndeep network. [after [307], @IEEE].\\nthe input at higher layers, which helps to achieve better speech recog-\\nnition accuracy.\\nTo the extreme end, deep learning would promote to use the lowest\\nlevel of raw features of speech, i.e., speech sound waveforms, for speech\\nrecognition, and learn the transformation automatically. As an initial\\nattempt toward this goal the study carried out by Jaitly and Hinton\\n[183] makes use of speech sound waves as the raw input feature to an\\nRBM with a convolutional structure as the classiﬁer. With the use\\nof rectiﬁed linear units in the hidden layer [130], it is possible, to a\\nlimited extent, to automatically normalize the amplitude variation\\nin the waveform signal. Although the ﬁnal results are disappointing,\\nthe work shows that much work is needed along this direction. For\\nexample, just as demonstrated by Sainath et al. [307] that the use of\\nraw spectra as features requires additional attention in normalization\\nthan MFCCs, the use of speech waveforms demands even more\\nattention in normalization [327]. This is true for both GMM-based\\nand deep learning based methods.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7c3e6b49-d7e7-432b-863c-3798cc6fff1b', embedding=None, metadata={'page_label': '267', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 267\\n7.1.2 The DNN–HMM architecture versus use of\\nDNN-derived features\\nAnother major theme in the recent studies reported in the literature on\\napplying deep learning methods to speech recognition is two disparate\\nways of using the DNN: (1) Direct applications of the DNN-HMM\\narchitecture as discussed in Section 5.3 to perform speech recognition;\\nand (2) The use of DNNs to extract or derive features, which are then\\nfed into a separate sequence classiﬁer. In the speech recognition lit-\\nerature [42], a system, in which a neural network’s output is directly\\nused to estimate the emission probabilities of an HMM, is often called\\nan ANN/HMM hybrid system. This should be distinguished from the\\nuse of “hybrid” in Section 5 and throughout this monograph, where\\na hybrid of unsupervised pre-training and of supervised ﬁne tuning is\\nexploited to learn the parameters of DNNs.\\n7.1.2.1 The DNN–HMM architecture as a recognizer\\nAn early DNN–HMM architecture [257] was presented at the NIPS\\nWorkshop [109], developed, analyzed, and assisted by University of\\nToronto and MSR speech researchers. In this work, a ﬁve-layer DNN\\n(called the DBN in the paper) was used to replace the Gaussian mixture\\nmodels in the GMM–HMM system, and the monophone state was used\\nas the modeling unit. Although monophones are generally accepted\\nas a weaker phonetic representation than triphones, the DNN–HMM\\napproach with monophones was shown to achieve higher phone recog-\\nnition accuracy than the state-of-the-art triphone GMM–HMM sys-\\ntems. Further, the DNN results were found to be slightly superior to\\nthe then-best-performing single system based on the generative hid-\\nden trajectory model (HTM) in the literature [105, 108] evaluated on\\nthe same, commonly used TIMIT task by many speech researchers\\n[107, 108, 274, 313]. At MSR, Redmond, the error patterns produced\\nby these two separate systems (the DNN vs. the HTM) were carefully\\nanalyzed and found to be very diﬀerent, reﬂecting distinct core capa-\\nbilities of the two approaches and igniting intensive further studies on\\nthe DNN–HMM approach described below.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='36a8faf2-ca0a-4f3e-bddc-2c90b7967311', embedding=None, metadata={'page_label': '268', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='268 Selected Applications in Speech and Audio Processing\\nMSR and University of Toronto researchers [67, 68, 414] extended\\nthe DNN–HMM system from the monophone phonetic representation of\\nthe DNN outputs to the triphone or context-dependent counterpart and\\nfrom phone recognition to large vocabulary speech recognition. Experi-\\nments conducted at MSR on the 24-hour and 48-hour Bing mobile voice\\nsearch datasets collected under the real usage scenario demonstrate\\nthat the context-dependent DNN–HMM signiﬁcantly outperforms the\\nstate-of-the-art GMM-HMM system. Three factors, in addition to the\\nuse of the DNN, contribute to the success: the use of tied triphones\\nas the DNN modeling units, the use of the best available tri-phone\\nGMM–HMM to generate the tri-phone state alignment, and the eﬀec-\\ntive exploitation of a long window of input features. Experiments also\\nindicate that the decoding time of a ﬁve-layer DNN–HMM is almost\\nthe same as that of the state-of-the-art triphone GMM–HMM.\\nThe success was quickly extended to large vocabulary speech recog-\\nnition tasks with hundreds and even thousands of hours of training set\\nand with thousands of tri-phone states, including the Switchboard and\\nBroadcast News databases, and Google’s voice search and YouTube\\ntasks [94, 161, 184, 309, 311, 324]. For example, on the Switchboard\\nbenchmark, the context-dependent DNN–HMM (CD-DNN–HMM) is\\nshown to cut error by one third compared to the state-of-the-art GMM–\\nHMM system [323]. As a summary, we show in Table 7.1 some quanti-\\ntative recognition error rates in relatively early literature produced by\\nthe basic DNN–HMM architecture in comparison with those by the pre-\\nvious state-of-the-art systems based on the generative models. (More\\nadvanced architectures have produced better results than shown here).\\nNote from sub-tables A to D, the training data are increased approx-\\nimately one order of magnitude from one task to the next. Not only\\nthe computation scales up well (i.e., almost linearly) with the training\\nsize, but most importantly the relative error rate reduction increases\\nsubstantially with increasing amounts of training data — from approx-\\nimately 10% to 20%, and then to 30%. This set of results highlight the\\nstrongly desirable properties of the DNN-based methods, despite the\\nconceptual simplicity of the overall DNN–HMM architecture and some\\nknown weaknesses.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8f0e490e-6415-4988-977b-ab0e8514300a', embedding=None, metadata={'page_label': '269', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 269\\nTable 7.1:Comparisons of the DNN–HMM architecture with the generative model\\n(e.g., the GMM–HMM) in terms of phone or word recognition error rates. From\\nsub-tables A to D, the training data are increased approximately three orders of\\nmagnitudes.\\nFeatures Setup Error Rates\\nA: TIMIT Phone recognition (3 hours of training)\\nGMM w. Hidden dynamics 24.8%\\nDNN 5 layers × 2048 23.0%\\nB: Voice Search SER (24–48 hours of training)\\nGMM MPE (760 24-mix) 36.2%\\nDNN 5 layers × 2048 30.1%\\nC: Switch Board WER (309 hours of training)\\nGMM BMMI (9K 40-mix) 23.6%\\nDNN 7 layers × 2048 15.8%\\nD: Switch Board WER (2000 hours of training)\\nGMM BMMI (18K 72-mix) 21.7%\\nDNN 7 layers × 2048 14.6%\\n7.1.2.2 The use of DNN-derived fea tures in a separate recognizer\\nOne clear weakness of the above DNN–HMM architecture for speech\\nrecognition is that much of the highly eﬀective techniques for the\\nGMM–HMM systems, including discriminative training (in both fea-\\nture space and model space), unsupervised speaker adaptation, noise\\nrobustness, and scalable batch training tools for big training data,\\ndeveloped over the past 20 some years may not be directly applica-\\nble to the new systems although similar techniques have been recently\\ndeveloped for DNN–HMMs. To remedy this problem, the “tandem”\\napproach, developed originally by Hermansky et al. [154], has been\\nadopted, where the output of the neural networks in the form of pos-\\nterior probabilities for phone classes, are used, often in conjunction\\nwith the acoustic features to form new augmented input features, in a\\nseparate GMM–HMM system.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4f7fe912-0fa8-4a1e-921f-eb121c29bc97', embedding=None, metadata={'page_label': '270', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='270 Selected Applications in Speech and Audio Processing\\nThis tandem approach is used by Vinyals and Ravuri [379] where a\\nDNN’s outputs are extracted to serve as the features for mismatched\\nnoisy speech. It is reported that DNNs outperform the neural net-\\nworks with a single hidden layer under the clean condition, but the\\ngains slowly diminish as the noise level is increased. Furthermore, using\\nMFCCs in conjunction with the posteriors computed from DNNs out-\\nperforms using the DNN features alone in low to moderate noise con-\\nditions with the tandem architecture. Comparisons of such tandem\\napproach with the direct DNN–HMM approach are made by Tüske\\net al. [368] and Imseng et al. [182].\\nAn alternative way of extracting the DNN features is to use the\\n“bottleneck” layer, which is narrower than other layers in the DNN,\\nto restrict the capacity of the network. Then, such bottleneck features\\nare fed to a GMM–HMM system, often in conjunction with the orig-\\ninal acoustic features and some dimensionality reduction techniques.\\nThe bottleneck features derived from the DNN are believed to capture\\ninformation complementary to conventional acoustic features derived\\nfrom the short-time spectra of the input. A speech recognizer based on\\nthe above bottleneck feature approach is built by Yu and Seltzer [425],\\nwith the overall architecture shown in Figure 7.2. Several variants of\\nthe DNN-based bottleneck-feature approach have been explored; see\\ndetails in [16, 137, 201, 285, 308, 368].\\nYet another method to derive the features from the DNN is to feed\\nits top-most hidden layer as the new features for a separate speech\\nFigure 7.2: Illustration of the use of bottleneck (BN) features extracted from a\\nDNN in a GMM–HMM speech recognizer. [after [425], @IEEE].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7723b002-0665-4b1c-b85b-2611d67e98cf', embedding=None, metadata={'page_label': '271', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 271\\nrecognizer. In [399], a GMM–HMM is used as such a recognizer, and\\nthe high-dimensional, DNN-derived features are subject to dimension-\\nality reduction before feeding them into the recognizer. More recently,\\na recurrent neural network (RNN) is used as the “backend” recognizer\\nreceiving the high-dimensional, DNN-derived fea tures as the input\\nwithout dimensionality reduction [48, 85]. These studies also show\\nthat the use of the top-most hidden layer of the DNN as features is\\nbetter than other hidden layers and also better than the output layer\\nin terms of recognition accuracy for the RNN sequence classiﬁer.\\n7.1.3 Noise robustness by deep learning\\nThe study of noise robustness in speech recognition has a long his-\\ntory, mostly before the recent rise of deep learning. One major con-\\ntributing factor to the often observed brittleness of speech recogni-\\ntion technology is the inability of the standard GMM–HMM-based\\nacoustic model to accurately model noise-distorted speech test data\\nthat diﬀers in character from the training data, which may or may\\nnot be distorted by noise. A wide range of noise-robust techniques\\ndeveloped over past 30 years can be analyzed and categorized using\\nﬁve diﬀerent criteria: (1) feature-domain versus model-domain pro-\\ncessing, (2) the use of prior knowledge about the acoustic environ-\\nment distortion, (3) the use of explicit environment-distortion mod-\\nels, (4) deterministic versus uncertainty processing, and (5) the use of\\nacoustic models trained jointly with the same feature enhancement or\\nmodel adaptation process used in the testing stage. See a comprehen-\\nsive review in [220] and some additional review literature or original\\nwork in [4, 82, 119, 140, 230, 370, 404, 431, 444].\\nMany of the model-domain techniques developed for GMM–HMMs\\n(e.g., model-domain noise robustness techniques surveyed by Li et al.\\n[220] and Gales [119]) are not directly applicable to the new deep\\nlearning models for speech recognition. The feature-domain techniques,\\nhowever, can be directly applied to the DNN system. A detailed inves-\\ntigation of the use of DNNs for noise robust speech recognition in the\\nfeature domain was reported by Seltzer et al. [325], who applied the\\nC-MMSE [415] feature enhancement algorithm on the input feature', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='415ece0a-90ac-49fd-b616-dd64ad3e0a4f', embedding=None, metadata={'page_label': '272', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='272 Selected Applications in Speech and Audio Processing\\nused in the DNN. By processing both the training and testing data\\nwith the same algorithm, any consistent errors or artifacts introduced\\nby the enhancement algorithm can be learned by the DNN–HMM rec-\\nognizer. This study also successfully explored the use of the noise aware\\ntraining paradigm for training the DNN, where each observation was\\naugmented with an estimate of the noise. Strong results were obtained\\non the Aurora4 task. More recently, Kashiwagi et al. [191] applied the\\nSPLICE feature enhancement technique [82] to a DNN speech rec-\\nognizer. In that study the DNN’s output layer was determined on\\nclean data instead of on noisy data as in the study reported by Seltzer\\net al. [325].\\nBesides DNN, other deep architectures have also been proposed to\\nperform feature enhancement and noise-robust speech recognition. For\\nexample, Mass et al. [235] applied a deep recurrent auto encoder neural\\nnetwork to remove noise in the input features for robust speech recogni-\\ntion. The model was trained on stereo (noisy and clean) speech features\\nto predict clean features given noisy input, similar to the SPLICE setup\\nbut using a deep model instead of a GMM. Vinyals and Ravuri [379]\\ninvestigated the tandem approaches to noise-robust speech recognition,\\nwhere DNNs were trained directly with noisy speech to generate pos-\\nterior features. Finally, Rennie et al. [300] explored the use of a version\\nof the RBM, called the factorial hidden RBM, for noise-robust speech\\nrecognition.\\n7.1.4 Output representations in the DNN\\nMost deep learning methods for speech recognition and other infor-\\nmation processing applications have focused on learning represen-\\ntations from input acoustic features without paying attention to\\noutput representations. The recent 2013 NIPS Workshop on Learning\\nOutput Representations (http://nips.cc/Conferences/2013/Program/\\nevent.php?ID=3714) was dedicated to bridging this gap. For exam-\\nple, the Deep Visual-Semantic Embedding Model described in [117],\\nto be discussed more in Section 11) exploits continuous-valued out-\\nput representations obtained from the text embeddings to assist in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='146b3f7b-2b71-4b41-bbac-3eb59597d869', embedding=None, metadata={'page_label': '273', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 273\\nbranch of the deep network for classifying images. For speech recogni-\\ntion, the importance of designing eﬀective linguistic representations for\\nthe output layers of deep networks is highlighted in [79].\\nMost current DNN systems use a high-dimensional output represen-\\ntation to match the context-dependent phonetic states in the HMMs.\\nFor this reason, the output layer evaluation can cost 1/3 of the total\\ncomputation time. To improve the decoding speed, techniques such\\nas low-rank approximation is typically applied to the output layer.\\nIn [310] and [397], the DNN with high-dimensional output layer was\\ntrained ﬁrst. The singular value decomposition (SVD)-based dimen-\\nsion reduction technique was then performed on the large output-layer\\nmatrix. The resulting matrices are further combined and as the result\\nthe original large weight matrix is approximated by a product of two\\nmuch smaller matrices. This technique in essence converts the origi-\\nnal large output layer to two layers — a bottleneck linear layer and\\na nonlinear output layer — both with smaller weight matrices. The\\nconverted DNN with reduced dimensionality is further reﬁned. The\\nexperimental results show that no speech recognition accuracy reduc-\\ntion was observed even when the size is cut to half, while the run-time\\ncomputation is signiﬁcantly reduced.\\nThe output representations for speech recognition can beneﬁt from\\nthe structured design of the symbolic or phonological units of speech\\nas presented in [79]. The rich phonological structure of symbolic nature\\nin human speech has been well known for many years. Likewise, it has\\nalso been well understood for a long time that the use of phonetic\\nor its ﬁner state sequences, even with contextual dependency, in engi-\\nneering speech recognition systems, is inadequate in representing such\\nrich structure [86, 273, 355], and thus leaving a promising open direc-\\ntion to improve the speech recognition systems’ performance. Basic\\ntheories about the internal structure of speech sounds and their rel-\\nevance to speech recognition technology in terms of the speciﬁcation,\\ndesign, and learning of possible output representations of the underly-\\ning speech model for speech target sequences are surveyed in [76] and\\nmore recently in [79].\\nThere has been a growing body of deep learning work in speech\\nrecognition with their focus placed on designing output representations', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='584c0873-c774-451f-b938-fe1e04a72679', embedding=None, metadata={'page_label': '274', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='274 Selected Applications in Speech and Audio Processing\\nrelated to linguistic structure. In [383, 384], a limitation of the out-\\nput representation design, based on the context-dependent phone units\\nas proposed in [67, 68], is recognized and a solution is oﬀered. The\\nroot cause of this limitation is that all context-dependent phone states\\nwithin a cluster created by the decision tree share the same set of\\nparameters and this reduces its resolution power for ﬁne-grained states\\nduring the decoding phase. The solution proposed formulates output\\nrepresentations of the context-dependent DNN as an instance of the\\ncanonical state modeling technique, making use of broad phonetic\\nclasses. First, triphones are clustered into multiple sets of shorter bi-\\nphones using broad phone contexts. Then, the DNN is trained to dis-\\ncriminate the bi-phones within each set. Logistic regression is used\\nto transform the canonical states into the detailed triphone state\\noutput probabilities. That is, the overall design of the output rep-\\nresentation of the context-dependent DNN is hierarchical in nature,\\nsolving both the data sparseness and low-resolution problems at the\\nsame time.\\nRelated work on designing the output linguistic representations for\\nspeech recognition can be found in [197] and in [241]. While the designs\\nare in the context of GMM–HMM-based speech recognition systems,\\nthey both can be extended to deep learning models.\\n7.1.5 Adaptation of the DNN-based speech recognizers\\nThe DNN–HMM is an advanced version of the artiﬁcial neural net-\\nwork and HMM “hybrid” system developed in 1990s, for which several\\nadaptation techniques have been developed. Most of these techniques\\nare based on linear transformation of the network weights of either\\ninput or output layers. A number of exploratory studies on DNN adap-\\ntation made use of the same or related linear transformation methods\\n[223, 401, 402]. However, compared with the earlier narrower and shal-\\nlower neural network systems, the DNN–HMM has signiﬁcantly more\\nparameters due to wider and deeper hidden layers used and the much\\nlarger output layer designed to model context dependent phones and\\nstates. This diﬀerence casts special challenges to adapting the DNN–\\nHMM, especially when the adaptation data is small. Here we discuss', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='810e7b8e-ac48-4739-9a45-85db8c647031', embedding=None, metadata={'page_label': '275', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 275\\nrepresentative recent studies on overcoming such challenges in adapting\\nthe large-sized DNN weights in distinct ways.\\nYu et al. [430] proposed a regularized adaptation technique for\\nDNNs. It adapts the DNN weights conservatively by forcing the distri-\\nbution estimated from the adapted model to be close to that estimated\\nfrom those before the adaptation. This constraint is realized by adding\\nKullback–Leibler divergence (KLD) regularization to the adaptation\\ncriterion. This type of regularization is shown to be equivalent to a\\nmodiﬁcation of the target distribution in the conventional backprop-\\nagation algorithm and thus the training of the DNN remains largely\\nunchanged. The new target distribution is derived to be a linear inter-\\npolation of the distribution estimated from the model before adaptation\\nand the ground truth alignment of the adaptation data. This interpola-\\ntion prevents overtraining by keeping the adapted model from straying\\ntoo far from the speaker-independent model. This type of adaptation\\ndiﬀers from L2 regularization, which constrains the model parameters\\nthemselves rather than the output probabilities.\\nIn [330], adaptation of the DNN was applied not on the conventional\\nnetwork weights but on the hidden activation functions. In this way, the\\nmain limitation of current adaptation techniques based on adaptable\\nlinear transformation of the network weights in either the input or the\\noutput layer is eﬀectively overcome, since the new method only needs\\nto adapt a more limited number of hidden activation functions.\\nSeveral studies were carried out on unsupervised or semi-supervised\\nadaptation of DNN acoustic models with diﬀerent types of input fea-\\ntures with success [223, 405].\\nMost recently, Saon et al. [317] explored a new and highly eﬀective\\nmethod in adapting DNNs for speech recognition. The method com-\\nbined I-vector features with fMLLR (feature-domain max-likelihood\\nlinear regression) features as the input into a DNN. I-vectors or\\n(speaker) identity vectors are commonly used for speaker veriﬁca-\\ntion and speaker recognition applications, as they encapsulate relevant\\ninformation about a speaker’s identity in a low-dimensional feature\\nvector. The fMLLR is an eﬀective adaptation technique developed for\\nGMM–HMM systems. Since I-vectors do not obey locality in frequency,\\nthey must be combined carefully with the fMLLR features that obey', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='15cac8eb-b9b4-47da-ac49-65a924a27f32', embedding=None, metadata={'page_label': '276', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='276 Selected Applications in Speech and Audio Processing\\nlocality. The architecture of the multi-scale CNN–DNN was shown to\\nbe eﬀective for the combination of these two diﬀerent types of features.\\nDuring both training and decoding, the speaker-speciﬁc I-vector was\\nappended to the frame-based fMLLR features.\\n7.1.6 Better architect ures and nonlinear units\\nOver recent years, since the success of the (fully-connected) DNN–\\nHMM hybrid system was demonstrated in [67, 68, 109, 161, 257, 258,\\n308, 309, 324, 429], many new architectures and nonlinear units have\\nbeen proposed and evaluated for speech recognition. Here we provide\\nan overview of this progress, extending the overview provided in [89].\\nThe tensor version of the DNN is reported by Yu et al. [421, 422],\\nwhich extends the conventional DNN by replacing one or more of its\\nlayers with a double-projection layer and a tensor layer. In the double-\\nprojection layer, each input vector is projected into two nonlinear sub-\\nspaces. In the tensor layer, two subspace projections interact with each\\nother and jointly predict the next layer in the overall deep architecture.\\nAn approach is developed to map the tensor layers to the conventional\\nsigmoid layers so that the former can be treated and trained in a simi-\\nlar way to the latter. With this mapping the tensor version of the DNN\\ncan be treated as the DNN augmented with double-projection layers\\nso that the backpropagation learning algorithm can be cleanly derived\\nand relatively easily implemented.\\nA related architecture to the above is the tensor version of the DSN\\ndescribed in Section 6, also usefully applied to speech classiﬁcation\\nand recognition [180, 181]. The same approach applies to mapping the\\ntensor layers (i.e., the upper layer in each of the many modules in the\\nDSN context) to the conventional sigmoid layers. Again, this mapping\\nsimpliﬁes the training algorithm so that it becomes not so far apart\\nfrom that for the DSN.\\nAs discussed in Section 3.2, the concept of convolution in time\\nwas originated in the TDNN (time-delay neural network) as a shallow\\nneural network [202, 382] developed during early days of speech\\nrecognition. Only recently and when deep architectures (e.g. deep\\nconvolutional neural network or deep CNN) were used, it has been', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ef5482c0-2eb2-4421-a2c9-add18516d8ef', embedding=None, metadata={'page_label': '277', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 277\\nfound that frequency-dimension weight sharing is more eﬀective\\nfor high-performance phone recognition, when the HMM is used to\\nhandle the time variability, than time-domain weight sharing as in the\\nprevious TDNN in which the HMM was not used [1, 2, 3, 81]. These\\nstudies also show that designing the pooling scheme in the deep CNN\\nto properly trade-oﬀ between invariance to vocal tract length and\\ndiscrimination among speech sounds, together with a regularization\\ntechnique of “dropout” [166], leads to even better phone recognition\\nperformance. This set of work further points to the direction of\\ntrading-oﬀ between trajectory discrimination and invariance expressed\\nin the whole dynamic pattern of speech deﬁned in mixed time and\\nfrequency domains using convolution and pooling. Moreover, the\\nmost recent studies reported in [306, 307, 312] show that CNNs also\\nbeneﬁt large vocabulary continuous speech recognition. They further\\ndemonstrate that multiple convolutional layers provide even more\\nimprovement when the convolutional layers use a large number of\\nconvolution kernels or feature maps. In particular, Sainath et al. [306]\\nextensively explored many variants of the deep CNN. In combination\\nwith several novel methods the deep CNN is shown to produce state\\nof the art results in a few large vocabulary speech recognition tasks.\\nIn addition to the DNN, CNN, and DSN, as well as their tensor\\nversions, other deep models have also been developed and reported in\\nthe literature for speech recognition. For example, the deep-structured\\nCRF, which stacks many layers of CRFs, have been usefully applied\\nto the task of language identiﬁcation [429], phone recognition [410],\\nsequential labeling in natural language processing [428], and conﬁ-\\ndence calibration in speech recognition [423]. More recently, Demuynck\\nand Triefenbach [70] developed the deep GMM architecture, where the\\naspects of DNNs that lead to strong performance are extracted and\\napplied to build hierarchical GMMs. They show that by going “deep\\nand wide” and feeding windowed probabilities of a lower layer of GMMs\\nto a higher layer of GMMs, the performance of the deep-GMM system\\ncan be made comparable to a DNN. One advantage of staying in the\\nGMM space is that the decades of work in GMM adaptation and dis-\\ncriminative learning remains applicable.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1cd7e459-4bbb-4f63-a49a-85d52108b784', embedding=None, metadata={'page_label': '278', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='278 Selected Applications in Speech and Audio Processing\\nPerhaps the most notable deep architecture among all is the recur-\\nrent neural network (RNN) as well as its stacked or deep versions\\n[135, 136, 153, 279, 377]. While the RNN saw its early success in phone\\nrecognition [304], it was not easy to duplicate due to the intricacy\\nin training, let alone to scale up for larger speech recognition tasks.\\nLearning algorithms for the RNN have been dramatically improved\\nsince then, and much better results have been obtained recently using\\nthe RNN [48, 134, 235], especially when the bi-directional LSTM (long\\nshort-term memory) is used [135, 136]. The basic information ﬂow in\\nthe bi-directional RNN and a cell of LSTM is shown in Figures 7.3 and\\n7.4, respectively.\\nLearning the RNN parameters is known to be diﬃcult due to van-\\nishing or exploding gradients [280]. Chen and Deng [48] and Deng and\\nFigure 7.3: Information ﬂow in the bi-directional RNN, with both diagrammatic\\nand mathematical descriptions. W’s are weight matrices, not shown but can be easily\\ninferred in the diagram. [after [136], @IEEE].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3da34d50-8ecf-4aba-9f62-9c64ef12cf6f', embedding=None, metadata={'page_label': '279', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 279\\nFigure 7.4:Information ﬂow in an LSTM unit of the RNN, with both diagrammatic\\nand mathematical descriptions. W’s are weight matrices, not shown but can easily\\nbe inferred in the diagram. [after [136], @IEEE].\\nChen [85] developed a primal-dual training method that formulates\\nthe learning of the RNN as a formal optimization problem, where cross\\nentropy is maximized subject to the condition that the inﬁnity norm of\\nthe recurrent matrix of the RNN is less than a ﬁxed value to guarantee\\nthe stability of RNN dynamics. Experimental results on phone recog-\\nnition demonstrate: (1) the primal-dual technique is highly eﬀective\\nin learning RNNs, with superior performance to the earlier heuristic\\nmethod of truncating the size of the gradient; (2) The use of a DNN\\nto compute high-level features of speech data to feed into the RNN\\ngives much higher accuracy than without using the DNN; and (3) The', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='990de348-03f3-4dea-8412-4d96e1f19a0f', embedding=None, metadata={'page_label': '280', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='280 Selected Applications in Speech and Audio Processing\\naccuracy drops progressively as the DNN features are extracted from\\nhigher to lower hidden layers of the DNN.\\nA special case of the RNN is reservoir models or echo state networks,\\nwhere the output layers are ﬁxed to be linear instead of nonlinear as\\nin the regular RNN, and where the recurrent matrices are carefully\\ndesigned but not learned. The input matrices are also ﬁxed and not\\nlearned, due partly to the diﬃculty of learning. Only the weight matri-\\nces between the hidden and output layers are learned. Since the output\\nlayer is linear, the learning is very eﬃcient and with global optimum\\nachievable by a closed-form solution. But due to the fact that many\\nparameters are not learned, the hidden layer needs to be very large\\nin order to obtain good results. Triefenbach et al. [365] applied such\\nmodels to phone recognition, with reasonably good accuracy obtained.\\nPalangi et al. [276] presented an improved version of the reservoir\\nmodel by learning both the input and recurrent matrices which were\\nﬁxed in the previous model that makes use of the linear output (or\\nreadout) units to simplify the learning of only the output matrix in the\\nRNN. Rather, a special technique is devised that takes advantage of the\\nlinearity in the output units in the reservoir model to learn the input\\nand recurrent matrices. Compared with the backpropagation through\\ntime (BPTT) algorithm commonly used in learning the general RNNs,\\nthe proposed technique makes use of the linearity in the output units\\nto provide constraints among various matrices in the RNN, enabling\\nthe computation of the gradients as the learning signal in an analytical\\nform instead of by recursion as in the BPTT.\\nIn addition to the recent innovations in better architectures of deep\\nlearning models for speech recognition reviewed above, there is also a\\ngrowing body of work on developing and implementing better nonlinear\\nunits. Although sigmoidal and tanh functions are the most commonly\\nused nonlinear types in DNNs their limitations are well known. For\\nexample, it is slow to learn the whole network due to weak gradients\\nwhen the units are close to saturation in both directions. Jaitly and\\nHinton [183] appear to be the ﬁrst to apply the rectiﬁed linear units\\n(ReLU) in the DNNs to speech recognition to overcome the weakness\\nof the sigmoidal units. ReLU refers to the units in a neural network\\nthat use the activation function of f(x)=m a x ( 0,x ). Dahl et al. [65]', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='79683bc8-032b-4626-9ee6-a5c7cc588f04', embedding=None, metadata={'page_label': '281', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 281\\nand Mass et al. [234] successfully applied ReLU to large vocabulary\\nspeech recognition, with the best accuracy obtained when combining\\nReLU with the “Dropout” regularization technique.\\nAnother new type of DNN units demonstrated more recently to be\\nuseful for speech recognition is the “maxout” units, which were used\\nfor forming the deep maxout network as described in [244]. A deep\\nmaxout network consists of multiple layers which generate hidden acti-\\nvations via the maximum or “maxout” operation over a ﬁxed number\\nof weighted inputs called a “group. ” This is the same operation as\\nthe max pooling used in the CNN as discussed earlier for both speech\\nrecognition and computer vision. The maximal value within each group\\nis taken as the output from the previous layer. Most recently, Zhang\\net al. [441] generalize the above “maxout” units to two new types. The\\n“soft-maxout” type of units replace the original max operation with\\nthe soft-max function. The second, p-norm type of units used the non-\\nlinearity of y = ∥ x∥ p. It is shown experimentally that the p-norm units\\nwith p = 2 perform consistently better than the maxout, tanh, and\\nReLU units. In Gulcehre et al. [138], techniques that automatically\\nlearn the p-norm was proposed and investigated.\\nFinally, Srivastava et al. [350] propose yet another new type of non-\\nlinear units, called winner-take-all units. Here, local competition among\\nneighboring neurons are incorporated into the otherwise regular feed-\\nforward architecture, which is then trained via backpropagation with\\ndiﬀerent gradients than the normal one. Winner-take-all is an inter-\\nesting new form of nonlinearity, and it forms groups of (typically two)\\nneurons where all the neurons in a group are made zero-valued except\\nthe one with the largest value. Experiments show that the network does\\nnot forget as much as networks with standard sigmoidal nonlinearity.\\nThis new type of nonlinear units are yet to be evaluated in speech\\nrecognition tasks.\\n7.1.7 Better optimization and regularization\\nAnother area where signiﬁcant advances are made recently in\\napplying deep learning to acoustic model for speech recognition is\\non optimization criteria and methods, as well as on the related', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='78ac4c20-d558-49eb-9b56-66309985a5be', embedding=None, metadata={'page_label': '282', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='282 Selected Applications in Speech and Audio Processing\\nregularization techniques to help prevent overﬁtting during the deep\\nnetwork training.\\nOne of the early studies on DNNs for speech recognition, conducted\\nat Microsoft Research and reported in [260], ﬁrst recognizes the mis-\\nmatch between the desired error rate and the cross-entropy training\\ncriterion in the conventional DNN training. The solution is provided by\\nreplacing the frame-based, cross-entropy training criterion with the full-\\nsequence-based maximum mutual information optimization objective,\\nin a similar way to deﬁning the training objective for the shallow neu-\\nral network interfaced with an HMM [194]. Equivalently, this amounts\\nto putting the model of conditional random ﬁeld (CRF) at the top of\\nthe DNN, replacing the original softmax layer which naturally leads to\\ncross entropy. (Note the DNN was called the DBN in the paper). This\\nnew sequential discriminative learning technique is developed to jointly\\noptimize the DNN weights, CRF transition weights, and bi-phone lan-\\nguage model. Importantly, the speech task is deﬁned in TIMIT, with\\nthe use of a simple bi-phone-gram “language” model. The simplicity of\\nthe bi-gram language model enables the full-sequence training to carry\\nout without the need to use lattices, drastically reducing the training\\ncomplexity.\\nAs another way to motivate the full-sequence training method of\\n[260], we note that the earlier DNN phone recognition experiments\\nmade use of the standard frame-based objective function in static pat-\\ntern classiﬁcation, cross-entropy, to optimize the DNN weights. The\\ntransition parameters and language model scores were obtained from\\nan HMM and were trained independently of the DNN weights. However,\\nit has been known during the long history of the HMM research that\\nsequence classiﬁcation criteria can be very helpful in improving speech\\nand phone recognition accuracy. This is because the sequence classiﬁca-\\ntion criteria are more directly correlated with the performance measure\\n(e.g., the overall word or phone error rate) than frame-level criteria.\\nMore speciﬁcally, the use of frame-level cross entropy to train the DNN\\nfor phone sequence recognition does not explicitly take into account the\\nfact that the neighboring frames have smaller distances between the\\nassigned probability distributions over phone class labels. To overcome\\nthis deﬁciency, one can optimize the conditional probability of the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e078226d-8867-458b-8f10-a9f728eafb9c', embedding=None, metadata={'page_label': '283', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 283\\nwhole sequence of labels, given the whole visible feature utterance or\\nequivalently the hidden feature sequence extracted by DNN. To opti-\\nmize the log conditional probability on the training data, the gradi-\\nent can be taken over the activation parameters, transition parame-\\nters and lower-layer weights, and then pursue back-propagation of the\\nerror deﬁned at the sentence level. We remark that in a much earlier\\nstudy [212], combining a neural network with a CRF-like structure was\\ndone, where the mathematical formulation appears to include CRFs as\\na special case. Also, the beneﬁt of using the full-sequence classiﬁcation\\ncriteria was shown earlier on shallow neural networks in [194, 291].\\nIn implementing the above full-sequence learning algorithm for the\\nDNN system as described in [260], the DNN weights are initialized\\nusing the frame-level cross entropy as the objective. The transition\\nparameters are initialized from the combination of the HMM tran-\\nsition matrices and the “bi-phone language” model scores, and are\\nthen further optimized by tuning the transition features while ﬁxing\\nthe DNN weights before the joint optimization. Using joint optimiza-\\ntion with careful scheduling to reduce overﬁtting, it is shown that the\\nfull-sequence training outperforms the DNN trained with frame-level\\ncross entropy by approximately 5% relative [260]. Without the eﬀort\\nto reduce overﬁtting, it is found that the DNN trained with MMI is\\nmuch more prone to overﬁtting than that trained with frame-level cross\\nentropy. This is because the correlations across frames in speech tend\\nto be diﬀerent among the training, development, and test data. Impor-\\ntantly, such diﬀerences do not show when frame-based objective func-\\ntions are used for training.\\nFor large vocabulary speech recognition where more complex lan-\\nguage models are in use, the optimization methods for full-sequence\\ntraining of the DNN–HMM are much more sophisticated. Kingsbury\\net al. [195] reported the ﬁrst success of such training using parallel,\\nsecond-order, Hessian-free optimization techniques, which are carefully\\nimplemented for large vocabulary speech recognition. Sainath et al.\\n[305] improved and speeded up the Hessian-free techniques by reduc-\\ning the number of Krylov subspace solver iterations [378], which are\\nused for implicit estimation of the Hessian. They also use sampling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='87c9846c-3548-440d-8656-1ac07c9d9878', embedding=None, metadata={'page_label': '284', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='284 Selected Applications in Speech and Audio Processing\\nmethods to decrease the amount of training data to speed up the train-\\ning. While the batch-mode, second-order Hessian-free techniques prove\\nsuccessful for full-sequence training of large-scale DNN–HMM systems,\\nthe success of the ﬁrst-order stochastic gradient descent methods is also\\nreported recently [353]. It is found that heuristics are needed to handle\\nthe problem of lattice sparseness. That is, the DNN must be adjusted to\\nthe updated numerator lattices by additional iterations of frame-based\\ncross-entropy training. Further, artiﬁcial silence arcs need to be added\\nto the denominator lattices, or the maximum mutual information objec-\\ntive function needs to be smoothed with the frame-based cross entropy\\nobjective. The conclusion is that for large vocabulary speech recog-\\nnition tasks with sparse lattices, the implementation of the sequence\\ntraining requires much greater engineering skills than the small tasks\\nsuch as reported in [260], although the objective function as well as the\\ngradient derivation are essentially the same. Similar conclusions are\\nreached by Vesely et al. [374] when carrying out full-sequence training\\nof DNN–HMMs for large-vocabulary speech recognition. However, dif-\\nferent heuristics from [353] are shown to be eﬀective in the training.\\nSeparately, Wiesler et al. [390] investigated the Hessian-free optimiza-\\ntion method for training the DNN with the cross-entropy objective and\\nempirically analyzed the properties of the method. And ﬁnally, Dognin\\nand Goel [113] combined stochastic average gradient and Hessian-free\\noptimization for sequence training of deep neural networks with suc-\\ncess in that the training procedure converges in about half the time\\ncompared with the full Hessian-free sequence training.\\nFor large DNN–HMM systems with either frame-level or sequence-\\nlevel optimization objectives, speeding up the training is essential\\nto take advantage of large amounts of training data and of large\\nmodel sizes. In addition to the methods described above, Dean et al.\\n[69] reported the use of the asynchronous stochastic gradient descent\\n(ASGD) method, the adaptive gradient descent (Adagrad) method, and\\nthe large-scale limited-memory BFGS (L-BFGS) method for very large\\nvocabulary speech recognition. Sainath et al. [312] provided a review\\nof a wide range of optimization methods for speeding up the training\\nof DNN-based systems for large speech recognition tasks.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='52ad1f78-6691-481c-ac2a-5f324f7db3e7', embedding=None, metadata={'page_label': '285', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1. Acoustic modeling for speech recognition 285\\nIn addition to the advances described above focusing on optimiza-\\ntion with the fully supervised learning paradigm, where all train-\\ning data contain the label information, the semi-supervised training\\nparadigm is also exploited for learning DNN–HMM systems for speech\\nrecognition. Liao et al. [223] reported the exploration of using semi-\\nsupervised training on the DNN–HMM system for the very challenging\\ntask of recognizing YouTube speech. The main technique is based on the\\nuse of “island of conﬁdence” ﬁltering heuristics to select useful training\\nsegments. Separately, semi-supervised training of DNNs is explored by\\nVesely et al. [374], where self-training strategies are used as the basis\\nfor data selection using both the utterance-level and frame-level con-\\nﬁdences. Frame-selection based on per-frame conﬁdences derived from\\nconfusion in a lattice is found beneﬁcial. Huang et al. [176] reported\\nanother variant of semi-supervised training technique in which multi-\\nsystem combination and conﬁdence recalibration is applied to select\\nthe training data. Further, Thomas et al. [362] overcome the problem\\nof lacking suﬃcient training data for acoustic modeling in a number of\\nlow-resource scenarios. They make use of transcribed multilingual data\\nand semi-supervised training to build the proposed feature front-ends\\nfor subsequent speech recognition.\\nFinally, we see important progress in deep learning based speech\\nrecognition in recent years with the introduction of new regulariza-\\ntion methods based on “dropout” originally proposed by Hinton et al.\\n[166]. Overﬁtting is very common in DNN training and co-adaptation is\\nprevalent within the DNN with multiple activations adapting together\\nto explain input acoustic data. Dropout is a technique to limit co-\\nadaptation. It operates as follows. On each training instance, each hid-\\nden unit is randomly omitted with a ﬁxed probability (e.g., p =0 .5).\\nThen, decoding is done normally except with straightforward scaling\\nof the DNN weights (by a factor of 1 − p). Alternatively, the scaling of\\nthe DNN weights can be done during training [by a factor of 1 /(1 − p)]\\nrather than in decoding. The beneﬁts of dropout regularization for\\ntraining DNNs are to make a hidden unit in the DNN act strongly by\\nitself without relying on others, and to serve a way to do model averag-\\ning of diﬀerent networks. These beneﬁts are most pronounced when the\\ntraining data is limited, or when the DNN size is disproportionally large', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f0781148-459d-4faa-82f8-1af7e99a2fa2', embedding=None, metadata={'page_label': '286', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='286 Selected Applications in Speech and Audio Processing\\nwith respect to the size of the training data. Dahl et al. [65] applied\\ndropout in conjunction with the ReLU units and to only the top few\\nlayers of a fully-connected DNN. Seltzer and Yu [325] applied it to noise\\nrobust speech recognition. Deng et al. [81], on the other hand, applied\\ndropout to all layers of a deep convolutional neural network, including\\nboth the top fully connected DNN layers and the bottom locally con-\\nnected CNN layer and the pooling layer. It is found that the dropout\\nrate need to be substantially smaller for the convolutional layer.\\nSubsequent work on applying dropout includes the study by Miao\\nand Metze [243], where DNN-based speech recognition is constrained\\nby low resources with sparse training data. Most recently, Sainath et al.\\n[306] combined dropout with a number of novel techniques described\\nin this section (including the use of deep CNNs, Hessian-free sequence\\nlearning, the use of ReLU units, and the use of joint fMLLR and ﬁlter-\\nbank features, etc.) to obtain state of the art results on several large\\nvocabulary speech recognition tasks.\\nAs a summary, the initial success of deep learning methods for\\nspeech analysis and recognition reported around 2010 has come a long\\nway over the past three years. An explosive growth in the work and\\npublications on this topic has been observed, and huge excitement has\\nbeen ignited within the speech recognition community. We expect that\\nthe growth in the research on deep learning based speech recognition\\nwill continue, at least in the near future. It is also fair to say that the\\ncontinuing large-scale success of deep learning in speech recognition as\\nsurveyed in this chapter (up to the ASRU-2013 time frame) is a key\\nstimulant to the large-scale exploration and applications of the deep\\nlearning methods to other areas, which we will survey in Sections 8–11.\\n7.2 Speech synthesis\\nIn addition to speech recognition, the impact of deep learning has\\nrecently spread to speech synthesis, aimed to overcome the limitations\\nof the conventional approach in statistical parametric synthesis based\\non Gaussian-HMM and decision-tree-based model clustering. The goal\\nof speech synthesis is to generate speech sounds directly from text and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8c216e24-ecca-476c-b1b1-cf95ef6e0a79', embedding=None, metadata={'page_label': '287', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2. Speech synthesis 287\\npossibly with additional information. The ﬁrst set of papers appeared at\\nICASSP, May 2013, where four diﬀerent deep learning approaches are\\nreported to improve the traditional HMM-based statistical paramet-\\nric speech synthesis systems built based on “shallow” speech models,\\nwhich we brieﬂy review here after providing appropriate background\\ninformation.\\nStatistical parametric speech synthesis emerged in the mid-1990s,\\nand is currently the dominant technology in speech synthesis. See a\\nrecent overview in [364]. In this approach, the relationship between\\ntexts and their acoustic realizations are modeled using a set of\\nstochastic generative acoustic models. Decision tree-clustered context-\\ndependent HMMs with a Gaussian distribution as the output of an\\nHMM state are the most popular generative acoustic model used. In\\nsuch HMM-based speech synthesis systems, acoustic features including\\nthe spectra, excitation and segment durations of speech are modeled\\nsimultaneously within a uniﬁed context-dependent HMM framework.\\nAt the synthesis time, a text analysis module extracts a sequence of\\ncontextual factors including phonetic, prosodic, linguistic, and gram-\\nmatical descriptions from an input text to be synthesized. Given\\nthe sequence of contextual factors, a sentence-level context-dependent\\nHMM corresponding to the input text is composed, where its model\\nparameters are determined by traversing the decision trees. The acous-\\ntic features are predicted so as to maximize their output probabili-\\nties from the sentence HMM under the constraints between static and\\ndynamic features. Finally, the predicted acoustic features are sent to a\\nwaveform synthesis module to reconstruct the speech waveforms. It\\nhas been known for many years that the speech sounds generated\\nby this standard approach are often muﬄed compared with natural\\nspeech. The inadequacy of acoustic modeling based on the shallow-\\nstructured HMM is conjectured to be one of the reasons. Several very\\nrecent studies have adopted deep learning approaches to overcome such\\ndeﬁciency. One signiﬁcant advantage of deep learning techniques is\\ntheir strong ability to represent the intrinsic correlation or mapping\\nrelationship among the units of a high-dimensional stochastic vector\\nusing a generative (e.g., the RBM and DBN discussed in Section 3.2)\\nor discriminative (e.g., the DNN discussed in Section 3.3) modeling', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e53eb716-a0d0-4204-9993-82902f919920', embedding=None, metadata={'page_label': '288', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='288 Selected Applications in Speech and Audio Processing\\nframework. The deep learning techniques are thus expected to help the\\nacoustic modeling aspect of speech synthesis in overcoming the limita-\\ntions of the conventional shallow modeling approach.\\nA series of studies are carried out recently on ways of overcoming\\nthe above limitations using deep learning methods, inspired partly by\\nthe intrinsically hierarchical processes in human speech production\\nand the successful applications of a number of deep learning methods\\nin speech recognition as reviewed earlier in this chapter. In Ling\\net al. [227, 229], the RBM and DBN as generative models are used\\nto replace the traditional Gaussian models, achieving signiﬁcant\\nquality improvement, in both subjective and objective measures,\\nof the synthesized voice. In the approach developed in [190], the\\nDBN as a generative model is used to represent joint distribution of\\nlinguistic and acoustic features. Both the decision trees and Gaussian\\nmodels are replaced by the DBN. The method is very similar to that\\nused for generating digit images by the DBN, where the issue of\\ntemporal sequence modeling speciﬁc to speech (non-issue for image)\\nis by-passed via the use of the relatively large, syllable-sized units in\\nspeech synthesis. On the other hand, in contrast to the generative\\ndeep models (RBMs and DBNs) exploited above, the study reported\\nin [435] makes use of the discriminative model of the DNN to represent\\nthe conditional distribution of the acoustic features given the linguistic\\nfeatures. Finally, in [115], the discriminative model of the DNN is used\\nas a feature extractor that summarizes high-level structure from the\\nraw acoustic features. Such DNN features are then used as the input\\nfor the second stage for the prediction of prosodic contour targets\\nfrom contextual features in the full speech synthesis system.\\nThe application of deep learning to speech synthesis is in its infancy,\\nand much more work is expected from that community in the near\\nfuture.\\n7.3 Audio and music processing\\nSimilar to speech recognition but to a less extent, in the area of audio\\nand music processing, deep learning has also become of intense interest', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='95fffdf0-f195-4973-a888-9b2ca72a9729', embedding=None, metadata={'page_label': '289', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3. Audio and music processing 289\\nbut only quite recently. As an example, the ﬁrst major event of deep\\nlearning for speech recognition took place in 2009, followed by a series of\\nevents including a comprehensive tutorial on the topic at ICASSP-2012\\nand with the special issue at IEEE Transactions on Audio, Speech, and\\nLanguage Processing, the premier publication for speech recognition,\\nin the same year. The ﬁrst major event of deep learning for audio and\\nmusic processing appears to be the special session at ICASSP-2014,\\ntitled Deep Learning for Music [14].\\nIn the general ﬁeld of audio and music processing, the impacted\\nareas by deep learning include mainly music signal processing and music\\ninformation retrieval [15, 22, 141, 177, 178, 179, 319]. Deep learning\\npresents a unique set of challenges in these areas. Music audio signals\\nare time series where events are organized in musical time, rather than\\nin real time, which changes as a function of rhythm and expression. The\\nmeasured signals typically combine multiple voices that are synchro-\\nnized in time and overlapping in frequency, mixing both short-term and\\nlong-term temporal dependencies. The inﬂuencing factors include musi-\\ncal tradition, style, composer and interpretation. The high complexity\\nand variety give rise to the signal representation problems well-suited\\nto the high levels of abstraction aﬀorded by the perceptually and bio-\\nlogically motivated processing techniques of deep learning.\\nIn the early work on audio signals as reported by Lee et al. [215]\\nand their follow-up work, the convolutional structure is imposed on\\nthe RBM while building up a DBN. Convolution is made in time by\\nsharing weights between hidden units in an attempt to detect the same\\n“invariant” feature over diﬀerent times. Then a max-pooling operation\\nis performed where the maximal activations over small temporal neigh-\\nborhoods of hidden units are obtained, inducing some local temporal\\ninvariance. The resulting convolutional DBN is applied to audio as well\\nas speech data for a number of tasks including music artist and genre\\nclassiﬁcation, speaker identiﬁcation, speaker gender classiﬁcation, and\\nphone classiﬁcation, with promising results presented.\\nThe RNN has also been recently applied to music processing appli-\\ncations [22, 40, 41], where the use of ReLU hidden units instead of\\nlogistic or tanh nonlinearities are explored in the RNN. As reviewed in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0da34e21-92fe-4755-b743-bde204cbe5a9', embedding=None, metadata={'page_label': '290', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='290 Selected Applications in Speech and Audio Processing\\nSection 7.2, ReLU units compute y =m a x (x, 0), and lead to sparser\\ngradients, less diﬀusion of credit and blame in the RNN, and faster\\ntraining. The RNN is applied to the task of automatic recognition of\\nchords from audio music, an active area of research in music information\\nretrieval. The motivation of using the RNN architecture is its power\\nin modeling dynamical systems. The RNN incorporates an internal\\nmemory, or hidden state, represented by a self-connected hidden layer\\nof neurons. This property makes them well suited to model temporal\\nsequences, such as frames in a magnitude spectrogram or chord labels\\nin a harmonic progression. When well trained, the RNN is endowed\\nwith the power to predict the output at the next time step given the\\nprevious ones. Experimental results show that the RNN-based auto-\\nmatic chord recognition system is competitive with existing state-of-\\nthe-art approaches [275]. The RNN is capable of learning basic musical\\nproperties such as temporal continuity, harmony and temporal dynam-\\nics. It can also eﬃciently search for the most musically plausible chord\\nsequences when the audio signal is ambiguous, noisy or weakly discrim-\\ninative.\\nA recent review article by Humphrey et al. [179] provides a detailed\\nanalysis on content-based music informatics, and in particular on why\\nthe progress is decelerating throughout the ﬁeld. The analysis con-\\ncludes that hand-crafted feature design is sub-optimal and unsustain-\\nable, that the power of shallow architectures is fundamentally limited,\\nand that short-time analysis cannot encode musically meaningful struc-\\nture. These conclusions motivate the use of deep learning methods\\naimed at automatic feature learning. By embracing feature learning, it\\nbecomes possible to optimize a music retrieval system’s internal feature\\nrepresentation or discovering it directly, since deep architectures are\\nespecially well-suited to characterize the hierarchical nature of music.\\nFinally, we review the very recent work by van den Oord, et al. [371]\\non content-based music recommendation using deep learning methods.\\nAutomatic music recommendation has become an increasingly signiﬁ-\\ncant and useful technique in practice. Most recommender systems rely\\non collaborative ﬁltering, suﬀering from the cold start problem where\\nit fails when no usage data is available. Thus, collaborative ﬁltering is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d75345d5-e2f8-4e44-b9bb-944de41db3d2', embedding=None, metadata={'page_label': '291', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3. Audio and music processing 291\\nnot eﬀective for recommending new and unpopular songs. Deep learning\\nmethods power the latent factor model for recommendation, which pre-\\ndicts the latent factors from music audio when they cannot be obtained\\nfrom usage data. A traditional approach using a bag-of-words represen-\\ntation of the audio signals is compared with deep CNNs with rigorous\\nevaluation made. The results show highly sensible recommendations\\nproduced by the predicted latent factors using deep CNNs. The study\\ndemonstrates that a combination of convolutional neural networks and\\nricher audio features lead to such promising results for content-based\\nmusic recommendation.\\nLike speech recognition and speech synthesis, much more work is\\nexpected from the music and audio signal processing community in the\\nnear future.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f38366ec-912d-425c-872c-215c0595818d', embedding=None, metadata={'page_label': '292', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8\\nSelected Applications in Language\\nModeling and Natural Language Processing\\nResearch in language, document, and text processing has seen\\nincreasing popularity recently in the signal processing community,\\nand has been designated as one of the main focus areas by the IEEE\\nSignal Processing Society’s Speech and Language Processing Technical\\nCommittee. Applications of deep learning to this area started with\\nlanguage modeling (LM), where the goal is to provide a probability\\nto any arbitrary sequence of words or other linguistic symbols (e.g.,\\nletters, characters, phones, etc.). Natural language processing (NLP)\\nor computational linguistics also deals with sequences of words or\\nother linguistic symbols, but the tasks are much more diverse (e.g.,\\ntranslation, parsing, text classiﬁcation, etc.), not focusing on providing\\nprobabilities for linguistic symbols. The connection is that LM is\\noften an important and very useful component of NLP systems.\\nApplications to NLP is currently one of the most active areas in\\ndeep learning research, and deep learning is also considered as one\\npromising direction by the NLP research community. However, the\\nintersection between the deep learning and NLP researchers is so far\\nnot nearly as large as that for the application areas of speech or vision.\\nThis is partly because the hard evidence for the superiority of deep\\n292', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='54e6ade8-6392-4d83-8db8-4eeeab499174', embedding=None, metadata={'page_label': '293', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1. Language modeling 293\\nlearning over the current state of the art NLP methods has not been\\nas strong as speech or visual object recognition.\\n8.1 Language modeling\\nLanguage models (LMs) are crucial part of many successful applica-\\ntions, such as speech recognition, text information retrieval, statistical\\nmachine translation and other tasks of NLP. Traditional techniques for\\nestimating the parameters in LMs are based on N-gram counts. Despite\\nknown weaknesses ofN-grams and huge eﬀorts of research communities\\nacross many ﬁelds, N-grams remained the state-of-the-art until neural\\nnetwork and deep learning based methods were shown to signiﬁcantly\\nlower the perplexity of LMs, one common (but not ultimate) measure of\\nthe LM quality, over several standard benchmark tasks [245, 247, 248].\\nBefore we discuss neural network based LMs, we note the use of\\nhierarchical Bayesian priors in building up deep and recursive struc-\\nture for LMs [174]. Speciﬁcally, Pitman-Yor process is exploited as the\\nBayesian prior, from which a deep (four layers) probabilistic genera-\\ntive model is built. It oﬀers a principled approach to LM smoothing\\nby incorporating the power-law distribution for natural language. As\\ndiscussed in Section 3, this type of prior knowledge embedding is more\\nreadily achievable in the generative probabilistic modeling setup than\\nin the discriminative neural network based setup. The reported results\\non LM perplexity reduction are not nearly as strong as that achieved\\nby the neural network based LMs, which we discuss next.\\nThere has been a long history [19, 26, 27, 433] of using (shallow)\\nfeed-forward neural networks in LMs, called the NNLM. The use of\\nDNNs in the same way for LMs appeared more recently in [8]. An LM\\nis a function that captures the salient statistical characteristics of the\\ndistribution of sequences of words in natural language. It allows one to\\nmake probabilistic predictions of the next word given preceding ones.\\nAn NNLM is one that exploits the neural network’s ability to learn\\ndistributed representations in order to reduce the impact of the curse\\nof dimensionality. The original NNLM, with a feed-forward neural net-\\nwork structure works as follows: the input of the N-gram NNLM is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8cfe4930-e1da-45a3-9333-399bb23127d2', embedding=None, metadata={'page_label': '294', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='294 Language Modeling and Natural Language Processing\\nformed by using a ﬁxed length history of N − 1 words. Each of the\\nprevious N − 1 words is encoded using the very sparse 1-of-V coding,\\nwhere V is the size of the vocabulary. Then, this 1-of-V orthogonal rep-\\nresentation of words is projected linearly to a lower dimensional space,\\nusing the projection matrix shared among words at diﬀerent positions\\nin the history. This type of continuous-space, distributed representation\\nof words is called “word embedding,” very diﬀerent from the common\\nsymbolic or localist presentation [26, 27]. After the projection layer,\\na hidden layer with nonlinear activation function, which is either a\\nhyperbolic tangent or a logistic sigmoid, is used. An output layer of\\nthe neural network then follows the hidden layer, with the number of\\noutput units equal to the size of the full vocabulary. After the network\\nis trained, the output layer activations represent the “ N-gram” LM’s\\nprobability distribution.\\nThe main advantage of NNLMs over the traditional counting-based\\nN-gram LMs is that history is no longer seen as exact sequence ofN −1\\nwords, but rather as a projection of the entire history into some lower\\ndimensional space. This leads to a reduction of the total number of\\nparameters in the model that have to be trained, resulting in automatic\\nclustering of similar histories. Compared with the class-basedN-gram\\nLMs, the NNLMs are diﬀerent in that they project all words into the\\nsame low dimensional space, in which there can be many degrees of\\nsimilarity between words. On the other hand, NNLMs have much larger\\ncomputational complexity than N-gram LMs.\\nLet’s look at the strengths of the NNLMs again from the view-\\npoint of distributed representations. A distributed representation of a\\nsymbol is a vector of features which characterize the meaning of the\\nsymbol. Each element in the vector participates in representing the\\nmeaning. With an NNLM, one relies on the learning algorithm to dis-\\ncover meaningful, continuous-valued features. The basic idea is to learn\\nto associate each word in the dictionary with a continuous-valued vec-\\ntor representation, which in the literature is called a word embedding,\\nwhere each word corresponds to a point in a feature space. One can\\nimagine that each dimension of that space corresponds to a semantic\\nor grammatical characteristic of words. The hope is that functionally', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0961b6dd-67da-4428-b203-bbc5e8558aba', embedding=None, metadata={'page_label': '295', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1. Language modeling 295\\nsimilar words get to be closer to each other in that space, at least along\\nsome directions. A sequence of words can thus be transformed into a\\nsequence of these learned feature vectors. The neural network learns to\\nmap that sequence of feature vectors to the probability distribution over\\nthe next word in the sequence. The distributed representation approach\\nto LMs has the advantage that it allows the model to generalize well to\\nsequences that are not in the set of training word sequences, but that\\nare similar in terms of their features, i.e., their distributed represen-\\ntation. Because neural networks tend to map nearby inputs to nearby\\noutputs, the predictions corresponding to word sequences with similar\\nfeatures are mapped to similar predictions.\\nThe above ideas of NNLMs have been implemented in various\\nstudies, some involving deep architectures. The idea of structuring\\nhierarchically the output of an NNLM in order to handle large\\nvocabularies was introduced in [18, 262]. In [252], the temporally\\nfactored RBM was used for language modeling. Unlike the traditional\\nN-gram model, the factored RBM uses distributed representations\\nnot only for context words but also for the words being predicted.\\nThis approach is generalized to deeper structures as reported in [253].\\nSubsequent work on NNLM with “deep” architectures can be found in\\n[205, 207, 208, 245, 247, 248]. As an example, Le et al. [207] describes\\nan NNLM with structured output layer (SOUL–NNLM) where the pro-\\ncessing depth in the LM is focused in the neural network’s output rep-\\nresentation. Figure 8.1 illustrates the SOUL-NNLM architecture with\\nhierarchical structure in the output layers of the neural network, which\\nshares the same architecture with the conventional NNLM up to the\\nhidden layer. The hierarchical structure for the network’s output vocab-\\nulary is in the form of a clustering tree, shown to the right of Figure 8.1,\\nwhere each word belongs to only one class and ends in a single leaf node\\nof the tree. As a result of the hierarchical structure, the SOUL–NNLM\\nenables the training of the NNLM with a full, very large vocabulary.\\nThis gives advantages over the traditional NNLM which requires short-\\nlists of words in order to carry out the eﬃcient computation in training.\\nAs another example neural-network-based LMs, the work described\\nin [247, 248] and [245] makes use of RNNs to build large scale language', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0a45857d-0dac-4a0c-a82e-0548e091a252', embedding=None, metadata={'page_label': '296', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='296 Language Modeling and Natural Language Processing\\nFigure 8.1: The SOUL–NNLM architecture with hierarchical structure in the out-\\nput layers of the neural network [after [207], @IEEE].\\nmodels, called RNNLMs. The main diﬀerence between the feed-forward\\nand the recurrent architecture for LMs is diﬀerent ways of representing\\nthe word history. For feed-forward NNLM, the history is still just pre-\\nvious several words. But for the RNNLM, an eﬀective representation\\nof history is learned from the data during training. The hidden layer of\\nRNN represents all previous history and not justN −1 previous words,\\nthus the model can theoretically represent long context patterns. A fur-\\nther important advantage of the RNNLM over the feed-forward coun-\\nterpart is the possibility to represent more advanced patterns in the\\nword sequence. For example, patterns that rely on words that could\\nhave occurred at variable positions in the history can be encoded much\\nmore eﬃciently with the recurrent architecture. That is, the RNNLM\\ncan simply remember some speciﬁc word in the state of the hidden\\nlayer, while the feed-forward NNLM would need to use parameters for\\neach speciﬁc position of the word in the history.\\nThe RNNLM is trained using the algorithm of back-propagation\\nthrough time; see details in [245], which provided Figure 8.2 to show\\nduring training how the RNN unfolds as a deep feed-forward network\\n(with three time steps back in time).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='739652ce-cb56-43f8-a04d-3dccfac881d1', embedding=None, metadata={'page_label': '297', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.1. Language modeling 297\\nFigure 8.2: During the training of RNNLMs, the RNN unfolds into a deep feed-\\nforward network; based on Figure 3.2 of [245].\\nThe training of the RNNLM achieves stability and fast convergence,\\nhelped by capping the growing gradient in training RNNs. Adaptation\\nschemes for the RNNLM are also developed by sorting the training\\ndata with respect to their relevance and by training the model during\\nprocessing of the test data. Empirical comparisons with other state-of-\\nthe-art counting-based N-gram LMs show much better performance of\\nRNNLM in the perplexity measure, as reported in [247, 248] and [245].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a9bd90ed-480c-46c3-9152-d7e3c5a359d1', embedding=None, metadata={'page_label': '298', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='298 Language Modeling and Natural Language Processing\\nA separate work on applying RNN to an LM with the unit\\nof characters instead of words can be found in [153, 357]. Many\\ninteresting properties such as predicting long-term dependencies (e.g.,\\nmaking open and closing quotes in a paragraph) are demonstrated.\\nHowever, the usefulness of characters instead of words as units in\\npractical applications is not clear because the word is such a powerful\\nrepresentation for natural language. Changing words to characters in\\nLMs may limit most practical application scenarios and the training\\nbecome more diﬃcult. Word-level models currently remain superior.\\nIn the most recent work, Mnih and Teh [255] and Mnih and\\nKavukcuoglu [254] have developed a fast and simple training algorithm\\nfor NNLMs. Despite their superior performance, NNLMs have been\\nused less widely than standard N-gram LMs due to the much longer\\ntraining time. The reported algorithm makes use of a method called\\nnoise-contrastive estimation or NCE [139] to achieve much faster train-\\ning for NNLMs, with time complexity independent of the vocabulary\\nsize; hence a ﬂat instead of tree-structured output layer in the NNLM\\nis used. The idea behind NCE is to perform nonlinear logistic regres-\\nsion to discriminate between the observed data and some artiﬁcially\\ngenerated noise. That is, to estimate parameters in a density model of\\nobserved data, we can learn to discriminate between samples from the\\ndata distribution and samples from a known noise distribution. As an\\nimportant special case, NCE is particularly attractive for unnormalized\\ndistributions (i.e., free from partition functions in the denominator). In\\norder to apply NCE to train NNLMs eﬃciently, Mnih and Teh [255]\\nand Mnih and Kavukcuoglu [254] ﬁrst formulate the learning problem\\nas one which takes the objective function as the distribution of the word\\nin terms of a scoring function. The NNLM then can be viewed as a way\\nto quantify the compatibility between the word history and a candidate\\nnext word using the scoring function. The objective function for train-\\ning the NNLM thus becomes exponentiation of the scoring function,\\nnormalized by the same constant over all possible words. Removing\\nthe costly normalization factor, NCE is shown to speed up the NNLM\\ntraining over an order of magnitude.\\nA similar concept to NCE is used in the recent work of [250], which\\nis called negative sampling. This is applied to a simpliﬁed version of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c13dc03f-575e-4873-bfee-5b60d0bfa894', embedding=None, metadata={'page_label': '299', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2. Natural language processing 299\\nan NNLM, for the purpose of constructing word embedding instead\\nof computing probabilities of word sequences. Word embedding is an\\nimportant concept for NLP applications, which we discuss next.\\n8.2 Natural language processing\\nMachine learning has been a dominant tool in NLP for many years.\\nHowever, the use of machine learning in NLP has been mostly limited to\\nnumerical optimization of weights for human designed representations\\nand features from the text data. The goal of deep or representation\\nlearning is to automatically develop features or representations from\\nthe raw text material appropriate for a wide range of NLP tasks.\\nRecently, neural network based deep learning methods have\\nbeen shown to perform well on various NLP tasks such as language\\nmodeling, machine translation, part-of-speech tagging, named entity\\nrecognition, sentiment analysis, and paraphrase detection. The most\\nattractive aspect of deep learning methods is their ability to perform\\nthese tasks without external hand-designed resources or time-intensive\\nfeature engineering. To this end, deep learning develops and makes use\\nan important concept called “embedding,” which refers to the represen-\\ntation of symbolic information in natural language text at word-level,\\nphrase-level, and even sentence-level in terms of continuous-valued\\nvectors.\\nThe early work highlighting the importance of word embedding\\ncame from [62], [367], and [63], although the original form came from\\n[26] as a side product of language modeling. Raw symbolic word rep-\\nresentations are transformed from the sparse vectors via 1-of-V coding\\nwith a very high dimension (i.e., the vocabulary size V or its square or\\neven its cubic) into low-dimensional, real-valued vectors via a neural\\nnetwork and then used for processing by subsequent neural network lay-\\ners. The key advantage of using the continuous space to represent words\\n(or phrases) is its distributed nature, which enables sharing or grouping\\nthe representations of words with a similar meaning. Such sharing is\\nnot possible in the original symbolic space, constructed by 1-of-V cod-\\ning with a very high dimension, for representing words. Unsupervised', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cd13ba8d-da96-482e-8c2e-7577f990d7dc', embedding=None, metadata={'page_label': '300', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='300 Language Modeling and Natural Language Processing\\nlearning is used where “context” of the word is used as the learning\\nsignal in neural networks. Excellent tutorials were recently given by\\nSocher et al. [338, 340] to explain how the neural network is trained\\nto perform word embedding. More recent work proposes new ways of\\nlearning word embeddings that better capture the semantics of words\\nby incorporating both local and global document contexts and better\\naccount for homonymy and polysemy by learning multiple embeddings\\nper word [169]. Also, there is strong evidence that the use of RNNs can\\nalso provide empirically good performance in learning word embeddings\\n[245]. While the use of NNLMs, whose aim is to predict the future words\\nin context, also induces word embeddings as its by-product, much sim-\\npler ways of achieving the embeddings are possible without the need to\\ndo word prediction. As shown by Collobert and Weston [62], the neural\\nnetworks used for creating word embeddings need much smaller output\\nunits than the huge size typically required for NNLMs.\\nIn the same early paper on word embedding, Collobert and Weston\\n[62] developed and employed a convolutional network as the common\\nmodel to simultaneously solve a number of classic problems includ-\\ning part-of-speech tagging, chunking, named entity tagging, semantic\\nrole identiﬁcation, and similar word identiﬁcation. More recent work\\nreported in [61] further developed a fast, purely discriminative approach\\nfor parsing based on the deep recurrent convolutional architecture. Col-\\nlobert et al. [63] provide a comprehensive review on ways of applying\\nuniﬁed neural network architectures and related deep learning algo-\\nrithms to solve NLP problems from “scratch,” meaning that no tradi-\\ntional NLP methods are used to extract features. The theme of this\\nline of work is to avoid task-speciﬁc, “man-made” feature engineering\\nwhile providing versatility and uniﬁed features constructed automati-\\ncally from deep learning applicable to all natural language processing\\ntasks. The systems described in [63] automatically learn internal repre-\\nsentations or word embedding from vast amounts of mostly unlabeled\\ntraining data while performing a wide range of NLP tasks.\\nThe recent work by Mikolov et al. [246] derives word embeddings\\nby simplifying the NNLM described in Section 8.1. It is found that\\nthe NNLM can be successfully trained in two steps. First, continuous\\nword vectors are learned using a simple model which eliminates the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dbda6279-00d6-4f6b-bd7f-51868d3d9972', embedding=None, metadata={'page_label': '301', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2. Natural language processing 301\\nFigure 8.3:The CBOW architecture (a) on the left, and the Skip-gram architecture\\n(b) on the right. [after [246], @ICLR].\\nnonlinearity in the upper neural network layer and share the projec-\\ntion layer for all words. And second, the N-gram NNLM is trained\\non top of the word vectors. So, after removing the second step in the\\nNNLM, the simple model is used to learn word embeddings, where the\\nsimplicity allows the use of very large amount of data. This gives rise\\nto a word embedding model called Continuous Bag-of-Words Model\\n(CBOW), as shown in Figure 8.3a. Further, since the goal is no longer\\ncomputing probabilities of word sequences as in LMs, the word embed-\\nding system here is made more eﬀective by not only to predict the\\ncurrent word based on the context but also to perform inverse pre-\\ndiction known as “Skip-gram” model, as shown in Figure 8.3b. In\\nthe follow-up work [250] by the same authors, this word embedding\\nsystem including the Skip-gram model is extended by a much faster\\nlearning method called negative sampling, similar to NCE discussed in\\nSection 8.1.\\nIn parallel with the above development, Mnih and Kavukcuoglu\\n[254] demonstrate that NCE training of lightweight word embedding', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='97bf3bbb-81b3-40ba-a003-9e520949ef25', embedding=None, metadata={'page_label': '302', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='302 Language Modeling and Natural Language Processing\\nmodels is a highly eﬃcient way of learning high-quality word represen-\\ntations, much like the somewhat earlier lightweight LMs developed by\\nMnih and Teh [255] described in Section 8.1. Consequently, results that\\nused to require very considerable hardware and software infrastructure\\ncan now be obtained on a single desktop with minimal programming\\neﬀort and using less time and data. This most recent work also shows\\nthat for representation learning, only ﬁve noise samples in NCE can be\\nsuﬃcient for obtaining strong results for word embedding, much fewer\\nthan that required for LMs. The authors also used an “inversed lan-\\nguage model” for computing word embeddings, similar to the way in\\nwhich the Skip-gram model is used in [250].\\nHuang et al. [169] recognized the limitation of the earlier work on\\nword embeddings in that these models were built with only local con-\\ntext and one representation per word. They extended the local context\\nmodels to one that can incorporate global context from full sentences\\nor the entire document. This extended models accounts for homonymy\\nand polysemy by learning multiple embeddings for each word. An illus-\\ntration of this model is shown in Figure 8.4. In the earlier work by the\\nsame research group [344], a recursive neural network with local con-\\ntext was developed to build a deep architecture. The network, despite\\nmissing global context, was already shown to be capable of successful\\nFigure 8.4:The extended word-embedding model using a recursive neural network\\nthat takes into account not only local context but also global context. The global\\ncontext is extracted from the document and put in the form of a global semantic\\nvector, as part of the input into the original word-embedding model with local\\ncontext. Taken from Figure 1 of [169]. [after [169], @ACL].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9fa5bb5a-6dfe-4828-9e72-53c4d5ea3d96', embedding=None, metadata={'page_label': '303', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2. Natural language processing 303\\nmerging of natural language words based on the learned semantic trans-\\nformations of their original features. This deep learning approach pro-\\nvided an excellent performance on natural language parsing. The same\\napproach was also demonstrated to be reasonably successful in pars-\\ning natural scene images. In related studies, a similar recursive deep\\narchitecture is used for paraphrase detection [346], and for predicting\\nsentiment distributions from text [345].\\nWe now turn to selected applications of deep learning methods\\nincluding the use of neural network architectures and word embed-\\ndings to practically useful NLP tasks. Machine translation is one of\\nsuch tasks, pursued by NLP researchers for many years based typically\\non shallow statistical models. The work described in [320] are perhaps\\nthe ﬁrst comprehensive report on the successful application of neural-\\nnetwork-based language models with word embeddings, trained on a\\nGPU, for large machine translation tasks. They address the problem of\\nhigh computation complexity, and provide a solution that allows train-\\ning 500 million words with 20 hours. Strong results are reported, with\\nperplexity down from 71 to 60 in LMs and the corresponding BLEU\\nscore gained by 1.8 points using the neural-network-based language\\nmodels with word embeddings compared with the best back-oﬀ LM.\\nA more recent study on applying deep learning methods to machine\\ntranslation appears in [121, 123], where the phrase-translation compo-\\nnent, rather than the LM component in the machine translation system\\nis replaced by the neural network models with semantic word embed-\\ndings. As shown in Figure 8.5 for the architecture of this approach,\\na pair of source (denoted by f) and target (denoted by e) phrases\\nare projected into continuous-valued vector representations in a low-\\ndimensional latent semantic space (denoted by the twoy vectors).Then\\ntheir translation score is computed by the distance between the pair in\\nthis new space. The projection is performed by two deep neural net-\\nworks (not shown here) whose weights are learned on parallel training\\ndata. The learning is aimed to directly optimize the quality of end-\\nto-end machine translation results. Experimental evaluation has been\\nperformed on two standard Europarl translation tasks used by the NLP\\ncommunity, English–French and German–English. The results show\\nthat the new semantic-based phrase translation model signiﬁcantly', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8d6d9128-f4c0-44f5-abf8-5e168627c165', embedding=None, metadata={'page_label': '304', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='304 Language Modeling and Natural Language Processing\\nFigure 8.5: Illustration of the basic approach reported in [122] for machine trans-\\nlation. Parallel pairs of source (denoted by f) and target (denoted by e) phrases\\nare projected into continuous-valued vector representations (denoted by the two y\\nvectors), and their translation score is computed by the distance between the pair in\\nthis continuous space. The projection is performed by deep neural networks (denoted\\nby the two arrows) whose weights are learned on parallel training data. [after [121],\\n@NIPS].\\nimproves the performance of a state-of-the-art phrase-based statisti-\\ncal machine translation system, leading to a gain close to 1.0 BLEU\\npoint.\\nA related approach to machine translation was developed by\\nSchwenk [320]. The estimation of the translation model probabilities of\\na phrase-based machine translation system is carried out using neural\\nnetworks. The translation probability of phrase pairs is learned using\\ncontinuous-space representations induced by neural networks. A sim-\\npliﬁcation is made that decomposes the translation probability of a\\nphrase or a sentence to a product of n-gram probabilities as in a stan-\\ndard n-gram language model. No joint representations of a phrase in\\nthe source language and the translated version in the target language\\nare exploited as in the approach reported by Gao et al. [122, 123].\\nYet another deep learning approach to machine translation\\nappeared in [249]. As in other approaches, a corpus of words in one\\nlanguage are compared with the same corpus of words translated into\\nanother, and words and phrases in such bilingual data that share similar\\nstatistical properties are considered equivalent. A new technique is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='786b2fcc-7744-42fd-ab9d-66d97f19c8f4', embedding=None, metadata={'page_label': '305', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2. Natural language processing 305\\nproposed that automatically generates dictionaries and phrase tables\\nthat convert one language into another. It does not rely on versions of\\nthe same document in diﬀerent languages. Instead, it uses data mining\\ntechniques to model the structure of a source language and then com-\\npares it to the structure of the target language. The technique is shown\\nto translate missing word and phrase entries by learning language struc-\\ntures based on large monolingual data and mapping between languages\\nfrom small bilingual data. It is based on vector-valued word embed-\\ndings as discussed earlier in this chapter and it learns a linear mapping\\nbetween vector spaces of source and target languages.\\nAn earlier study on applying deep learning techniques with DBNs\\nwas provided in [111] to attack a machine transliteration problem,\\na much easier task than machine translation. This type of deep\\narchitectures and learning may be generalized to the more diﬃcult\\nmachine translation problem but no follow-up work has been reported.\\nAs another early NLP application, Sarikaya et al. [318] applied DNNs\\n(called DBNs in the paper) to perform a natural language call–routing\\ntask. The DNNs use unsupervised learning to discover multiple layers\\nof features that are then used to optimize discrimination. Unsupervised\\nfeature discovery is found to make DBNs far less prone to overﬁtting\\nthan the neural networks initialized with random weights. Unsuper-\\nvised learning also makes it easier to train neural networks with many\\nhidden layers. DBNs are found to produce better classiﬁcation results\\nthan several other widely used learning techniques, e.g., maximum\\nentropy and boosting based classiﬁers.\\nOne most interesting NLP task recently tackled by deep learn-\\ning methods is that of knowledge base (ontology) completion, which\\nis instrumental in question-answering and many other NLP applica-\\ntions. An early work in this space came from [37], where a process is\\nintroduced to automatically learn structured distributed embeddings\\nof knowledge bases. The proposed representations in the continuous-\\nvalued vector space are compact and can be eﬃciently learned from\\nlarge-scale data of entities and relations. A specialized neural network\\narchitecture, a generalization of “Siamese” network, is used. In the\\nfollow-up work that focuses on multi-relational data [36], the semantic\\nmatching energy model is proposed to learn vector representations for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='52b3c3f0-7baf-4693-ae5f-192f26204688', embedding=None, metadata={'page_label': '306', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='306 Language Modeling and Natural Language Processing\\nboth entities and relations. More recent work [340] adopts an alterna-\\ntive approach, based on the use of neural tensor networks, to attack\\nthe problem of reasoning over a large joint knowledge graph for rela-\\ntion classiﬁcation. The knowledge graph is represented as triples of a\\nrelation between two entities, and the authors aim to develop a neu-\\nral network model suitable for inference over such relationships. The\\nmodel they presented is a neural tensor network, with one layer only.\\nThe network is used to represent entities in a ﬁxed-dimensional vectors,\\nwhich are created separately by averaging pre-trained word embedding\\nvectors. It then learn the tensor with the newly added relationship ele-\\nment that describes the interactions among all the latent components\\nin each of the relationships. The neural tensor network can be visu-\\nalized in Figure 8.6, where each dashed box denotes one of the two\\nslices of the tensor. Experimentally, the paper [340] shows that this\\ntensor model can eﬀectively classify unseen relationships in WordNet\\nand FreeBase.\\nAs the ﬁnal example of deep learning applied successfully to NLP,\\nwe discuss here sentiment analysis applications based on recursive deep\\nFigure 8.6: Illustration of the neural tensor network described in [340], with two\\nrelationships shown as two slices in the tensor. The tensor is denoted by W [1:2].T h e\\nnetwork contains a bilinear tensor layer that directly relates the two entity vectors\\n(shown as e1 and e2) across three dimensions. Each dashed box denotes one of the\\ntwo slices of the tensor. [after [340], @NIPS].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fe31f6b6-d2b2-4824-9583-c724559789b1', embedding=None, metadata={'page_label': '307', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8.2. Natural language processing 307\\nmodels published recently by Socher et al. [347]. Sentiment analysis is\\na task that is aimed to estimate the positive or negative opinion by an\\nalgorithm based on input text information. As we discussed earlier in\\nthis chapter, word embeddings in the semantic space achieved by neural\\nnetwork models have been very useful but it is diﬃcult for them to\\nexpress the meaning of longer phrases in a principled way. For sentiment\\nanalysis with the input data from typically many words and phrases,\\nthe embedding model requires the compositionality properties. To this\\nend, Socher et al. [347] developed the recursive neural tensor network,\\nwhere each layer is constructed similarly to that of the neural tensor\\nnetwork described in [340] with an illustration shown in Figure 8.6.\\nThe recursive construction of the full network exhibiting properties\\nof compositionality follows that of [344] for the regular, non-tensor\\nnetwork. When trained on a carefully constructed sentiment analysis\\ndatabase, the recursive neural tensor network is shown to outperform all\\nprevious methods on several metrics. The new model pushes the state of\\nthe art in single sentence positive/negative classiﬁcation accuracy from\\n80% up to 85.4%. The accuracy of predicting ﬁne-grained sentiment\\nlabels for all phrases reaches 80.7%, an improvement of 9.7% over bag-\\nof-features baselines.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a9ef742d-1541-49ba-ad16-3a62f36d2bcb', embedding=None, metadata={'page_label': '308', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9\\nSelected Applications in Information Retrieval\\n9.1 A brief introduction to information retrieval\\nInformation retrieval (IR) is a process whereby a user enters a query\\ninto the automated computer system that contains a collection of many\\ndocuments with the goal of obtaining a set of most relevant documents.\\nQueries are formal statements of information needs, such as search\\nstrings in web search engines. In IR, a query does not uniquely identify\\na single document in the collection. Instead, several documents may\\nmatch the query with diﬀerent degrees of relevancy.\\nA document, sometimes called an object as a more general term\\nwhich may include not only a text document but also an image, audio\\n(music or speech), or video, is an entity that contains information\\nand represented as an entry in a database. In this section, we limit\\nthe “object” to only text documents. User queries in IR are matched\\nagainst the documents’ representation stored in the database. Docu-\\nments themselves often are not kept or stored directly in the IR sys-\\ntem. Rather, they are represented in the system by metadata. Typical\\nIR systems compute a numeric score on how well each document in\\nthe database matches the query, and rank the objects according to this\\nvalue. The top-ranking documents from the system are then shown to\\n308', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0d12d6e5-7236-4fcb-8e5a-fa4a82c4c3ee', embedding=None, metadata={'page_label': '309', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.2. Semantic hashing with deep autoencoders for document 309\\nthe user. The process may then be iterated if the user wishes to reﬁne\\nthe query.\\nBased partly on [236], common IR methods consist of several\\ncategories:\\n• Boolean retrieval, where a document either matches a query or\\ndoes not.\\n• Algebraic approaches to retrieval, where models are used to rep-\\nresent documents and queries as vectors, matrices, or tuples. The\\nsimilarity of the query vector and document vector is represented\\nas a scalar value. This value can be used to produce a list of doc-\\numents that are rank-ordered for a query. Common models and\\nmethods include vector space model, topic-based vector space\\nmodel, extended Boolean model, and latent semantic analysis.\\n• Probabilistic approaches to retrieval, where the process of IR\\nis treated as a probabilistic inference. Similarities are computed\\nas probabilities that a document is relevant for a given query,\\nand the probability value is then used as the score in ranking\\ndocuments. Common models and methods include binary\\nindependence model, probabilistic relevance model with the\\nBM25 relevance function, methods of inference with uncertainty,\\nprobabilistic, language modeling, http://en.wikipedia.org/wiki/\\nUncertain_inference and the technique of latent Dirichlet\\nallocation.\\n• Feature-based approaches to retrieval, where documents are\\nviewed as vectors of values of feature functions. Principled meth-\\nods of “learning to rank” are devised to combine these features\\ninto a single relevance score. Feature functions are arbitrary\\nfunctions of document and query, and as such Feature-based\\napproaches can easily incorporate almost any other retrieval\\nmodel as just yet another feature.\\nDeep learning applications to IR are rather recent. The approaches\\nin the literature so far belong mostly to the category of feature-based\\napproaches. The use of deep networks is mainly for extracting seman-\\ntically meaningful features for subsequent document ranking stages.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e294f737-0184-438b-9fdb-5216dc18bc74', embedding=None, metadata={'page_label': '310', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='310 Selected Applications in Information Retrieval\\nWe will review selected studies in the recent literature in the remain-\\nder of this section below.\\n9.2 Semantic hashing with deep autoencoders for document\\nindexing and retrieval\\nHere we discuss the “semantic hashing” approach for the application\\nof deep autoencoders to document indexing and retrieval as published\\nin [159, 314]. It is shown that the hidden variables in the ﬁnal layer of\\na DBN not only are easy to infer after using an approximation based\\non feed-forward propagation, but they also give a better representation\\nof each document, based on the word-count features, than the widely\\nused latent semantic analysis and the traditional TF-IDF approach\\nfor information retrieval. Using the compact code produced by deep\\nautoencoders, documents are mapped to memory addresses in such a\\nway that semantically similar text documents are located at nearby\\naddresses to facilitate rapid document retrieval. The mapping from a\\nword-count vector to its compact code is highly eﬃcient, requiring only\\na matrix multiplication and a subsequent sigmoid function evaluation\\nfor each hidden layer in the encoder part of the network.\\nA deep generative model of DBN is exploited for the above purpose\\nas discussed in [165]. Brieﬂy, the lowest layer of the DBN represents\\nthe word-count vector of a document and the top layer represents a\\nlearned binary code for that document. The top two layers of the DBN\\nform an undirected associative memory and the remaining layers form\\na Bayesian (also called belief) network with directed, top-down connec-\\ntions. This DBN, composed of a set of stacked RBMs as we reviewed\\nin Section 5, produces a feed-forward “encoder” network that converts\\nword-count vectors to compact codes. By composing the RBMs in the\\nopposite order, a “decoder” network is constructed that maps com-\\npact code vectors into reconstructed word-count vectors. Combining\\nthe encoder and decoder, one obtains a deep autoencoder (subject to\\nfurther ﬁne-tuning as discussed in Section 4) for document coding and\\nsubsequent retrieval.\\nAfter the deep model is trained, the retrieval process starts with\\nmapping each query into a 128-bit binary code by performing a forward', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='606227f0-572b-407a-beb1-27fac8e89a51', embedding=None, metadata={'page_label': '311', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3. DSSM for document retrieval 311\\npass through the model with thresholding. Then the Hamming dis-\\ntance between the query binary code and all the documents’ 128-bit\\nbinary codes, especially those of the “neighboring” documents deﬁned\\nin the semantic space, are computed extremely eﬃciently. The eﬃ-\\nciency is accomplished by looking up the neighboring bit vectors in\\nthe hash table. The same idea as discussed here for coding text docu-\\nments for information retrieval has been explored for audio document\\nretrieval and speech feature coding problems with some initial explo-\\nration reported in [100], discussed in Section 4 in detail.\\n9.3 Deep-structured semantic modeling (DSSM)\\nfor document retrieval\\nHere we discuss the more advanced and recent approach to large-scale\\ndocument retrieval (Web search) based on a specialized deep architec-\\nture, called deep-structured semantic model or deep semantic similarity\\nmodel (DSSM), as published in [172], and its convolutional version (C-\\nDSSM), as published in [328].\\nModern search engines retrieve Web documents mainly by match-\\ning keywords in documents with those in a search query. However, lex-\\nical matching can be inaccurate due to the fact that a concept is often\\nexpressed using diﬀerent vocabularies and language styles in documents\\nand queries. Latent semantic models are able to map a query to its rel-\\nevant documents at the semantic level where lexical-matching often\\nfails [236]. These models address the language discrepancy between\\nWeb documents and search queries by grouping diﬀerent terms that\\noccur in a similar context into the same semantic cluster. Thus, a query\\nand a document, represented as two vectors in the lower-dimensional\\nsemantic space, can still have a high similarity even if they do not share\\nany term. Probabilistic topic models such as probabilistic latent seman-\\ntic models and latent Dirichlet allocation models have been proposed\\nfor semantic matching to partially overcome such diﬃculties. However,\\nthe improvement on IR tasks has not been as signiﬁcant as originally\\nexpected because of two main factors: (1) most state-of-the-art latent\\nsemantic models are based on linear projection, and thus are inadequate\\nin capturing eﬀectively the complex semantic properties of documents;', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='28bcb1c0-b74c-4bc1-958f-2001f6928293', embedding=None, metadata={'page_label': '312', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='312 Selected Applications in Information Retrieval\\nand (2) these models are often trained in an unsupervised manner using\\nan objective function that is only loosely coupled with the evaluation\\nmetric for the retrieval task. In order to improve semantic matching for\\nIR, two lines of research have been conducted to extend the above latent\\nsemantic models. The ﬁrst is the semantic hashing approach reviewed\\nin Section 9.1 above in this section based on the use of deep autoen-\\ncoders [165, 314]. While the hierarchical semantic structure embedded\\nin the query and the document can be extracted via deep learning,\\nthe deep learning approach used for their models still adopts an unsu-\\npervised learning method where the model parameters are optimized\\nfor the re-construction of the documents rather than for diﬀerentiating\\nthe relevant documents from the irrelevant ones for a given query. As\\na result, the deep neural network models do not signiﬁcantly outper-\\nform strong baseline IR models that are based on lexical matching. In\\nthe second line of research, click-through data, which consists of a list\\nof queries and the corresponding clicked documents, is exploited for\\nsemantic modeling so as to bridge the language discrepancy between\\nsearch queries and Web documents in recent studies [120, 124]. These\\nmodels are trained on click-through data using objectives that tailor to\\nthe document ranking task. However, these click-through-based models\\nare still linear, suﬀering from the issue of expressiveness. As a result,\\nthese models need to be combined with the keyword matching models\\n(such as BM25) in order to obtain a signiﬁcantly better performance\\nthan baselines.\\nThe DSSM approach reported in [172] aims to combine the\\nstrengths of the above two lines of work while overcoming their weak-\\nnesses. It uses the DNN architecture to capture complex semantic prop-\\nerties of the query and the document, and to rank a set of documents\\nfor a given query. Brieﬂy, a nonlinear projection is performed ﬁrst to\\nmap the query and the documents to a common semantic space. Then,\\nthe relevance of each document given the query is calculated as the\\ncosine similarity between their vectors in that semantic space. The\\nDNNs are trained using the click-through data such that the condi-\\ntional likelihood of the clicked document given the query is maximized.\\nDiﬀerent from the previous latent semantic models that are learned\\nin an unsupervised fashion, the DSSM is optimized directly for Web', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3b6aa76a-34b8-4d8d-b918-8214d541687a', embedding=None, metadata={'page_label': '313', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3. DSSM for document retrieval 313\\nFigure 9.1:The DNN component of the DSSM architecture for computing semantic\\nfeatures. The DNN uses multiple layers to map high-dimensional sparse text features,\\nfor both Queries and Documents into low-dimensional dense features in a semantic\\nspace. [after [172], @CIKM].\\ndocument ranking, and thus gives superior performance. Furthermore,\\nto deal with large vocabularies in Web search applications, a new word\\nhashing method is developed, through which the high-dimensional term\\nvectors of queries or documents are projected to low-dimensional letter\\nbased n-gram vectors with little information loss.\\nFigure 9.1 illustrates the DNN part in the DSSM architecture. The\\nDNN is used to map high-dimensional sparse text features into low-\\ndimensional dense features in a semantic space. The ﬁrst hidden layer,\\nwith 30k units, accomplishes word hashing. The word-hashed features\\nare then projected through multiple layers of non-linear projections.\\nThe ﬁnal layer’s neural activities in this DNN form the feature in the\\nsemantic space.\\nTo show the computational steps in the various layers of the DNN\\nin Figure 9.1, we denote x as the input term vector, y as the output\\nvector, li, i =1 ,...,N − 1, as the intermediate hidden layers, Wi as\\nthe ith projection matrix, and bi as the ith bias vector, we have\\nl1 = W1x,\\nli = f(Wili−1 + bi),i > 1\\ny = f(WN lN−1 + bN ),', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2bfa3c0f-bf85-483b-999a-18a2051ea2f0', embedding=None, metadata={'page_label': '314', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='314 Selected Applications in Information Retrieval\\nwhere tanh function is used at the output layer and the hidden layers\\nli,i =2 ,...,N − 1:\\nf(x)= 1 − e−2x\\n1+ e−2x .\\nThe semantic relevance score between a query Q and a document D\\ncan then be computed as the consine distance\\nR(Q, D)=c o s i n e (yQ,y D)=\\nyT\\nQyD\\n∥ yQ∥∥ yD∥ ,\\nwhere yQ and yD are the concept vectors of the query and the docu-\\nment, respectively. In Web search, given the query, the documents can\\nbe sorted by their semantic relevance scores.\\nLearning of the DNN weights Wi and bi shown in Figure 9.1 is an\\nimportant contribution of the study of [172]. Compared with the DNNs\\nused in speech recognition where the targets or labels of the training\\ndata are readily available, the DNN in the DSSM does not have such\\nlabel information well deﬁned. That is, rather than using the common\\ncross entropy or mean square errors as the training objective function,\\nIR-centric loss functions need to be developed in order to train the DNN\\nweights in the DSSM using the available data such as click-through logs.\\nThe click-through logs consist of a list of queries and their clicked\\ndocuments. A query is typically more relevant to the documents that\\nare clicked on than those that are not. This weak supervision informa-\\ntion can be exploited to train the DSSM. More speciﬁcally, the weight\\nmatrices in the DSSM, Wi, is learned to maximize the posterior prob-\\nability of the clicked documents given the queries\\nP(D | Q)= exp(γR(Q, D))\\n∑\\nD′ ∈D exp(γR(Q, D′))\\ndeﬁned on the semantic relevance scoreR(Q, D) between the Query (Q)\\nand the Document (D), where γ is a smoothing factor set empirically\\non a held-out data set, and D denotes the set of candidate documents\\nto be ranked. Ideally, D should contain all possible documents, as in\\nthe maximum mutual information training for speech recognition where\\nall possible negative candidates may be considered [147]. However in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f9fd745b-1e90-46be-8fa1-be068e215601', embedding=None, metadata={'page_label': '315', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.3. DSSM for document retrieval 315\\nthis case D is of Web scale and thus is intractable in practice. In the\\nimplementation of DSSM learning described in [172], a subset of the\\nnegative candidates are used, following the common practice adopted\\nin MCE (Minimum Classiﬁcation Error) training in speech recognition\\n[52, 118, 417, 418]. In other words, for each query and clicked-document\\npair, denoted by (QD+)w h e r eQ is a query and D+ is the clicked doc-\\nument, the set of D is approximated by including D+ and only four\\nrandomly selected unclicked documents, denoted by D−\\nj ; j =1 ,..., 4}.\\nIn the study reported in [172], no signiﬁcant diﬀerence was found when\\ndiﬀerent sampling strategies were used to select the unclicked docu-\\nments.\\nWith the above simpliﬁcation the DSSM parameters are estimated\\nto maximize the approximate likelihood of the clicked documents given\\nthe queries across the training set\\nL(Λ) = log\\n∏\\n(Q,D+,D−\\nj )\\nP(D+ | Q),\\nwhere Λ denotes the parameter set of the DNN weights {Wi} in the\\nDSSM. In Figure 9.2, we show the overall DSSM architecture that\\ncontains several DNNs. All these DNNs share the same weights but take\\ndiﬀerent documents (one positive and several negatives) as inputs when\\ntraining the DSSM parameters. Details of the gradient computation\\nof this approximate loss function with respect to the DNN weights\\ntied across documents and queries can be found in [172] and are not\\nelaborated here.\\nMost recently, the DSSM described above has been extended to its\\nconvolutional version, or C-DSSM [328]. In the C-DSSM, semantically\\nsimilar words within context are projected to vectors that are close\\nto each other in the contextual feature space through a convolutional\\nstructure. The overall semantic meaning of a sentence is found to be\\ndetermined by a few key words in the sentence, and thus the C-DSSM\\nuses an additional max pooling layer to extract the most salient local\\nfeatures to form a ﬁxed-length global feature vector. The global feature\\nvector is then fed to the remaining nonlinear DNN layer(s) to map it\\nto a point in the shared semantic space.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='09d5313d-f43a-4b30-9d60-aee00dce5320', embedding=None, metadata={'page_label': '316', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='316 Selected Applications in Information Retrieval\\nFigure 9.2: Architectural illustration of the DSSM for document retrieval (from\\n[170, 171]). All DNNs shown have shared weights. A set of n documents are shown\\nhere to illustrate the random negative sampling discussed in the text for simplifying\\nthe training procedure for the DSSM. [after [172], @CIKM].\\nThe convolutional neural network component of the C-DSSM is\\nshown in Figure 9.3, where a window size of three is illustrated for\\nthe convolutional layer. The overall C-DSSM architecture is similar\\nto the DSSM architecture shown in Figure 9.2 except that the fully-\\nconnected DNNs are replaced by the convolutional neural networks\\nwith locally-connected tied weights and additional max-pooling layers.\\nThe model component shown in Figure 9.3 contains (1) a word hashing\\nlayer to transform words into letter-tri-gram count vectors in the same\\nway as the DSSM; (2) a convolutional layer to extract local contextual\\nfeatures for each context window; (3) a max-pooling layer to extract\\nand combine salient local contextual features to form a global feature\\nvector; and (4) a semantic layer to represent the high-level semantic\\ninformation of the input word sequence.\\nThe main motivation for using the convolutional structure in the\\nC-DSSM is its ability to map a variable-length word sequence to a low-\\ndimensional vector in a latent semantic space. Unlike most previous\\nmodels that treat a query or a document as a bag of words, a query\\nor a document in the C-DSSM is viewed as a sequence of words with\\ncontextual structures. By using the convolutional structure, local con-\\ntextual information at the word n-gram level is modeled ﬁrst. Then,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0a2285eb-65de-455f-b7bd-53c899e4a94d', embedding=None, metadata={'page_label': '317', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.4. Use of deep stacking networks for information retrieval 317\\nFigure 9.3:The convolutional neural network component of the C-DSSM, with the\\nwindow size of three is illustrated for the convolutional layer. [after [328], @WWW].\\nsalient local features in a word sequence are combined to form a global\\nfeature vector. Finally, the high-level semantic information of the word\\nsequence is extracted to form a global vector representation. Like the\\nDSSM just described, the C-DSSM is also trained on click-through data\\nby maximizing the conditional likelihood of the clicked documents given\\na query using the back-propagation algorithm.\\n9.4 Use of deep stacking networks for information retrieval\\nIn parallel with the IR studies reviewed above, the deep stacking net-\\nwork (DSN) discussed in Section 6 has also been explored recently\\nfor IR with insightful results [88]. The experimental results suggest\\nthat the classiﬁcation error rate using the binary decision of “relevant”\\nversus “non-relevant” from the DSN, which is closely correlated with\\nthe DSN training objective, is also generally correlated well with the\\nNDCG (normalized discounted cumulative gain) as the most common', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7944d75e-dc09-49a4-b13c-7b3da2811fa2', embedding=None, metadata={'page_label': '318', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='318 Selected Applications in Information Retrieval\\nIR quality measure. The exception is found in the region of high IR\\nquality.\\nAs described in Section 6, the simplicity of the DSN’s training objec-\\ntive, the mean square error (MSE), drastically facilitates its success-\\nful applications to image recognition, speech recognition, and speech\\nunderstanding. The MSE objective and classiﬁcation error rate have\\nbeen shown to be well correlated in these speech or image applications.\\nFor information retrieval (IR) applications, however, the inconsistency\\nbetween the MSE objective and the desired objective (e.g., NDCG)\\nis much greater than that for the above classiﬁcation-focused applica-\\ntions. For example, the NDCG as a desirable IR objective function is\\na highly non-smooth function of the parameters to be learned, with a\\nvery diﬀerent nature from the nonlinear relationship between MSE and\\nclassiﬁcation error rate. Thus, it is of interest to understand to what\\nextent the NDCG is reasonably well correlated with classiﬁcation rate\\nor MSE where the relevance level in IR is used as the DSN prediction\\ntarget. Further, can the advantage of learning simplicity in the DSN\\nbe applied to improve IR quality measures such as the NDCG? Our\\nexperimental results presented in [88] provide largely positive answers\\nto both of the above questions. In addition, special care that need to\\nbe taken in implementing DSN learning algorithms when moving from\\nclassiﬁcation to IR applications are addressed.\\nThe IR task in the experiments of [88] is the sponsored search\\nrelated to ad placement. In addition to the organic web search\\nresults, commercial search engines also provide supplementary spon-\\nsored results in response to the user’s query. The sponsored search\\nresults are selected from a database pooled by advertisers who bid to\\nhave their ads displayed on the search result pages. Given an input\\nquery, the search engine will retrieve relevant ads from the database,\\nrank them, and display them at the proper place on the search result\\npage; e.g., at the top or right hand side of the web search results. Find-\\ning relevant ads to a query is quite similar to common web search. For\\ninstance, although the documents come from a constrained database,\\nthe task resembles typical search ranking that targets on predicting\\ndocument relevance to the input query. The experiments conducted for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6bb3ddc1-91c7-4b08-a1f5-58e98e330234', embedding=None, metadata={'page_label': '319', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9.4. Use of deep stacking networks for information retrieval 319\\nthis task are the ﬁrst with the use of deep learning techniques (based\\non the DSN architecture) on the ad-related IR problem. The prelimi-\\nnary results from the experiments are the close correlation between the\\nMSE as the DSN training objective with the NDCG as the IR quality\\nmeasure over a wide NDCG range.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1fec4769-5c85-4170-9b9e-86ab9b54e0b7', embedding=None, metadata={'page_label': '320', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10\\nSelected Applications in Object Recognition\\nand Computer Vision\\nOver the past two years or so, tremendous progress has been made in\\napplying deep learning techniques to computer vision, especially in the\\nﬁeld of object recognition. The success of deep learning in this area\\nis now commonly accepted by the computer vision community. It is\\nthe second area in which the application of deep learning techniques\\nis successful, following the speech recognition area as we reviewed and\\nanalyzed in Sections 2 and 7.\\nExcellent surveys on the recent progress of deep learning for\\ncomputer vision are available in the NIPS-2013 tutorial (https://\\nnips.cc/Conferences/2013/Program/event.php?ID=4170 with video\\nrecording at http://research.microsoft.com/apps/video/default.aspx?\\nid=206976&l=i) and slides at http://cs.nyu.edu/∼fergus/presentations/\\nnips2013_ﬁnal.pdf, and also in the CVPR-2012 tutorial (http://cs.nyu.\\nedu/∼fergus/tutorials/deep_learning_cvpr12). The reviews provided\\nin this section below are based partly on these tutorials, in connection\\nwith the earlier deep learning material in this monograph. Another\\nexcellent source which this section draws from is the most recent Ph.D.\\nthesis on the topic of deep learning for computer vision [434].\\n320', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0a4c8d53-cd1f-4501-851d-3ca4111d7454', embedding=None, metadata={'page_label': '321', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.1. Unsupervised or generative feature learning 321\\nOver many years, object recognition in computer vision has been\\nrelying on hand-designed features such as SIFT (scale invariant fea-\\nture transform) and HOG (histogram of oriented gradients), akin to\\nthe reliance of speech recognition on hand-designed features such as\\nMFCC and PLP. However, features like SIFT and HOG only capture\\nlow-level edge information. The design of features to eﬀectively capture\\nmid-level information such as edge intersections or high-level represen-\\ntation such as object parts becomes much more diﬃcult. Deep learning\\naims to overcome such challenges by automatically learning hierarchies\\nof visual features in both unsupervised and supervised manners directly\\nfrom data. The review below categorizes the many deep learning meth-\\nods applied to computer vision into two classes: (1) unsupervised fea-\\nture learning where the deep learning is used to extract features only,\\nwhich may be subsequently fed to relatively simple machine learning\\nalgorithm for classiﬁcation or other tasks; and (2) supervised learning\\nmethods where end-to-end learning is adopted to jointly optimize fea-\\nture extractor and classiﬁer components of the full system when large\\namounts of labeled training data are available.\\n10.1 Unsupervised or generative feature learning\\nWhen labeled data are relatively scarce, unsupervised learning algo-\\nrithms have been shown to learn useful visual feature hierarchies. In\\nfact, prior to the demonstration of remarkable successes of CNN archi-\\ntectures with supervised learning in the 2012 ImageNet competition,\\nmuch of the work in applying deep learning methods to computer\\nvision had been on unsupervised feature learning. The original unsuper-\\nvised deep autoencoder that exploits DBN pre-training was developed\\nand demonstrated by Hinton and Salakhutdinov [164] with success on\\nthe image recognition and dimensionality reduction (coding) tasks of\\nMNIST with only 60,000 samples in the training set; see details of this\\ntask in http://yann.lecun.com/exdb/mnist/ and an analysis in [78].\\nIt is interesting to note that the gain of coding eﬃciency using the DBN-\\nbased autoencoder on the image data over the conventional method of\\nprincipal component analysis as demonstrated in [164] is very similar to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b27595f2-b02a-4a29-9199-1c3342d46497', embedding=None, metadata={'page_label': '322', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='322 Selected Applications in Object Recognition and Computer Vision\\nthe gain reported in [100] and described in Section 4 of this monograph\\non the speech data over the traditional technique of vector quantiza-\\ntion. Also, Nair and Hinton [265] developed a modiﬁed DBN where the\\ntop-layer model uses a third-order Boltzmann machine. This type of\\nDBN is applied to the NORB database — a three-dimensional object\\nrecognition task. An error rate close to the best published result on this\\ntask is reported. In particular, it is shown that the DBN substantially\\noutperforms shallow models such as SVMs. In [358], two strategies to\\nimprove the robustness of the DBN are developed. First, sparse connec-\\ntions in the ﬁrst layer of the DBN are used as a way to regularize the\\nmodel. Second, a probabilistic de-noising algorithm is developed. Both\\ntechniques are shown to be eﬀective in improving robustness against\\nocclusion and random noise in a noisy image recognition task. DBNs\\nhave also been successfully applied to create compact but meaning-\\nful representations of images [360] for retrieval purposes. On this large\\ncollection image retrieval task, deep learning approaches also produced\\nstrong results. Further, the use of a temporally conditional DBN for\\nvideo sequence and human motion synthesis were reported in [361]. The\\nconditional RBM and DBN make the RBM and DBN weights associ-\\nated with a ﬁxed time window conditioned on the data from previous\\ntime steps. The computational tool oﬀered in this type of temporal\\nDBN and the related recurrent networks may provide the opportunity\\nto improve the DBN–HMMs towards eﬃcient integration of temporal-\\ncentric human speech production mechanisms into DBN-based speech\\nproduction model.\\nDeep learning methods have a rich family, including hierarchical\\nprobabilistic and generative models (neural networks or otherwise).\\nOne most recent example of this type developed and applied to facial\\nexpression datasets is the stochastic feed-forward neural networks that\\ncan be learned eﬃciently and that can induce a rich multiple-mode\\ndistribution in the output space not possible with the standard, deter-\\nministic neural networks [359]. In Figure 10.1, we show the architecture\\nof a typical stochastic feed-forward neural network with four hidden\\nlayers with mixed deterministic and stochastic neurons (left) used to\\nmodel multi-mode distributions illustrated on the right. The stochastic\\nnetwork here is a deep, directed graphical model, where the generation', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6cc90d2f-f9c0-4778-b018-bc14bd460788', embedding=None, metadata={'page_label': '323', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.1. Unsupervised or generative feature learning 323\\nFigure 10.1: Left: A typical architecture of the stochastic feed-forward neural\\nnetwork with four hidden layers. Right: Illustration of how the network can produce\\na distribution with two distinct modes and use them to represent two or more\\ndiﬀerent facial expressions y given a neutral face x. [after [359], @NIPS].\\nprocess starts from input x, a neural face, and generates the output\\ny, the facial expression. In face expression classiﬁcation experiments,\\nthe learned unsupervised hidden features generated from this stochas-\\ntic network are appended to the image pixels and helped to obtain\\nsuperior accuracy to the baseline classiﬁer based on the conditional\\nRBM/DBN [361].\\nPerhaps the most notable work in the category of unsupervised deep\\nfeature learning for computer vision (prior to the recent surge of the\\nwork on CNNs) is that of [209], a nine-layer locally connected sparse\\nautoencoder with pooling and local contrast normalization. The model\\nhas one billion connections, trained on the dataset with 10 million\\nimages downloaded from the Internet. The unsupervised feature learn-\\ning methods allow the system to train a face detector without having to\\nlabel images as containing a face or not. And the control experiments\\nshow that this feature detector is robust not only to translation but\\nalso to scaling and out-of-plane rotation.\\nAnother set of popular studies on unsupervised deep feature learn-\\ning for computer vision are based on deep sparse coding models [226].\\nThis type of deep models produced state-of-the-art accuracy results on\\nthe ImageNet object recognition tasks prior to the rise of the CNN\\narchitectures armed with supervised learning to perform joint feature\\nlearning and classiﬁcation, which we turn to now.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cc4eec0e-f032-44e4-a8a8-1798e8f69440', embedding=None, metadata={'page_label': '324', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='324 Selected Applications in Object Recognition and Computer Vision\\n10.2 Supervised feature learning and classiﬁcation\\nThe origin of the applications of deep learning to object recognition\\ntasks can be traced to the convolutional neural networks (CNNs)\\nin the early 90s; see a comprehensive overview in [212]. The CNN-\\nbased architectures in the supervised learning mode have captured\\nintense interest in computer vision since October 2012 shortly after\\nthe ImageNet competition results were released (http://www.image-\\nnet.org/challenges/LSVRC/2012/). This is mainly due to the huge\\nrecognition accuracy gain over competing approaches when large\\namounts of labeled data are available to eﬃciently train large CNNs\\nusing GPU-like high-performance computing platforms. Just like DNN-\\nbased deep learning methods have outperformed previous state-of-\\nthe-art approaches in speech recognition in a series of benchmark\\ntasks including phone recognition, large-vocabulary speech recognition,\\nnoise-robust speech recognition, and multi-lingual speech recognition,\\nCNN-based deep learning methods have demonstrated the same in a\\nset of computer vision benchmark tasks including category-level object\\nrecognition, object detection, and semantic segmentation.\\nThe basic architecture of the CNN described in [212] is shown in\\nFigure 10.1. To incorporate the relative invariance of the spatial rela-\\ntionship in typical image pixels with respect to the location, the CNN\\nuses a convolutional layer with local receptive ﬁelds and with tied ﬁl-\\nter weights, much like 2-dimensional FIR ﬁlters in image processing.\\nThe output of the FIR ﬁlters is then passed through a nonlinear acti-\\nvation function to create activation maps, followed by another non-\\nlinear pooling (labeled as “subsampling” in Figure 10.2) layer that\\nreduces the data rate while providing invariance to slightly diﬀer-\\nent input images. The output of the pooling layer is fed to a few\\nfully connected layers as in the DNN discussed in earlier chapters.\\nThe whole architecture above is also called the deep CNN in the\\nliterature.\\nDeep models with convolution structure such as CNNs have been\\nfound eﬀective and have been in use in computer vision and image\\nrecognition since 90s [57, 185, 192, 198, 212]. The most notable advance\\nwas achieved in the 2012 ImageNet LSVRC competition, in which', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='43bbdc59-84bc-4974-a2d0-1b50281a4f2c', embedding=None, metadata={'page_label': '325', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.2. Supervised feature learning and classiﬁcation 325\\nFigure 10.2: The original convolutional neural network that is composed of mul-\\ntiple alternating convolution and pooling layers followed by fully connected layers.\\n[after [212], @IEEE].\\nthe task is to train a model with 1.2 million high-resolution images\\nto classify unseen images to one of the 1000 diﬀerent image classes.\\nOn the test set consisting of 150k images, the deep CNN approach\\ndescribed in [198] achieved the error rates considerably lower than the\\nprevious state-of-the-art. Very large deep-CNNs are used, consisting of\\n60 million weights, and 650,000 neurons, and ﬁve convolutional layers\\ntogether with max-pooling layers. Additional two fully-connected layers\\nas in the DNN described previously are used on top of the CNN layers.\\nAlthough all the above structures were developed separately in earlier\\nwork, their best combination accounted for major part of the success.\\nSee the overall architecture of the deep CNN system in Figure 10.3. Two\\nadditional factors contribute to the ﬁnal success. The ﬁrst is a powerful\\nregularization technique called “dropout”; see details in [166] and a\\nseries of further analysis and improvement in [10, 13, 240, 381, 385]. In\\nparticular, Warde-Farley et al. [385] analyzed the disentangling eﬀects\\nof dropout and showed that it helps because diﬀerent members of the\\nbag share parameters. Applications of the same “dropout” techniques\\nare also successful for some speech recognition tasks [65, 81]. The\\nsecond factor is the use of non-saturating neurons or rectiﬁed linear\\nunits (ReLU) that compute f(x)=m a x ( x, 0), which signiﬁcantly\\nspeeds up the overall training process especially with eﬃcient GPU\\nimplementation. This deep-CNN system achieved a winning top-5 test\\nerror rate of 15.3% using extra training data from ImageNet Fall 2011\\nrelease, or 16.4% using only supplied training data in ImageNet-2012,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1fa5af3e-a82b-4466-8002-22564186c544', embedding=None, metadata={'page_label': '326', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='326 Selected Applications in Object Recognition and Computer Vision\\nFigure 10.3: The architecture of the deep-CNN system which won the 2012 Ima-\\ngeNet competition by a large margin over the second-best system and the state of\\nthe art by 2012. [after [198], @NIPS].\\nsigniﬁcantly lower than 26.2% achieved by the second-best system\\nwhich combines scores from many classiﬁers using a set of hand-\\ncrafted features such as SIFT and Fisher vectors. See details in http://\\nwww.image-net.org/challenges/LSVRC/2012/oxford_vgg.pdf about\\nthe best competing method. It is noted, however, that the Fisher-\\nvector-encoding approach has recently been extended by Simonyan\\net al. [329] via stacking in multiple layers to form deep Fisher net-\\nworks, which achieve competitive results with deep CNNs at a smaller\\ncomputational learning cost.\\nThe state of the art performance demonstrated in [198] using the\\ndeep-CNN approach is further improved by another signiﬁcant mar-\\ngin during 2013, using a similar approach but with bigger models\\nand larger amounts of training data. A summary of top-5 test error\\nrates from 11 top-performing teams participating in the 2013 Ima-\\ngeNet ILSVRC competition is shown in Figure 10.4, with the best\\nresult of the 2012 competition shown to the right most as the baseline.\\nHere we see rapid error reduction on the same task from the lowest\\npre-2012 error rate of 26.2% (non-neural networks) to 15.3% in 2012\\nand further to 11.2% in 2013, both achieved with deep-CNN technol-\\nogy. It is also interesting to observe that all major entries in the 2013\\nImageNet ILSVRC competition is based on deep learning approaches.\\nFor example, the Adobe system shown in Figure 10.4 is based on the\\ndeep-CNN reported in [198] including the use of dropout. The network', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b7c0a0d8-8e9c-4fe9-ae28-a9237620e2ab', embedding=None, metadata={'page_label': '327', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.2. Supervised feature learning and classiﬁcation 327\\nFigure 10.4: Summary results of ImageNet Large Scale Visual Recognition\\nChallenge 2013 (ILSVRC2013), representing the state-of-the-are performance of\\nobject recognition systems. Data source: http://www.image-net.org/challenges/\\nLSVRC/2013/results.php.\\narchitecture is modiﬁed to include more ﬁlters and connections. At test\\ntime, image saliency is used to obtain 9 crops from original images,\\nwhich are combined with the standard ﬁve multiview crops. The NUS\\nsystem uses a non-parametric, adaptive method to combine the out-\\nputs from multiple shallow and deep experts, including deep-CNN,\\nkernel, and GMM methods. The VGG system is described in [329]\\nand uses a combination of the deep Fisher vector network and the\\ndeep-CNN. The ZF system is based on a combination of a large CNN\\nwith a range of diﬀerent architectures. The choice of architectures was\\nassisted by visualization of model features using a deconvolutional net-\\nwork as described by Zeiler et al. [437], Zeiler and Fergus [435, 436],\\nand Zeiler ([434]). The CognitiveVision system uses an image classiﬁ-\\ncation scheme based on a DNN architecture. The method is inspired\\nby cognitive psychophysics about how the human vision system ﬁrst\\nlearns to classify the basic-level categories and then learns to clas-\\nsify categories at the subordinate level for ﬁne-grained object recogni-\\ntion. Finally, the best-performing system called Clarifai in Figure 10.4\\nis based on a large and deep CNN with dropout regularization. It', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='607c19a0-6719-48e9-84f2-c69e7d8fbc51', embedding=None, metadata={'page_label': '328', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='328 Selected Applications in Object Recognition and Computer Vision\\naugments the amount of training data by down-sampling images to\\n256 pixels. The system contains a total of 65M parameters. Multiple\\nsuch models were averaged together to further boost performance. The\\nmain novelty is to use the visualization technique based on the deconvo-\\nlutional networks as described in [434, 437] to identify what makes the\\ndeep model perform well, based on which a powerful deep architecture\\nwas chosen. See more details of these systems in http://www.image-\\nnet.org/challenges/LSVRC/2013/results.php.\\nWhile the deep CNN has demonstrated remarkable classiﬁcation\\nperformance on object recognition tasks, there has been no clear under-\\nstanding of why they perform so well until recently. Zeiler and Fergus\\n[435, 436] conducted research to address just this issue, and then used\\nthe gained understanding to further improve the CNN systems, which\\nyielded excellent performance as shown in Figure 10.4 with labels “ZF”\\nand “Clarifai. ” A novel visualization technique is developed that gives\\ninsight into the function of intermediate feature layers of the deep CNN.\\nThe technique also sheds light onto the operation of the full network\\nacting as a classiﬁer. The visualization technique is based on a decon-\\nvolutional network, which maps the neural activities in intermediate\\nlayers of the original convolutional network back to the input pixel\\nspace. This allows the researchers to examine what input pattern orig-\\ninally caused a given activation in the feature maps. Figure 10.5 (the\\ntop portion) illustrates how a deconvolutional network is attached to\\neach of its layers, thereby providing a closed loop back to image pixels\\nas the input to the original CNN. The information ﬂow in this closed\\nloop is as follows. First, an input image is presented to the deep CNN in\\na feed-forward manner so that the features at all layers are computed.\\nTo examine a given CNN activation, all other activations in the layer\\nare set to zero and the feature maps are passed as input to the attached\\ndeconvolutional network’s layer. Then, successive operations, opposite\\nto the feed-forward computation in the CNN, are carried out including\\nunpooling, rectifying, and ﬁltering. This allows the reconstruction of\\nthe activity in the layer beneath that gave rise to the chosen activa-\\ntion. These operations are repeated until input layer is reached. During\\nunpooling, non-invertibility of the max pooling operation in the CNN is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='033fe370-3efc-4ae8-b503-2c0361644277', embedding=None, metadata={'page_label': '329', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10.2. Supervised feature learning and classiﬁcation 329\\nFigure 10.5: The top portion shows how a deconvolutional network’s layer (left)\\nis attached to a corresponding CNN’s layer (right). The d econvolutional network\\nreconstructs an approximate version of the CNN features from the layer below. The\\nbottom portion is an illustration of the unpooling operation in the deconvolutional\\nnetwork, where “Switches” are used to record the location of the local max in each\\npooling region during pooling in the CNN. [after [436], @arXiv].\\nresolved by an approximate inverse, where the locations of the maxima\\nwithin each pooling region are recorded in a set of “switch” variables.\\nThese switches are used to place the reconstructions from the layer\\nabove into appropriate locations, preserving the structure of the stim-\\nulus. This procedure is shown at the bottom portion of Figure 10.5.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='da9fb8ad-9822-4586-9bde-ded59974d0c5', embedding=None, metadata={'page_label': '330', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='330 Selected Applications in Object Recognition and Computer Vision\\nIn addition to the deep-CNN architecture described above, the DNN\\narchitecture has also been shown to be highly successful in a number\\nof computer vision tasks [54, 55, 56, 57]. We have not found in the\\nliterature on direct comparisons among the CNN, DNN, and other\\nrelated architectures on the identical tasks.\\nFinally, the most recent study on supervised learning for computer\\nvision shows that the deep CNN architecture is not only successful for\\nobject/image classiﬁcation discussed earlier in this section but also suc-\\ncessful for objection detection in the whole images [128]. The detection\\ntask is substantially more complex than the classiﬁcation task.\\nAs a brief summary of this chapter, deep learning has made huge\\ninroads into computer vision, soon after its success in speech recogni-\\ntion discussed in Section 7. So far, it is the supervised learning paradigm\\nbased on the deep CNN architecture and the related classiﬁcation tech-\\nniques that are making the greatest impact, showcased by the ImageNet\\ncompetition results from 2012 and 2013. These methods can be used\\nfor not only object recognition but also many other computer vision\\ntasks. There has been some debate as to the reasons for the success of\\nthese CNN-based deep learning methods, and about their limitations.\\nMany questions are still open as to how these methods can be tai-\\nlored to certain computer vision applications and how to scale up the\\nmodels and training data. Finally, we discussed a number of studies on\\nunsupervised and generative approaches of deep learning to computer\\nvision and image modeling problems in the earlier part of this chapter.\\nTheir performance has not been competitive with the supervised learn-\\ning approach on object recognition tasks with ample training data. To\\nachieve long term and ultimate success in computer vision, it is likely\\nthat unsupervised learning will be needed. To this end, many open\\nproblems in unsupervised feature learning and deep learning need to\\nbe addressed and much more research need to be carried out.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='95240315-14e2-4065-89f9-ecbf41e75ff8', embedding=None, metadata={'page_label': '331', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11\\nSelected Applications in Multimodal\\nand Multi-task Learning\\nMulti-task learning is a machine learning approach that learns to solve\\nseveral related problems at the same time, using a shared represen-\\ntation. It can be regarded as one of the two major classes of transfer\\nlearning or learning with knowledge transfer, which focuses on general-\\nizations across distributions, domains, or tasks. The other major class\\nof transfer learning is adaptive learning, where knowledge transfer is\\ncarried out in a sequential manner, typically from a source task to a\\ntarget task [95]. Multi-modal learning is a closely related concept to\\nmulti-task learning, where the learning domains or “tasks” cut across\\nseveral modalities for human–computer interactions or other applica-\\ntions embracing a mixture of textual, audio/speech, touch, and visual\\ninformation sources.\\nThe essence of deep learning is to automate the process of dis-\\ncovering eﬀective features or representations for any machine learn-\\ning task, including automatically transferring knowledge from one task\\nto another concurrently. Multi-task learning is often applied to con-\\nditions where no or very little training data are available for the tar-\\nget task domain, and hence is sometimes called zero-shot or one-shot\\nlearning. It is evident that diﬃcult multi-task leaning naturally ﬁts the\\nparadigm of deep learning or representation learning where the shared\\n331', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b71050d2-d3a5-4167-90d3-d8951f10fb82', embedding=None, metadata={'page_label': '332', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='332 Selected Applications in Multimodal and Multi-task Learning\\nrepresentations and statistical strengths across tasks (e.g., those involv-\\ning separate modalities of audio, image, touch, and text) is expected\\nto greatly facilitate many machine learning scenarios under low- or\\nzero-resource conditions. Before deep learning methods were adopted,\\nthere had been numerous eﬀorts in multi-modal and multi-task learn-\\ning. For example, a prototype called MiPad for multi-modal interac-\\ntions involving capturing, leaning, coordinating, and rendering a mix\\nof speech, touch, and visual information was developed and reported\\nin [175, 103]. And in [354, 443], mixed sources of information from\\nmultiple-sensory microphones with separate bone-conductive and air-\\nborn paths were exploited to de-noise speech. These early studies all\\nused shallow models and learning methods and achieved worse than\\ndesired performance. With the advent of deep learning, it is hopeful\\nthat the diﬃcult multi-modal learning problems can be solved with\\neventual success to enable a wide range of practical applications. In\\nthis chapter, we will review selected applications in this area, orga-\\nnized according to diﬀerent combinations of more than one modalities\\nor learning tasks. Much of the work reviewed here is on-going research,\\nand readers should expect follow-up publications in the future.\\n11.1 Multi-modalities: Text and image\\nThe underlying mechanism for potential eﬀectiveness of multi-modal\\nlearning involving text and image is the common semantics associated\\nwith the text and image. The relationship between the text and image\\nmay come, for example, from the text annotations of an image (as the\\ntraining data for a multi-modal learning system). If the related text\\nand image share the same representation in a common semantic space,\\nthe system can generalize to the unseen situation where either text\\nor image is unavailable. It can thus be naturally used for zero-shot\\nlearning for image or text. In other words, multi-modality learning can\\nuse text information to help image/visual recognition, and vice versa.\\nExploiting text information for image/visual recognition constitutes\\nmost of the work done in this space, which we review in this section\\nbelow.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e6da3778-a7c6-459a-8eb4-e1126c23d17c', embedding=None, metadata={'page_label': '333', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.1. Multi-modalities: Text and image 333\\nThe deep architecture, called DeViSE (deep visual-semantic embed-\\nding) and developed by Frome et al. [117], is a typical example of the\\nmulti-modal learning where text information is used to improve the\\nimage recognition system, especially for performing zero-shot learning.\\nImage recognition systems are often limited in their ability to scale\\nto large number of object categories, due in part to the increasing\\ndiﬃculty of acquiring suﬃcient training data with text labels as the\\nnumber of image categories grows. The multi-modal DeViSE system\\nis aimed to leverage text data to train the image models. The joint\\nmodel is trained to identify image classes using both labeled image\\ndata and the semantic information learned from unannotated text. An\\nillustration of the DeViSE architecture is shown in the center portion\\nof Figure 10.1. It is initialized with the parameters pre-trained at the\\nlower layers of two models: the deep-CNN for image classiﬁcation in\\nthe left portion of the ﬁgure and the text embedding model in the\\nright portion of the ﬁgure. The part of the deep CNN, labeled “core\\nvisual model” in Figure 10.1, is further learned to predict the target\\nword-embedding vector using a projection layer labeled “transforma-\\ntion” and using a similarity metric. The loss function used in training\\nadopts a combination of dot-product similarity and max-margin, hinge\\nrank loss. The former is the un-normalized version of the cosine loss\\nfunction used for training the DSSM model in [170] as described in\\nSection 9.3. The latter is similar to the earlier joint image-text model\\ncalled WSABIE (web scale annotation by image embedding developed\\nby Weston et al. [388, 389]. The results show that the information pro-\\nvided by text improves zero-shot image predictions, achieving good hit\\nrates (close to 15%) across thousands of the labels never seen by the\\nimage model.\\nThe earlier WSABIE system as described in [388, 389] adopted\\na shallow architecture and trained a joint embedding model of both\\nimages and labels. Rather than using deep architectures to derive the\\nhighly nonlinear image (as well as text-embedding) feature vectors as in\\nDeViSE, the WSABIE uses simple image features and a linear mapping\\nto arrive at the joint embedding space. Further, it uses an embedding\\nvector for each possible label. Thus, unlike DeViSE, WSABIE could\\nnot generalize to new classes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8bd88b11-d98f-41a2-965d-527181a4e2ca', embedding=None, metadata={'page_label': '334', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='334 Selected Applications in Multimodal and Multi-task Learning\\nFigure 11.1:Illustration of the multi-modal DeViSE architecture. The left portion\\nis an image recognition neural network with a softmax output layer. The right por-\\ntion is a skip-gram text model providing word embedding vectors; see Section 8.2\\nand Figure 8.3 for details. The center is the joint deep image-text model of DeViSE,\\nwith the two Siamese branches initialized by the image and word embedding mod-\\nels below the softmax layers. The layer labeled “transformation” is responsible for\\nmapping the outputs of the image (left) and text (right) branches into the same\\nsemantic space. [after [117], @NIPS].\\nIt is also interesting to compare the DeViSE architecture of\\nFigure 11.1 with the DSSM architecture of Figure 9.2 in Section 9.\\nThe branches of “Query” and “Documents” in DSSM are analogous to\\nthe branches of “image” and “text-label” in DeViSE. Both DeViSE and\\nDSSM use the objective function related to cosine distance between\\ntwo vectors for training the network weights in an end-to-end fash-\\nion. One key diﬀerence, however, is that the two sets of inputs to the\\nDSSM are both text (i.e., “Query” and “Documents” designed for IR),\\nand thus mapping “Query” and “Documents” to the same semantic\\nspace is conceptually more straightforward compared with the need\\nin DeViSE for mapping from one modality (image) to another (text).\\nAnother key diﬀerence is that the generalization ability of DeViSE to\\nunseen image classes comes from computing text embedding vectors\\nfor many unsupervised text sources (i.e., with no image counterparts)\\nthat would cover the text labels corresponding to the unseen classes.\\nThe generalization ability of the DSSM over unseen words, however,\\nis derived from a special coding scheme for words in terms of their\\nconstituent letters.\\nThe DeViSE architecture has inspired a more recent method,\\nwhich maps images into the semantic embedding space via convex', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e07912a0-f9e4-4239-9117-ffcfa7f7b923', embedding=None, metadata={'page_label': '335', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.1. Multi-modalities: Text and image 335\\ncombination of embedding vectors for the text label and the image\\nclasses [270]. Here is the main diﬀerence. DeViSE replaces the last,\\nsoftmax layer of a CNN image classiﬁer with a linear transformation\\nlayer. The new transformation layer is then trained together with the\\nlower layers of the CNN. The method in [270] is much simpler — keep-\\ning the softmax layer of the CNN while not training the CNN. For a\\ntest image, the CNN ﬁrst produces top N-best candidates. Then, the\\nconvex combination of the corresponding N embedding vectors in the\\nsemantic space is computed. This gives a deterministic transformation\\nfrom the outputs of the softmax classiﬁer into the embedding space.\\nThis simple multi-modal learning method is shown to work very well\\non the ImageNet zero-shot learning task.\\nAnother thread of studies separate from but related to the above\\nwork on multi-modal learning involving text and image have cen-\\ntered on the use of multi-modal embeddings, where data from multiple\\nsources with separate modalities of text and image are projected into\\nthe same vector space. For example, Socher and Fei-Fei [341] project\\nwords and images into the same space using kernelized canonical cor-\\nrelation analysis. Socher et al. [342] map images to single-word vectors\\nso that the constructed multi-modal system can classify images with-\\nout seeing any examples of the class, i.e., zero-shot learning similar\\nto the capability of DeViSE. The most recent work by Socher et al.\\n[343] extends their earlier work from single-word embeddings to those\\nof phrases and full-length sentences. The mechanism for mapping sen-\\ntences instead of the earlier single words into the multi-modal embed-\\nding space is derived from the power of the recursive neural network\\ndescribed in Socher et al. [347] as summarized in Section 8.2, and its\\nextension with dependency tree.\\nIn addition to mapping text to image (or vice versa) into the same\\nvector space or to creating the joint image/text embedding space,\\nmulti-modal learning for text and image can also be cast in the frame-\\nwork of language models. In [196], a model of natural language is made\\nconditioned on other modalities such as image as the focus of the\\nstudy. This type of multi-modal language model is used to (1) retrieve\\nimages given complex description queries, (2) retrieve phrase descrip-\\ntions given image queries, and (3) generate text conditioned on images.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='23ddaf68-23b0-4fc8-bb0a-96867a8e47cc', embedding=None, metadata={'page_label': '336', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='336 Selected Applications in Multimodal and Multi-task Learning\\nFigure 11.2:Illustration of the multi-modal DeViSE architecture. The left portion\\nis an image recognition neural network with a softmax output layer. The right por-\\ntion is a skip-gram text model providing word embedding vectors; see Section 8.2\\nand Figure 8.3 for details. The center is the joint deep image-text model of DeViSE,\\nwith the two Siamese branches initialized by the image and word embedding mod-\\nels below the softmax layers. The layer labeled “transformation” is responsible for\\nmapping the outputs of the image (left) and text (right) branches into the same\\nsemantic space. [after [196], @NIPS].\\nWord representations and image features are jointly learned by train-\\ning the multi-modal language model together with a convolutional net-\\nwork. An illustration of the multi-modal language model is shown in\\nFigure 11.2.\\n11.2 Multi-modalities: Speech and image\\nNgiam et al. [268, 269] propose and evaluate an application of\\ndeep networks to learn features over audio/speech and image/video\\nmodalities. They demonstrate cross-modality feature learning, where\\nbetter features for one modality (e.g., image) is learned when multiple\\nmodalities (e.g., speech and image) are present at feature learning time.\\nA bi-modal deep autoencoder architecture for separate audio/speech\\nand video/image input channels are shown in Figure 11.3. The essence\\nof this architecture is to use a shared, middle layer to represent both\\ntypes of modalities. This is a straightforward generalization from\\nthe single-modal deep autoencoder for speech shown in Figure 4.1 of\\nSection 4 to bi-modal counterpart. The authors further show how to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f7b816f0-6dca-437b-9228-1ac4e1292e82', embedding=None, metadata={'page_label': '337', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.2. Multi-modalities: Speech and image 337\\nFigure 11.3: The architecture of a deep denoising autoencoder for multi-modal\\naudio/speech and visual features. [after [269], @ICML].\\nlearn a shared audio and video representation, and evaluate it on a\\nﬁxed task, where the classiﬁer is trained with audio-only data but\\ntested with video-only data and vice versa. The work concludes that\\ndeep learning architectures are generally eﬀective in learning multi-\\nmodal features from unlabeled data and in improving single modality\\nfeatures through cross modality information transfer. One exception\\nis the cross-modality setting using the CUAVE dataset. The results\\npresented in [269, 268] show that learning video features with both\\nvideo and audio outperforms that with only video data. However, the\\nsame paper also shows that a model of [278] in which a sophisticated\\nsignal processing technique for extracting visual features, together\\nwith the uncertainty-compensation method developed originally from\\nrobust speech recognition [104], gives the best classiﬁcation accuracy\\nin the cross-modal learning task, beating the features derived from the\\ngenerative deep architecture designed for this task.\\nWhile the deep generative architecture for multimodal learning\\ndescribed in [268, 269] is based on non-probabilistic autoencoder neural', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4091ee67-1f9c-4dda-81a0-a990a1d64600', embedding=None, metadata={'page_label': '338', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='338 Selected Applications in Multimodal and Multi-task Learning\\nnets, a probabilistic version based on deep Boltzmann machine (DBM)\\nhas appeared more recently for the same multimodal application. In\\n[348], a DBM is used to extract a uniﬁed representation integrat-\\ning separate modalities, useful for both classiﬁcation and information\\nretrieval tasks. Rather than using the “bottleneck” layers in the deep\\nautoencoder to represent multimod al inputs, here a p robability den-\\nsity is deﬁned on the joint space of multimodal inputs, and states of\\nsuitably deﬁned latent variables are used for the representation. The\\nadvantage of this probabilistic formulation, possibly lacking in the tra-\\nditional deep autoencoder, is that the missing modality’s information\\ncan be ﬁlled in naturally by sampling from its conditional distribution.\\nMore recent work on autoencoders [22, 30] shows the capability of gen-\\neralized denoising autoencoders in carrying out sampling, thus they\\nmay overcome the earlier problem of ﬁlling-in the missing modality’s\\ninformation. For the bi-modal data consisting of image and text, the\\nmultimodal DBM was shown to slightly outperform the traditional ver-\\nsion of the deep multimodal autoencoder as well as multimodal DBN\\nin classiﬁcation and information retrieval tasks. No results on the com-\\nparisons with the generalized version of deep autoencoders has been\\nreported but may appear soon.\\nThe several architectures discussed so far in this chapter for multi-\\nmodal processing and learning can be regarded as special cases of\\nmore general multi-task learning and transfer learning [22, 47]. Trans-\\nfer learning, encompassing both adaptive and multi-task learning, refers\\nto the ability of a learning architecture and technique to exploit com-\\nmon hidden explanatory factors among diﬀerent learning tasks. Such\\nexploitation permits sharing of aspects of diverse types of input data\\nsets, thus allowing the possibility of transferring knowledge across seem-\\ningly diﬀerent learning tasks. As argued in [22], the learning archi-\\ntecture shown in Figure 11.4 and the associated learning algorithms\\nhave an advantage for such tasks because they learn representations\\nthat capture underlying factors, a subset of which may be relevant\\nfor each particular task. We will discuss a number of such multi-task\\nlearning applications in the remainder of this chapter that are conﬁned\\nwith a single modality of speech, natural language processing, or image\\ndomain.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8f211ebd-de07-41f2-99d2-30bfa44b711a', embedding=None, metadata={'page_label': '339', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.3 ML within the speech, NLP or image domain 339\\nFigure 11.4: A DNN architecture for multitask learning that is aimed to dis-\\ncover hidden explanatory factors shared among three tasks A, B, and C. [after [22],\\n@IEEE].\\n11.3 Multi-task learning within the speech, NLP or image\\ndomain\\nWithin the speech domain, one most interesting application of multi-\\ntask learning is multi-lingual or cross-lingual speech recognition, where\\nspeech recognition for diﬀerent languages is considered as diﬀerent\\ntasks. Various approaches have been taken to attack this rather chal-\\nlenging acoustic modeling problem for speech recognition, where the\\ndiﬃculty lies in the lack of transcribed speech data due to economic\\nconsiderations in developing speech recognition systems for all lan-\\nguages in the world. Cross-language data sharing and data weighing\\nare common and useful approaches for the GMM–HMM system [225].\\nAnother successful approach for the GMM–HMM is to map pronunci-\\nation units across languages either via knowledge-based or data-driven\\nmethods [420]. But they are much inferior to the DNN–HMM approach\\nwhich we now summarize.\\nIn recent papers of [94, 170] and [150], two research groups inde-\\npendently developed closely related DNN architectures with multi-task\\nlearning capabilities for multilingual speech recognition. See Figure 11.5\\nfor an illustration of this type of architecture. The idea behind these\\narchitectures is that the hidden layers in the DNN, when learned', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c2806323-a1e7-4455-ba9e-ecd01905ca95', embedding=None, metadata={'page_label': '340', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='340 Selected Applications in Multimodal and Multi-task Learning\\nFigure 11.5: A DNN architecture for multilingual speech recognition. [after [170],\\n@IEEE].\\nappropriately, serve as increasingly complex feature transformations\\nsharing common hidden factors across the acoustic data in diﬀerent\\nlanguages. The ﬁnal softmax layer representing a log-linear classiﬁer\\nmakes use of the most abstract feature vectors represented in the top-\\nmost hidden layer. While the log-linear classiﬁer is necessarily sepa-\\nrate for diﬀerent languages, the feature transformations can be shared\\nacross languages. Excellent multilingual speech recognition results are\\nreported, far exceeding the earlier results using the GMM–HMM based\\napproaches [225, 420]. The implication of this set of work is signif-\\nicant and far reaching. It points to the possibility of quickly build-\\ning a high-performance DNN-based system for a new language from\\nan existing multilingual DNN. This huge beneﬁt would require only a\\nsmall amount of training data from the target language, although hav-\\ning more data would further improve the performance. This multitask\\nlearning approach can reduce the need for the unsupervised pre-training\\nstage, and can train the DNN with much fewer epochs. Extension\\nof this set of work would be to eﬃciently build a language-universal\\nspeech recognition system. Such a system cannot only recognize many\\nlanguages and improve the accuracy for each individual language, but', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6dcbc4b8-5e33-4655-a907-b1adf9ceb243', embedding=None, metadata={'page_label': '341', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11.3. ML within the speech, NLP or image domain 341\\nFigure 11.6: A DNN architecture for speech recognition trained with mixed-\\nbandwidth acoustic data with 16-kHz and 8-kHz sampling rates; [after [221],\\n@IEEE].\\nalso expand the languages supported by simply stacking softmax layers\\non the DNN for new languages.\\nA closely related DNN architecture, as shown in Figure 11.6, with\\nmultitask learning capabilities was also recently applied to another\\nacoustic modeling problem — learning joint representations for two\\nseparate sets of acoustic data [94, 221]. The set that consists of the\\nspeech data with 16 kHz sampling rate is of wideband and high qual-\\nity, which is often collected from increasingly popular smart phones\\nunder the voice search scenario. Another, narrowband data set has a\\nlower sampling rate of 8kHz, often collected using the telephony speech\\nrecognition systems.\\nAs a ﬁnal example of multi-task learning within the speech domain,\\nlet us consider phone recognition and word recognition as separate\\n“tasks. ” That is, phone recognition results are used not for producing\\ntext outputs but for language-type identiﬁcation or for spoken doc-\\nument retrieval. Then, the use of pronunciation dictionary in almost\\nall speech systems can be considered as multi-task learning that share\\nthe tasks of phone recognition and word recognition. More advanced\\nframeworks in speech recognition have pushed this direction further', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a8eb2f96-7fda-43eb-a68e-8e645391c5f0', embedding=None, metadata={'page_label': '342', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='342 Selected Applications in Multimodal and Multi-task Learning\\nby advocating the use of even ﬁner units of speech than phones to\\nbridge the raw acoustic information of speech to semantic content of\\nspeech via a hierarchy of linguistic structure. These atomic speech units\\ninclude “speech attributes” in the detection-based and knowledge-rich\\nmodeling framework for speech recognition, whose accuracy has been\\nsigniﬁcantly boosted recently by the use of deep learning methods\\n[332, 330, 427].\\nWithin the natural language processing domain, the best known\\nexample of multi-task learning is the comprehensive studies reported\\nin [62, 63], where a range of separate “tasks” of part-of-speech tag-\\nging, chunking, named entity tagging, semantic role identiﬁcation, and\\nsimilar-word identiﬁcation in natural language processing are attacked\\nusing a common representation of words and a uniﬁed deep learning\\napproach. A summary of these studies can be found in Section 8.2.\\nFinally, within the domain of image/vision as a single modality,\\ndeep learning has also been found eﬀective in multi-task learning. Sri-\\nvastava and Salakhutdinov [349] present a multi-task learning approach\\nb a s e do nh i e r a r c h i c a lB a y e s i a np r i o r si naD N Ns y s t e ma p p l i e dt ov a r -\\nious image classiﬁcation data sets. The priors are combined with a\\nDNN, which improves discriminative learning by encouraging infor-\\nmation sharing among tasks and by discovering similar classes among\\nwhich knowledge is transferred. More speciﬁcally, methods are devel-\\noped to jointly learn to classify images and a hierarchy of classes, such\\nthat “poor classes,” for which there are relatively few training examples,\\ncan beneﬁt from similar “rich classes,” for which more training exam-\\nples are available. This work can be considered as an excellent instance\\nof learning output representations, in addition to learning input rep-\\nresentation of the DNN as the focus of nearly all deep learning work\\nreported in the literature.\\nAs another example of multi-task learning within the single-\\nmodality domain of image, Ciresan et al. [58] applied the architec-\\nture of deep CNNs to character recognition tasks for Latin and for\\nChinese. The deep CNNs trained on Chinese characters are shown to\\nbe easily capable of recognizing uppercase Latin letters. Further, learn-\\ning Chinese characters is accelerated by ﬁrst pre-training a CNN on a\\nsmall subset of all classes and then continuing to train on all classes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='329ed659-19a9-4692-8d27-b0a09a11f110', embedding=None, metadata={'page_label': '343', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12\\nConclusion\\nThis monograph ﬁrst presented a brief history of deep learning (focus-\\ning on speech recognition) and developed a categorization scheme to\\nanalyze the existing deep networks in the literature into unsupervised\\n(many of which are generative), supervised, and hybrid classes. The\\ndeep autoencoder, the DSN (as well as many of its variants), and\\nthe DBN–DNN or pre-trained DNN architectures, one in each of the\\nthree classes, are discussed and analyzed in detail, as they appear to\\nbe popular and promising approaches based on the authors’ personal\\nresearch experiences. Applications of deep learning in ﬁve broad\\nareas of information processing are also reviewed, including speech\\nand audio (Section 7), natural language modeling and processing\\n(Section 8), information retrieval (Section 9), object recognition and\\ncomputer vision (Section 10), and multi-modal and multi-task learning\\n(Section 11). There are other interesting yet non-mainstream applica-\\ntions of deep learning, which are not covered in this monograph. For\\ninterested readers, please consult recent papers on the applications of\\ndeep learning to optimal control in [219], to reinforcement learning in\\n[256], to malware classiﬁcation in [66], to compressed sensing in [277],\\nto recognition conﬁdence prediction in [173], to acoustic-articulatory\\ninversion mapping in [369], to emotion recognition from video in [189],\\n343', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3deafd69-1f03-44ff-8278-9749b645f9ff', embedding=None, metadata={'page_label': '344', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='344 Conclusion\\nto emotion recognition from speech in [207, 222], to spoken language\\nunderstanding in [242, 366, 403], to speaker recognition in [351, 372],\\nto language-type recognition in [112], to dialogue state tracking for\\nspoken dialogue systems in [94, 152], to automatic voice activity\\ndetection in [442], to speech enhancement in [396], to voice conversion\\nin [266], and to single-channel source separation in [132, 387].\\nThe literature on deep learning is vast, mostly coming from\\nthe machine learning community. The signal processing community\\nembraced deep learning only within the past four years or so (start-\\ning around end of 2009) and the momentum is growing fast ever since.\\nThis monograph is written mainly from the signal and information pro-\\ncessing perspective. Beyond surveying the existing deep learning work,\\na classiﬁcatory scheme based on the architectures and on the nature\\nof the learning algorithms is developed, and an analysis and discus-\\nsions with concrete examples are presented. It is our hope that the\\nsurvey conducted in this monograph will provide insight for readers to\\nbetter understand the capability of the various deep learning systems\\ndiscussed in the monograph, the connection among diﬀerent but sim-\\nilar deep learning methods, and how to design proper deep learning\\nalgorithms under diﬀerent circumstances.\\nThroughout this review, the important message is conveyed that\\nbuilding and learning deep hierarchies of features are highly desirable.\\nWe have discussed the diﬃculty of learning parameters in all layers of\\ndeep networks in one shot due to optimization diﬃculties that need\\nto be better understood. The unsupervised pre-training method in the\\nhybrid architecture of the DBN–DNN, which we reviewed in detail in\\nSection 5, appears to have oﬀered a useful, albeit empirical, solution\\nto poor local optima in optimization and to regularization for the deep\\nmodel containing massive parameters even though a solid theoretical\\nfoundation is still lacking. The eﬀectiveness of the pre-training method,\\nwhich was one factor that stimulated the interest in deep learning by\\nthe signal processing community in 2009 via collaborations between\\nacademic and industrial researchers, is most prominent when the super-\\nvised training data are limited.\\nDeep learning is an emerging technology. Despite the empirical\\npromising results reported so far, much more work needs to be carried', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7beb5763-d55f-4615-97e9-8dc4985abd36', embedding=None, metadata={'page_label': '345', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='345\\nout. Importantly, it has not been the experience of deep learning\\nresearchers that a single deep learning technique can be successful for\\nall classiﬁcation tasks. For example, while the popular learning strat-\\negy of generative pre-training followed by discriminative ﬁne-tuning\\nseems to work well empirically for many tasks, it failed to work for\\nsome other tasks that have been explored (e.g., language identiﬁca-\\ntion or speaker recognition; unpublished). For these tasks, the features\\nextracted at the generative pre-training phase seem to describe the\\nunderlying speech variations well but do not contain suﬃcient infor-\\nmation to distinguish between diﬀerent languages. A learning strategy\\nthat can extract discriminative yet also invariant features is expected to\\nprovide better solutions. This idea has also been called “disentangling”\\nand is developed further in [24]. Further, extracting discriminative fea-\\ntures may greatly reduce the model size needed in many of the current\\ndeep learning systems. Domain knowledge such as what kind of invari-\\nance is useful for a speciﬁc task in hand (e.g., vision, speech, or natural\\nlanguage) and what kind of regularization in terms of parameter con-\\nstraints is key to the success of applying deep learning methods. More-\\nover, new types of DNN architectures and learning beyond the several\\npopular ones discussed in this monograph are currently under active\\ndevelopment by the deep learning research community (e.g., [24, 89]),\\nholding the promise to improve the performance of deep learning mod-\\nels in more challenging applications in signal processing and in artiﬁcial\\nintelligence.\\nRecent published work s howed that there is vast room to improve\\nthe current optimization techniques for learning deep architectures\\n[69, 208, 238, 239, 311, 356, 393]. To what extent pre-training is essen-\\ntial to learning the full set of parameters in deep architectures is\\ncurrently under investigation, especially when very large amounts of\\nlabeled training data are available, reducing or even obliterating the\\nneed for model regularization. Some preliminary results have been dis-\\ncussed in this monograph and in [55, 161, 323, 429].\\nIn recent years, machine learning is becoming increasingly depen-\\ndent on large-scale data sets. For instance, many of the recent successes\\nof deep learning as discussed in this monograph have relied on the access', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7baa3b18-b931-4f59-9372-6679ecfc261e', embedding=None, metadata={'page_label': '346', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='346 Conclusion\\nto massive data sets and massive computing power. It would become\\nincreasingly diﬃcult to explore the new algorithmic space without the\\naccess to large, real-world data sets and without the related engineer-\\ning expertise. How well deep learning algorithms behave would depend\\nheavily on the amount of data and computing power available. As we\\nshowed with speech recognition examples, a deep learning algorithm\\nthat appears to be performing not so remarkably on small data sets\\ncan begin to perform considerably better when these limitations are\\nremoved, one of main reasons for the recent resurgence in neural net-\\nwork research. As an example, the DBN pre-training that ignited a new\\nera of (deep) machine learning research appears unnecessary if enough\\ndata and computing power are used.\\nAs a consequence, eﬀective and scalable parallel algorithms are\\ncritical for training deep models with large data sets, as in many com-\\nmon information processing applications such as speech recognition\\nand machine translation. The popular mini-batch stochastic gradient\\ntechnique is known to be diﬃcult to parallelize over computers.\\nThe common practice nowadays is to use GPGPUs to speed up the\\nlearning process, although recent advance in developing asynchronous\\nstochastic gradient descent learning has shown promises by using\\nlarge-scale CPU clusters [69, 209] and GPU clusters [59]. In this\\ninteresting computing architecture, many diﬀerent replicas of the DNN\\ncompute gradients on diﬀerent subsets of the training data in parallel.\\nThese gradients are communicated to a central parameter server\\nthat updates the shared weights. Even though each replica typically\\ncomputes gradients using parameter values not immediately updated,\\nstochastic gradient descent is robust to the slight errors this has\\nintroduced. To make deep learning techniques scalable to very large\\ntraining data, theoretically sound parallel learning and optimization\\nalgorithms together with novel architectures need to be further devel-\\noped [31, 39, 49, 69, 181, 322, 356]. Optimization methods speciﬁc to\\nspeech recognition problems may need to be taken into account in order\\nto push speech recognition advances to the next level [46, 149, 393].\\nOne major barrier to the application of DNNs and related deep\\nmodels is that it currently requires considerable skill and experience to', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d5576acd-123a-442d-b21b-7f47c8567fc6', embedding=None, metadata={'page_label': '347', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='347\\nchoose sensible values for hyper-parameters such as the learning rate\\nschedule, the strength of the regularizer, the number of layers and the\\nnumber of units per layer, etc. Sensible values for one hyper-parameter\\nmay depend on the values chosen for other hyper-parameters and\\nhyper-parameter tuning in DNNs is especially expensive. Some inter-\\nesting methods for solving the problem have been developed recently,\\nincluding random sampling [32] and Bayesian optimization procedure\\n[337]. Further research is needed in this important area.\\nThis monograph, mainly in Sections 8 and 11 on natural language\\nand multi-modal applications, has touched on some recent work on\\nusing deep learning methods to do reasoning, moving beyond the topic\\nof more straightforward pattern recognition using supervised, unsuper-\\nvised or hybrid learning methods to which much of this monograph\\nhas been devoted to. In principle, since deep networks are naturally\\nequipped with distributed representations (rf. Table 3.1) using their\\nlayer-wise collections of units for coding relations and coding entities,\\nconcepts, events, topics, etc., they can potentially perform powerful\\nreasoning over structures, as argued in various historical publications\\nas well as recent essays [38, 156, 286, 288, 292, 336, 335]. While initial\\nexplorations on this capability of deep networks have recently appeared\\nin the literature, as reviewed in Sections 8 and 11, much research is\\nneeded. If successful, this new type of deep learning “machine” will\\nopen up many novel and exciting applications in applied artiﬁcial intel-\\nligence as a “thinking brain. ” We expect growing work of deep learning\\nin this area, full of new challenges, in the future.\\nFurther, solid theoretical foundations of deep learning need to be\\nestablished in a myriad of aspects. As an example, the success of deep\\nlearning in unsupervised learning has not been demonstrated as much\\nas for supervised learning; yet the essence and major motivation of deep\\nlearning lie right in unsupervised learning for automatically discover-\\ning data representation. The issues involve appropriate objectives for\\nlearning eﬀective feature representations and the right deep learning\\narchitectures/algorithms for distributed representations to eﬀectively\\ndisentangle the hidden explanatory factors of variation in the data.\\nUnfortunately, a majority of the successful deep learning techniques', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dcb232f8-7762-4ad7-b2a3-de23f61a750c', embedding=None, metadata={'page_label': '348', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='348 Conclusion\\nhave so far dealt with unstructured or “ﬂat” classiﬁcation problems.\\nFor example, although speech recognition is a sequential classiﬁcation\\nproblem by nature, in the most successful and large-scale systems, a\\nseparate HMM is used to handle the sequence structure and the DNN\\nis only used to produce the frame-level, unstructured posterior dis-\\ntributions. Recent proposals have called for and investigated moving\\nbeyond the “ﬂat” representations and incorporating structures in both\\nthe deep learning architectures and input and output representations\\n[79, 136, 338, 349].\\nFinally, deep learning researchers have been advised by neuroscien-\\ntists to seriously consider a broader set of issues and learning architec-\\ntures so as to gain insight into biologically plausible representations in\\nthe brain that may be useful for practical applications [272]. How can\\ncomputational neuroscience models about hierarchical brain structure\\nhelp improve engineering deep learning architectures? How may the\\nbiologically feasible learning styles in the brain [158, 395] help design\\nmore eﬀective and more robust deep learning algorithms? All these\\nissues and those discussed earlier in this section will need intensive\\nresearch in order to further push the frontier of deep learning.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='55e0575c-f838-4311-9fee-2d084441dbb7', embedding=None, metadata={'page_label': '349', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References\\n[1] O. Abdel-Hamid, L. Deng, and D. Yu. Exploring convolutional neural\\nnetwork structures and optimization for speech recognition. Proceedings\\nof Interspeech, 2013.\\n[ 2 ] O .A b d e l - H a m i d ,L .D e n g ,D .Y u ,and H. Jiang. Deep segmental neural\\nnetworks for speech recognition. In Proceedings of Interspeech. 2013.\\n[3] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn. Applying convo-\\nlutional neural networks concepts to hybrid NN-HMM model for speech\\nrecognition. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n[4] A. Acero, L. Deng, T. Kristjansson, and J. Zhang. HMM adaptation\\nusing vector taylor series fo r noisy speech recognition. In Proceedings\\nof Interspeech. 2000.\\n[5] G. Alain and Y. Bengio. What regularized autoencoders learn from the\\ndata generating distribution. In Proceedings of International Conference\\non Learning Representations (ICLR). 2013.\\n[6] G. Anthes. Deep learning comes of age. Communications of the Asso-\\nciation for Computing Machinery (ACM) , 56(6):13–15, June 2013.\\n[7] I. Arel, C. Rose, and T. Karnowski. Deep machine learning — a new\\nfrontier in artiﬁcial intelligence. IEEE Computational Intelligence Mag-\\nazine, 5:13–18, November 2010.\\n349', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0e6e8db2-c01f-4368-b5b9-b071d197eea9', embedding=None, metadata={'page_label': '350', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='350 References\\n[8] E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. Deep neural\\nnetwork language models. In Proceedings of the Joint Human Language\\nTechnology Conference and the North American Chapter of the Associ-\\nation of Computational Linguistics (HLT-NAACL) Workshop . 2012.\\n[9] O. Aslan, H. Cheng, D. Schuurmans, and X. Zhang. Convex two-layer\\nmodeling. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2013.\\n[10] J. Ba and B. Frey. Adaptive dropout for training deep neural networks.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2013.\\n[11] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O’Shaughnessy. Research developments and directions in speech\\nrecognition and understanding. IEEE Signal Processing Magazine ,\\n26(3):75–80, May 2009.\\n[12] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O’Shaughnessy. Updated MINS report on speech recognition and\\nunderstanding. IEEE Signal Processing Magazine, 26(4), July 2009.\\n[13] P. Baldi and P. Sadowski. Understanding dropout. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2013.\\n[14] E. Battenberg, E. Schmidt, and J. Bello. Deep learning for\\nmusic, special session at International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP) (http://www.icassp2014.org/\\nspecial_sections.html#ss8), 2014.\\n[15] E. Batternberg and D. Wessel. Analyzing drum patterns using condi-\\ntional deep belief networks. In Proceedings of International Symposium\\non Music Information Retrieval (ISMIR) . 2012.\\n[16] P. Bell, P. Swietojanski, and S. Renals. Multi-level adaptive networks\\nin tandem and hybrid ASR systems. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[17] Y. Bengio. Artiﬁcial neural networks and their application to sequence\\nrecognition. Ph.D. Thesis, McGill University, Montreal, Canada, 1991.\\n[18] Y. Bengio. New distributed probabilistic language models. Technical\\nReport, University of Montreal, 2002.\\n[19] Y. Bengio. Neural net language models. Scholarpedia, 3, 2008.\\n[20] Y. Bengio. Learning d eep architectures for AI. in Foundations and\\nTrends in Machine Learning, 2(1):1–127, 2009.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='10e13cb0-3ad7-4de3-b864-6806150ac9a1', embedding=None, metadata={'page_label': '351', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 351\\n[21] Y. Bengio. Deep learning of representations for unsupervised and trans-\\nfer learning. Journal of Machine Learning Research Workshop and Con-\\nference Proceedings, 27:17–37, 2012.\\n[22] Y. Bengio. Deep learning of representations: Looking forward. In Sta-\\ntistical Language and Speech Processing, pages 1–37. Springer, 2013.\\n[23] Y. Bengio, N. Boulanger, and R. Pascanu. Advances in optimizing recur-\\nrent networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n[24] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\\nreview and new perspectives. IEEE Transactions on Pattern Analysis\\nand Machine Intelligence (PAMI) , 38:1798–1828, 2013.\\n[25] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimiza-\\ntion of a neural network-hidden markov model hybrid. IEEE Transac-\\ntions on Neural Networks , 3:252–259, 1992.\\n[26] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. InProceedings of Neural Information Processing\\nSystems (NIPS). 2000.\\n[27] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. Journal of Machine Learning Research, 3:1137–\\n1155, 2003.\\n[28] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\\nwise training of deep networks. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2006.\\n[29] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\\ncies with gradient descent is diﬃcult. IEEE Transactions on Neural\\nNetworks, 5:157–166, 1994.\\n[30] Y. Bengio, E. Thibodeau-Laufer, and J. Yosinski. Deep generative\\nstochastic networks trainable by backprop. arXiv 1306:1091, 2013.\\nalso accepted to appear in Proceedings of International Conference on\\nMachine Learning (ICML), 2014.\\n[31] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized denoising\\nautoencoders as generative models. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2013.\\n[32] J. Bergstra and Y. Bengio. Random search for hyper-parameter opti-\\nmization. Journal on Machine Learning Research, 3:281–305, 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='761c4d41-e01d-49c8-a083-9f22946332de', embedding=None, metadata={'page_label': '352', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='352 References\\n[33] A. Biem, S. Katagiri, E. McDermott, and B. Juang. An application\\nof discriminative feature extractio n to ﬁlter-bank-based speech recog-\\nnition. IEEE Transactions on Speech and Audio Processing , 9:96–110,\\n2001.\\n[34] J. Bilmes. Dynamic graphical models. IEEE Signal Processing Maga-\\nzine, 33:29–42, 2010.\\n[35] J. Bilmes and C. Bartels. Graphi cal model architectures for speech\\nrecognition. IEEE Signal Processing Magazine, 22:89–100, 2005.\\n[36] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching\\nenergy function for learning with multi-relational data — application\\nto word-sense disambiguation.Machine Learning, May 2013.\\n[37] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured\\nembeddings of knowledge bases. In Proceedings of Association for the\\nAdvancement of Artiﬁcial Intelligence (AAAI) . 2011.\\n[38] L. Bottou. From machine learning to machine reasoning: An essay.\\nJournal of Machine Learning Research, 14:3207–3260, 2013.\\n[39] L. Bottou and Y. LeCun. Large scale online learning. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2004.\\n[40] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling\\nTemporal dependencies in high-dime nsional sequences: Application to\\npolyphonic music generation and transcription. In Proceedings of Inter-\\nnational Conference on Machine Learning (ICML) . 2012.\\n[41] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord\\nrecognition with recurrent neural networks. In Proceedings of Interna-\\ntional Symposium on Music Information Retrieval (ISMIR) . 2013.\\n[42] H. Bourlard and N. Morgan. Connectionist Speech Recognition: A\\nHybrid Approach. Kluwer, Norwell, MA, 1993.\\n[43] J. Bouvrie. Hierarchical learning: Theory with applications in speech\\nand vision. Ph.D. thesis, MIT, 2009.\\n[44] L. Breiman. Stacked regression. Machine Learning, 24:49–64, 1996.\\n[45] J. Bridle, L. Deng, J. Picone, H. Richards, J. Ma, T. Kamm, M. Schus-\\nter, S. Pike, and R. Reagan. An investigation of segmental hidden\\ndynamic models of speech coarticulation for automatic speech recogni-\\ntion. Final Report for 1998 Workshop on Language Engineering, CLSP,\\nJohns Hopkins, 1998.\\n[46] P. Cardinal, P. Dumouchel, and G. Boulianne. Large vocabulary speech\\nrecognition on parallel architectures. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(11):2290–2300, November 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='24059c11-4cf4-444a-a323-353c68822e43', embedding=None, metadata={'page_label': '353', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 353\\n[47] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.\\n[48] J. Chen and L. Deng. A primal-dual method for training recurrent\\nneural networks constrained by the echo-state property. In Proceedings\\nof International Conference on Learning Representations. April 2014.\\n[ 4 9 ]X .C h e n ,A .E v e r s o l e ,G .L i ,D .Y u ,a n dF .S e i d e . P i p e l i n e db a c k -\\npropagation for context-dependent deep neural networks. InProceedings\\nof Interspeech. 2012.\\n[50] R. Chengalvarayan and L. Deng. Hmm-based speech recognition using\\nstate-dependent, discriminatively derived transforms on Mel-warped\\nDFT features. IEEE Transactions on Speech and Audio Processing ,\\npages 243–256, 1997.\\n[51] R. Chengalvarayan and L. Deng. Use of generalized dynamic feature\\nparameters for speech recognition. IEEE Transactions on Speech and\\nAudio Processing, pages 232–242, 1997a.\\n[52] R. Chengalvarayan and L. Deng. Speech trajectory discrimination using\\nthe minimum classiﬁcation error learning. IEEE Transactions on Speech\\nand Audio Processing, 6(6):505–515, 1998.\\n[53] Y. Cho and L. Saul. Kernel methods for deep learning. In Proceedings of\\nNeural Information Processing Systems (NIPS), pages 342–350. 2009.\\n[54] D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber. Deep neural\\nnetworks segment neuronal membranes in electron microscopy images.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2012.\\n[55] D. Ciresan, U. Meier, L. Gambardella, and J. Schmidhuber. Deep, big,\\nsimple neural nets for handwritten digit recognition. Neural Computa-\\ntion, December 2010.\\n[56] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. A committee of\\nneural networks for traﬃc sign classiﬁcation. In Proceedings of Interna-\\ntional Joint Conference on Neural Networks (IJCNN) . 2011.\\n[57] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural\\nnetworks for image classiﬁcation. In Proceedings of Computer Vision\\nand Pattern Recognition (CVPR). 2012.\\n[58] D. C. Ciresan, U. Meier, and J. Schmidhuber. Transfer learning for Latin\\nand Chinese characters with deep neural networks. In Proceedings of\\nInternational Joint Conference on Neural Networks (IJCNN) . 2012.\\n[59] A. Coates, B. Huval, T. Wang, D. Wu, A. Ng, and B. Catanzaro. Deep\\nlearning with COTS HPC. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9c91a3f6-b9ed-464d-b084-8aa97add336c', embedding=None, metadata={'page_label': '354', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='354 References\\n[60] W. Cohen and R. V. de Carvalho. Stacked sequential learning. In\\nProceedings of International Joint Conference on Artiﬁcial Intelligence\\n(IJCAI), pages 671–676. 2005.\\n[61] R. Collobert. Deep learning for eﬃcient discriminative parsing. In\\nProceedings of Artiﬁcial Intelligence and Statistics (AISTATS). 2011.\\n[62] R. Collobert and J. Weston. A uniﬁed architecture for natural language\\nprocessing: Deep neural networks with multitask learning. In Proceed-\\nings of International Conference on Machine Learning (ICML) . 2008.\\n[63] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa. Natural language processing (almost) from scratch. Journal\\non Machine Learning Research, 12:2493–2537, 2011.\\n[64] G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition\\nwith the mean-covariance restricted boltzmann machine. In Proceedings\\nof Neural Information Processing Systems (NIPS) , volume 23, pages\\n469–477. 2010.\\n[65] G. Dahl, T. Sainath, and G. Hinton. Improving deep neural networks\\nfor LVCSR using rectiﬁed linear units and dropout. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n[66] G. Dahl, J. Stokes, L. Deng, and D. Yu. Large-scale malware classiﬁ-\\ncation using random project ions and neural networks. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n[67] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent DBN-\\nHMMs in large vocabulary continuous speech recognition. In Proceed-\\nings of International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP). 2011.\\n[68] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent, pre-trained\\ndeep neural networks for large vocabulary speech recognition. IEEE\\nTransactions on Audio, Speech, & Language Processing , 20(1):30–42,\\nJanuary 2012.\\n[69] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,\\nM. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale\\ndistributed deep networks. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n[70] K. Demuynck and F. Triefenbach. Porting concepts from DNNs back\\nto GMMs. In Proceedings of the Automatic Speech Recognition and\\nUnderstanding Workshop (ASRU). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='76566c2e-73f7-4945-b9c4-cd5a040c6ae6', embedding=None, metadata={'page_label': '355', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 355\\n[71] L. Deng. A generalized hidden Markov model with state-conditioned\\ntrend functions of time for the speech signal. Signal Processing ,\\n27(1):65–78, 1992.\\n[72] L. Deng. A stochastic model of speech incorporating hierarchical nonsta-\\ntionarity. IEEE Transactions on Speech and Audio Processing, 1(4):471–\\n475, 1993.\\n[73] L. Deng. A dynamic, feature-based approach to the interface between\\nphonology and phonetics for speech modeling and recognition. Speech\\nCommunication, 24(4):299–323, 1998.\\n[74] L. Deng. Computational models for speech production. In Compu-\\ntational Models of Speech Pattern Processing , pages 199–213. Springer\\nVerlag, 1999.\\n[75] L. Deng. Switching dynamic system models for speech articulation and\\nacoustics. In Mathematical Foundations of Speech and Language Pro-\\ncessing, pages 115–134. Springer-Verlag, New York, 2003.\\n[76] L. Deng. Dynamic Speech Models — Theory, Algorithm, and Applica-\\ntion. Morgan & Claypool, December 2006.\\n[77] L. Deng. An overview of deep-structured learning for information pro-\\ncessing. In Proceedings of Asian-Paciﬁc Signal & Information Process-\\ning Annual Summit and Conference (APSIPA-ASC) . October 2011.\\n[78] L. Deng. The MNIST database of handwritten digit images for machine\\nlearning research. IEEE Signal Processing Magazine, 29(6), November\\n2012.\\n[79] L. Deng. Design and learning of output representations for speech recog-\\nnition. In Neural Information Processing Systems (NIPS) Workshop on\\nLearning Output Representations. December 2013.\\n[80] L. Deng. A tutorial survey of architectures, algorithms, and applications\\nfor deep learning. In Asian-Paciﬁc Signal & Information Processing\\nAssociation Transactions on Signal and Information Processing. 2013.\\n[81] L. Deng, O. Abdel-Hamid, and D. Yu. A deep convolutional neural\\nnetwork using heterogeneous pooling for trading acoustic invariance\\nwith phonetic confusion. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[82] L. Deng, A. Acero, L. Jiang, J. Droppo, and X. Huang. High perfor-\\nmance robust speech recognition using stereo training data. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2001.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b70b38bd-db23-4821-a59d-4771a0da9df1', embedding=None, metadata={'page_label': '356', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='356 References\\n[83] L. Deng and M. Aksmanovic. Speaker-independent phonetic classiﬁ-\\ncation using hidden markov models with state-conditioned mixtures of\\ntrend functions. IEEE Transactions on Speech and Audio Processing ,\\n5:319–324, 1997.\\n[84] L. Deng, M. Aksmanovic, D. Sun, and J. Wu. Speech recognition using\\nhidden Markov models with polynomial regression functions as nonsta-\\ntionary states. IEEE Transactions on Speech and Audio Processing ,\\n2(4):507–520, 1994.\\n[85] L. Deng and J. Chen. Sequence classiﬁcation using the high-level fea-\\ntures extracted from d eep neural networks. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n[86] L. Deng and K. Erler. Structural design of a hidden Markov model\\nbased speech recognizer using multi-valued phonetic features: Compar-\\nison with segmental speech units. Journal of the Acoustical Society of\\nAmerica, 92(6):3058–3067, 1992.\\n[87] L. Deng, K. Hassanein, and M. Elmasry. Analysis of correlation struc-\\nture for a neural predictive model with application to speech recognition.\\nNeural Networks, 7(2):331–339, 1994.\\n[88] L. Deng, X. He, and J. Gao. Deep stacking networks for informa-\\ntion retrieval. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013c.\\n[89] L. Deng, G. Hinton, and B. Kingsbury. New types of deep neural\\nnetwork learning for speech recognition and related applications: An\\noverview. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013b.\\n[90] L. Deng and X. D. Huang. Challenges in adopting speech recognition.\\nCommunications of the Association for Computing Machinery (ACM) ,\\n47(1):11–13, January 2004.\\n[91] L. Deng, B. Hutchinson, and D. Yu. Parallel training of deep stacking\\nnetworks. In Proceedings of Interspeech. 2012b.\\n[92] L. Deng, M. Lennig, V. Gupta, F. Seitz, P. Mermelstein, and P. Kenny.\\nPhonemic hidden Markov models with continuous mixture output den-\\nsities for large vocabulary word recognition. IEEE Transactions on\\nSignal Processing, 39(7):1677–1681, 1991.\\n[93] L. Deng, M. Lennig, F. Seitz, and P. Mermelstein. Large vocabulary\\nword recognition using context-dependent allophonic hidden Markov\\nmodels.Computer Speech and Language, 4(4):345–357, 1990.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1cfc2f9d-67ee-4100-ab68-5eeea742406b', embedding=None, metadata={'page_label': '357', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 357\\n[94] L. Deng, J. Li, K. Huang, Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig,\\nX. He, J. Williams, Y. Gong, and A. Acero. Recent advances in deep\\nlearning for speech research at Microsoft. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013a.\\n[95] L. Deng and X. Li. Machine learning paradigms in speech recogni-\\ntion: An overview. IEEE Transactions on Audio, Speech, & Language ,\\n21:1060–1089, May 2013.\\n[96] L. Deng and J. Ma. Spontaneous speech recognition using a statistical\\ncoarticulatory model for the vocal tract resonance dynamics. Journal\\nof the Acoustical Society America, 108:3036–3048, 2000.\\n[97] L. Deng and D. O’Shaughnessy. Speech Processing — A Dynamic and\\nOptimization-Oriented Approach. Marcel Dekker, 2003.\\n[98] L. Deng, G. Ramsay, and D. Sun. Production models as a structural\\nbasis for automatic speech recognition. Speech Communication, 33(2–\\n3):93–111, August 1997.\\n[99] L. Deng and H. Sameti. Transitional speech units and their represen-\\ntation by regressive Markov states: Applications to speech recognition.\\nIEEE Transactions on speech and audio processing, 4(4):301–306, July\\n1996.\\n[100] L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton.\\nBinary coding of speech spectrogr ams using a deep autoencoder. In\\nProceedings of Interspeech. 2010.\\n[101] L. Deng and D. Sun. A statistical approach to automatic speech\\nrecognition using the atomic speech units constructed from overlap-\\nping articulatory features.Journal of the Acoustical Society of America,\\n85(5):2702–2719, 1994.\\n[102] L. Deng, G. Tur, X. He, and D. Hakkani-Tur. Use of kernel deep convex\\nnetworks and end-to-end learning for spoken language understanding.\\nIn Proceedings of IEEE Workshop on Spoken Language Technologies .\\nDecember 2012.\\n[103] L. Deng, K. Wang, A. Acero, H. W. Hon, J. Droppo, C. Boulis, Y. Wang,\\nD. Jacoby, M. Mahajan, C. Chelba, and X. Huang. Distributed speech\\nprocessing in mipad’s multimodal user interface. IEEE Transactions on\\nSpeech and Audio Processing, 10(8):605–619, 2002.\\n[104] L. Deng, J. Wu, J. Droppo, and A. Acero. Dynamic compensation of\\nHMM variances using the feature enhancement uncertainty computed\\nfrom a parametric model of speech distortion.IEEE Transactions on\\nSpeech and Audio Processing, 13(3):412–421, 2005.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dd9f2b51-97b0-40e8-9fb1-aa34c9edabeb', embedding=None, metadata={'page_label': '358', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='358 References\\n[105] L. Deng and D. Yu. Use of diﬀerential cepstra as acoustic features\\nin hidden trajectory modeling for phonetic recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2007.\\n[106] L. Deng and D. Yu. Deep convex network: A scalable architecture for\\nspeech pattern classiﬁcation. In Proceedings of Interspeech. 2011.\\n[107] L. Deng, D. Yu, and A. Acero. A bidirectional target ﬁltering model of\\nspeech coarticulation: Two-stage implementation for phonetic recogni-\\ntion. IEEE Transactions on Audio and Speech Processing , 14(1):256–\\n265, January 2006.\\n[108] L. Deng, D. Yu, and A. Acero. Structured speech modeling. IEEE\\nTransactions on Audio, Speech and Language Processing , 14(5):1492–\\n1504, September 2006.\\n[109] L. Deng, D. Yu, and G. Hinton. Deep learning for speech recognition and\\nrelated applications. Neural Information Processing Systems (NIPS)\\nWorkshop, 2009.\\n[110] L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for build-\\ning deep architectures. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012a.\\n[111] T. Deselaers, S. Hasan, O. Bender, and H. Ney. A deep learning\\napproach to machine transliteration. In Proceedings of 4th Workshop on\\nStatistical Machine Translation, pages 233–241. Athens, Greece, March\\n2009.\\n[112] A. Diez. Automatic language recognition using deep neural networks.\\nThesis, Universidad Autonoma de Madrid, SPAIN, September 2013.\\n[113] P. Dognin and V. Goel. Combining stochastic average gradient and\\nhessian-free optimization for sequence training of deep neural networks.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 2013.\\n[114] D. Erhan, Y. Bengio, A. Courvelle, P. Manzagol, P. Vencent, and S. Ben-\\ngio. Why does unsupervised pre-training help deep learning? Journal\\non Machine Learning Research, pages 201–208, 2010.\\n[115] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory. F0 contour\\nprediction with a deep belief network-gaussian process hybrid model. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 6885–6889. 2013.\\n[116] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden Markov\\nmodel: Analysis and applications. Machine Learning, 32:41–62, 1998.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5de72ba8-54d6-4041-9058-3928f0401e66', embedding=None, metadata={'page_label': '359', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 359\\n[117] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and\\nT. Mikolov. Devise: A deep visual-semantic embedding model. In Pro-\\nceedings of Neural Information Processing Systems (NIPS). 2013.\\n[118] Q. Fu, X. He, and L. Deng. Phone-discriminating minimum classiﬁca-\\ntion error (p-mce) training for phonetic recognition. In Proceedings of\\nInterspeech. 2007.\\n[119] M. Gales. Model-based approaches to handling uncertainty. In Robust\\nSpeech Recognition of Uncertain or Missing Data: Theory and Applica-\\ntion, pages 101–125. Springer, 2011.\\n[120] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based translation models\\nfor web search: From word models to phrase models. In Proceedings of\\nConference on Information and Knowledge Management (CIKM). 2010.\\n[121] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS) Workshop on Deep Learning. December\\n2013.\\n[122] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model. MSR-TR-2013-88, September 2013.\\n[123] J. Gao, X. He, W. Yih, and L. Deng. Learning continuous phrase rep-\\nresentations for translation modeling. In Proceedings of Association for\\nComputational Linguistics (ACL). 2014.\\n[124] J. Gao, K. Toutanova, and W.-T. Yih. Clickthrough-based latent seman-\\ntic models for web search. In Proceedings of Special Interest Group on\\nInformation Retrieval (SIGIR). 2011.\\n[125] R. Gens and P. Domingo. Discriminative learning of sum-product net-\\nworks. Neural Information Processing Systems (NIPS), 2012.\\n[126] D. George. How the brain might work: A hierarchical and temporal\\nmodel for learning and recognition. Ph.D. thesis, Stanford University,\\n2008.\\n[127] M. Gibson and T. Hain. Error approximation and minimum phone error\\nacoustic model estimation. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1269–1279, August 2010.\\n[128] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\\nhierarchies for accurate object det ection and semantic segmentation.\\narXiv:1311.2524v1, 2013.\\n[129] X. Glorot and Y. Bengio. Understanding the diﬃculty of training deep\\nfeed-forward neural networks. In Proceedings of Artiﬁcial Intelligence\\nand Statistics (AISTATS). 2010.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e93b21ac-4c6f-4491-b018-567477218fc4', embedding=None, metadata={'page_label': '360', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='360 References\\n[130] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural\\nnetworks. In Proceedings of Artiﬁcial Intelligence and Statistics (AIS-\\nTATS). April 2011.\\n[131] I. Goodfellow, M. Mirza, A. Courv ille, and Y. Bengio. Multi-prediction\\ndeep boltzmann machines. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2013.\\n[132] E. Grais, M. Sen, and H. Erdogan. Deep neural networks for single\\nchannel source separation. arXiv:1311.2746v1, 2013.\\n[133] A. Graves. Sequence transduction with recurrent neural networks. Rep-\\nresentation Learning Workshop, International Conference on Machine\\nLearning (ICML), 2012.\\n[134] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connection-\\nist temporal classiﬁcation: Labelin g unsegmented sequence data with\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2006.\\n[135] A. Graves, N. Jaitly, and A. Mohamed. Hybrid speech recognition with\\ndeep bidirectional LSTM. InProceedings of the Automatic Speech Recog-\\nnition and Understanding Workshop (ASRU) . 2013.\\n[136] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[137] F. Grezl and P. Fousek. Optimizing bottle-neck features for LVCSR. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2008.\\n[138] C. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio. Learned-\\nnorm pooling for deep feedforward and recurrent neural networks.\\nhttp://arxiv.org/abs/1311.1780, 2014.\\n[139] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation of unnor-\\nmalized statistical models, with applications to natural image statistics.\\nJournal of Machine Learning Research, 13:307–361, 2012.\\n[140] T. Hain, L. Burget, J. Dines, P. Garner, F. Grezl, A. Hannani, M. Hui-\\njbregts, M. Karaﬁat, M. Lincoln, and V. Wan. Transcribing meetings\\nwith the AMIDA systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 20:486–498, 2012.\\n[141] P. Hamel and D. Eck. Learning features from music audio with deep\\nbelief networks. In Proceedings of International Symposium on Music\\nInformation Retrieval (ISMIR). 2010.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d51dc062-00b9-4bab-8cae-2b4a99f9c1e7', embedding=None, metadata={'page_label': '361', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 361\\n[142] G. Hawkins, S. Ahmad, and D. Dubinsky. Hierarchical temporal mem-\\nory including HTM cortical learning algorithms. Numenta Technical\\nReport, December 10 2010.\\n[143] J. Hawkins and S. Blakeslee. On Intelligence: How a New Understanding\\nof the Brain will lead to the Creation of Truly Intelligent Machines .\\nTimes Books, New York, 2004.\\n[144] X. He and L. Deng. Speech recognition, machine translation, and speech\\ntranslation — a unifying discriminative framework. IEEE Signal Pro-\\ncessing Magazine, 28, November 2011.\\n[145] X. He and L. Deng. Optimization in speech-centric information process-\\ning: Criteria and techniques. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[146] X. He and L. Deng. Speech-centric information processing: An\\noptimization-oriented approach. In Proceedings of the IEEE. 2013.\\n[147] X. He, L. Deng, and W. Chou. Discriminative learning in sequential pat-\\ntern recognition — a unifying review f or optimization-oriented speech\\nrecognition. IEEE Signal Processing Magazine, 25:14–36, 2008.\\n[148] G. Heigold, H. Ney, P. Lehnen, T. Gass, and R. Schluter. Equivalence of\\ngenerative and log-liner models. IEEE Transactions on Audio, Speech,\\nand Language Processing, 19(5):1138–1148, February 2011.\\n[149] G. Heigold, H. Ney, and R. Schluter. Investigations on an EM-style opti-\\nmization algorithm for discriminative training of HMMs. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(12):2616–2626,\\nDecember 2013.\\n[150] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. Ranzato, M. Devin,\\nand J. Dean. Multilingual acoustic models using distributed deep neu-\\nral networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n[151] I. Heintz, E. Fosler-Lussier, and C. Brew. Discriminative input stream\\ncombination for conditional random ﬁeld phone recognition. IEEE\\nTransactions on Audio, Speech, and Language Processing , 17(8):1533–\\n1546, November 2009.\\n[152] M. Henderson, B. Thomson, and S. Young. Deep neural network\\napproach for the dialog state tracking challenge. In Proceedings of Spe-\\ncial Interest Group on Disclosure and Dialogue (SIGDIAL) . 2013.\\n[153] M. Hermans and B. Schrauwen. Training and analysing deep recur-\\nrent neural networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cc7b8e12-1149-4292-af64-e3574e93723f', embedding=None, metadata={'page_label': '362', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='362 References\\n[154] H. Hermansky, D. Ellis, and S. Sha rma. Tandem connectionist feature\\nextraction for conventional HMM systems. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2000.\\n[155] Y. Hifny and S. Renals. Speech recognition using augmented conditional\\nrandom ﬁelds. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 17(2):354–365, February 2009.\\n[156] G. Hinton. Mapping part-whole hierarchies into connectionist networks.\\nArtiﬁcial Intelligence, 46:47–75, 1990.\\n[157] G. Hinton. Preface to the special issue on connectionist symbol pro-\\ncessing. Artiﬁcial Intelligence, 46:1–4, 1990.\\n[158] G. Hinton. The ups and downs of Hebb synapses. Canadian Psychology,\\n44:10–13, 2003.\\n[159] G. Hinton. A practical guide to training restricted boltzmann machines.\\nUTML Tech Report 2010-003, Univ. Toronto, August 2010.\\n[160] G. Hinton. A better way to learn features. Communications of the\\nAssociation for Computing Machinery (ACM) , 54(10), October 2011.\\n[161] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,\\nV. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neu-\\nral networks for acoustic modeling in speech recognition. IEEE Signal\\nProcessing Magazine, 29(6):82–97, November 2012.\\n[162] G. Hinton, A. Krizhevsky, and S. Wang. Transforming autoencoders. In\\nProceedings of International Conference on Artiﬁcial Neural Networks.\\n2011.\\n[163] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep\\nbelief nets. Neural Computation, 18:1527–1554, 2006.\\n[164] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data\\nwith neural networks. Science, 313(5786):504–507, July 2006.\\n[165] G. Hinton and R. Salakhutdinov. Discovering binary codes for docu-\\nments by learning deep generative models. Topics in Cognitive Science,\\npages 1–18, 2010.\\n[166] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhut-\\ndinov. Improving neural networks by preventing co-adaptation of fea-\\nture detectors. arXiv: 1207.0580v1, 2012.\\n[167] S. Hochreiter. Untersuchungen zu dynamischen neuronalen net-\\nzen. Diploma thesis, Institut fur Informatik, Technische Universitat\\nMunchen, 1991.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4525f35d-00bb-4c58-8f00-38fa613113dc', embedding=None, metadata={'page_label': '363', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 363\\n[168] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\\nComputation, 9:1735–1780, 1997.\\n[169] E. Huang, R. Socher, C. Manning, and A. Ng. Improving word represen-\\ntations via global context and multiple word prototypes. In Proceedings\\nof Association for Computational Linguistics (ACL) . 2012.\\n[170] J. Huang, J. Li, L. Deng, and D. Yu. Cross-language knowledge transfer\\nusing multilingual deep neural networks with shared hidden layers. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n[171] P. Huang, L. Deng, M. Hasegawa-Johnson, and X. He. Random fea-\\ntures for kernel deep convex network. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[172] P. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning\\ndeep structured semantic models for web search using clickthrough data.\\nAssociation for Computing Machinery (ACM) International Conference\\nInformation and Knowledge Management (CIKM) , 2013.\\n[173] P. Huang, K. Kumar, C. Liu, Y. Gong, and L. Deng. Predicting speech\\nrecognition conﬁdence using deep learning with word identity and score\\nfeatures. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n[174] S. Huang and S. Renals. Hierarchical bayesian language models for\\nconversational speech recognition.IEEE Transactions on Audio, Speech,\\nand Language Processing, 18(8):1941–1954, November 2010.\\n[175] X. Huang, A. Acero, C. Chelba, L. Deng, J. Droppo, D. Duchene,\\nJ. Goodman, and H. Hon. Mipad: A multimodal interaction proto-\\ntype. InProceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2001.\\n[176] Y. Huang, D. Yu, Y. Gong, and C. Liu. Semi-supervised GMM and DNN\\nacoustic model training with multi-system combination and conﬁdence\\nre-calibration. In Proceedings of Interspeech, pages 2360–2364. 2013.\\n[177] E. Humphrey and J. Bello. Rethinking automatic chord recognition\\nwith convolutional neural networks. In Proceedings of International\\nConference on Machine Learning and Application (ICMLA) . 2012a.\\n[178] E. Humphrey, J. Bello, and Y. LeCun. Moving beyond feature design:\\nDeep architectures and automatic feature learning in music informat-\\nics. In Proceedings of International Symposium on Music Information\\nRetrieval (ISMIR). 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='707c83e6-3155-45fe-8177-2fa793bcb6d3', embedding=None, metadata={'page_label': '364', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='364 References\\n[179] E. Humphrey, J. Bello, and Y. LeCun. Feature learning and deep archi-\\ntectures: New directions for music informatics. Journal of Intelligent\\nInformation Systems, 2013.\\n[180] B. Hutchinson, L. Deng, and D. Yu. A deep architecture with bilinear\\nmodeling of hidden representations: Applications to phonetic recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n[181] B. Hutchinson, L. Deng, and D. Yu. Tensor deep stacking net-\\nworks. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence, 35:1944–1957, 2013.\\n[182] D. Imseng, P. Motlicek, P. Garner, and H. Bourlard. Impact of deep\\nMLP architecture on diﬀerent modeling techniques for under-resourced\\nspeech recognition. In Proceedings of the Automatic Speech Recognition\\nand Understanding Workshop (ASRU) . 2013.\\n[183] N. Jaitly and G. Hinton. Learning a better representation of speech\\nsound waves using restricted boltzmann machines. In Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n[184] N. Jaitly, P. Nguyen, and V. Vanhoucke. Application of pre-trained deep\\nneural networks to large vocabulary speech recognition. In Proceedings\\nof Interspeech. 2012.\\n[185] K. Jarrett, K. Kavukcuoglu, and Y. LeCun. What is the best multi-\\nstage architecture for object recognition? InProceedings of International\\nConference on Computer Vision, pages 2146–2153. 2009.\\n[186] H. Jiang and X. Li. Parameter estimation of statistical models using\\nconvex optimization: An advanced method of discriminative training\\nfor speech and language processing. IEEE Signal Processing Magazine,\\n27(3):115–127, 2010.\\n[187] B. Juang, S. Levinson, and M. Sondhi. Maximum likelihood estimation\\nfor multivariate mixture observations of Markov chains. IEEE Trans-\\nactions on Information Theory , 32:307–309, 1986.\\n[188] B.-H. Juang, W. Chou, and C.-H. Lee. Minimum classiﬁcation error\\nrate methods for speech recognition. IEEE Transactions On Speech\\nand Audio Processing, 5:257–265, 1997.\\n[189] S. Kahou et al. Combining modality speciﬁc deep neural networks for\\nemotion recognition in video. InProceedings of International Conference\\non Multimodal Interaction (ICMI). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b719ef79-f2d7-446c-9c08-5673ffd2a7a6', embedding=None, metadata={'page_label': '365', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 365\\n[190] S. Kang, X. Qian, and H. Meng. Multi-distribution deep belief network\\nfor speech synthesis. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP) , pages 8012–8016.\\n2013.\\n[191] Y. Kashiwagi, D. Saito, N. Minematsu, and K. Hirose. Discriminative\\npiecewise linear transformation based on deep learning for noise robust\\nautomatic speech recognition. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n[192] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu,\\nand Y. LeCun. Learning convolutional feature hierarchies for visual\\nrecognition. InProceedings of Neural Information Processing Systems\\n(NIPS). 2010.\\n[193] H. Ketabdar and H. Bourlard. Enhanced phone posteriors for improving\\nspeech recognition systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1094–1106, August 2010.\\n[194] B. Kingsbury. Lattice-based optimization of sequence classiﬁcation cri-\\nteria for neural-network acoustic modeling. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2009.\\n[195] B. Kingsbury, T. Sainath, and H. Soltau. Scalable minimum bayes\\nrisk training of deep neural network acoustic models using distributed\\nhessian-free optimization. In Proceedings of Interspeech. 2012.\\n[196] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neural lan-\\nguage models. In Proceedings of Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop. 2013.\\n[197] T. Ko and B. Mak. Eigentriphones for context-dependent acoustic mod-\\neling. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(6):1285–1294, 2013.\\n[198] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with\\ndeep convolutional neural networks. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2012.\\n[199] Y. Kubo, T. Hori, and A. Nakamura. Integrating deep neural networks\\ninto structural classiﬁcation approach based on weighted ﬁnite-state\\ntransducers. In Proceedings of Interspeech. 2012.\\n[200] R. Kurzweil. How to Create a Mind . Viking Books, December 2012.\\n[201] P. Lal and S. King. Cross-lingual automatic speech recognition using\\ntandem features. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(12):2506–2515, December 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='880e77aa-87c6-47db-8720-d7d83b396a44', embedding=None, metadata={'page_label': '366', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='366 References\\n[202] K. Lang, A. Waibel, and G. Hinton. A time-delay neural network archi-\\ntecture for isolated word recognition.Neural Networks, 3(1):23–43, 1990.\\n[203] H. Larochelle and Y. Bengio. Classiﬁcation using discriminative\\nrestricted boltzm ann machines. In Proceedings of International Con-\\nference on Machine Learning (ICML). 2008.\\n[204] D. Le and P. Mower. Emotion recognition from spontaneous speech\\nusing hidden markov models with deep belief networks. In Proceed-\\nings of the Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n[205] H. Le, A. Allauzen, G. Wisniewski, and F. Yvon. Training continuous\\nspace language models: Some practical issues. In Proceedings of Empiri-\\ncal Methods in Natural Language Processing (EMNLP), pages 778–788.\\n2010.\\n[206] H. Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. Structured\\noutput layer neural network language model. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2011.\\n[207] H. Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon. Struc-\\ntured output layer neural network language models for speech recogni-\\ntion.IEEE Transactions on Audio, Speech, and Language Processing ,\\n21(1):197–206, January 2013.\\n[208] Q. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Ng. On\\noptimization methods for deep learning. In Proceedings of International\\nConference on Machine Learning (ICML). 2011.\\n[209] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen, J. Dean,\\nand A. Ng. Building high-level features using large scale unsupervised\\nlearning. In Proceedings of International Conference on Machine Learn-\\ning (ICML). 2012.\\n[210] Y. LeCun. Learning invariant feature hierarchies. In Proceedings of\\nEuropean Conference on Computer Vision (ECCV). 2012.\\n[211] Y. LeCun and Y. Bengio. Convolutional networks for images, speech,\\nand time series. In M. Arbib, editor, The Handbook of Brain The-\\nory and Neural Networks, pages 255–258. MIT Press, Cambridge, Mas-\\nsachusetts, 1995.\\n[212] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learn-\\ning applied to document recognition. Proceedings of the IEEE, 86:2278–\\n2324, 1998.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0ba8ff09-fcfd-494b-a392-44540f181ab3', embedding=None, metadata={'page_label': '367', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 367\\n[213] Y. LeCun, S. Chopra, M. Ranzato, and F. Huang. Energy-based models\\nin document recognition and computer vision. In Proceedings of Inter-\\nnational Conference on Document Analysis and Recognition (ICDAR) .\\n2007.\\n[214] C.-H. Lee. From knowledge-ignorant to knowledge-rich modeling: A new\\nspeech research paradigm for next-g eneration automatic speech recog-\\nnition. In Proceedings of International Conference on Spoken Language\\nProcessing (ICSLP), pages 109–111. 2004.\\n[215] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief\\nnetworks for scalable unsupervised learning of hierarchical representa-\\ntions. InProceedings of International Conference on Machine Learning\\n(ICML). 2009.\\n[216] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Unsupervised learning\\nof hierarchical representations with convolutional deep belief networks.\\nCommunications of the Association for Computing Machinery (ACM) ,\\n54(10):95–103, October 2011.\\n[217] H. Lee, Y. Largman, P. Pham, and A. Ng. Unsupervised feature learning\\nfor audio classiﬁcation using convolutional deep belief networks. In\\nProceedings of Neural Information Processing Systems (NIPS). 2010.\\n[218] P. Lena, K. Nagata, and P. Baldi. Deep spatiotemporal architectures\\nand learning for protein structure prediction. In Proceedings of Neural\\nInformation Processing Systems (NIPS). 2012.\\n[219] S. Levine. Exploring deep and recurrent architectures for optimal con-\\ntrol. arXiv:1311.1761v1.\\n[220] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach. An overview of\\nnoise-robust automatic speech recognition. IEEE/Association for Com-\\nputing Machinery (ACM) Transactions on Audio, Speech, and Language\\nProcessing, pages 1–33, 2014.\\n[221] J. Li, D. Yu, J. Huang, and Y. Gong. Improving wideband speech\\nrecognition using mixed-bandwidth training data in CD-DNN-HMM.\\nIn Proceedings of IEEE Spoken Language Technology (SLT). 2012.\\n[222] L. Li, Y. Zhao, D. Jiang, and Y. Zhang etc. Hybrid deep neural network–\\nhidden markov model (DNN-HMM) based speech emotion recognition.\\nIn Proceedings Conference on Aﬀective Computing and Intelligent Inter-\\naction (ACII), pages 312–317. September 2013.\\n[223] H. Liao. Speaker adaptation of context dependent deep neural net-\\nworks. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='185dbd15-ae38-46f4-b4e7-6b5527620105', embedding=None, metadata={'page_label': '368', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='368 References\\n[224] H. Liao, E. McDermott, and A. Senior. Large scale deep neural network\\nacoustic modeling with semi-supervised training data for youtube video\\ntranscription. In Proceedings of the Automatic Speech Recognition and\\nUnderstanding Workshop (ASRU). 2013.\\n[225] H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C.-H. Lee. A study on\\nmultilingual acoustic modeling for large vocabulary ASR. InProceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009.\\n[226] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, and T. Huang.\\nLarge-scale image classiﬁcation: Fast feature extraction and SVM train-\\ning. InProceedings of Computer Vision and Pattern Recognition\\n(CVPR). 2011.\\n[227] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann m achines and deep belief networks for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio Speech\\nLanguage Processing, 21(10):2129–2139, 2013.\\n[228] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann machines for statistical parametric speech synthe-\\nsis. In International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP), pages 7825–7829. 2013.\\n[229] Z. Ling, K. Richmond, and J. Yamagishi. Articulatory control of HMM-\\nbased parametric speech synthesis us ing feature-space-switched multi-\\nple regression. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21, January 2013.\\n[230] L. Lu, K. Chin, A. Ghoshal, and S. Renals. Joint uncertainty decoding\\nfor noise robust subspace gaussian mixture models. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(9):1791–1804, 2013.\\n[231] J. Ma and L. Deng. A path-stack algorithm for optimizing dynamic\\nregimes in a statistical hidden dynamical model of speech. Computer,\\nSpeech and Language, 2000.\\n[232] J. Ma and L. Deng. Eﬃcient decoding strategies for conversational\\nspeech recognition using a constrain ed nonlinear state-space model.\\nIEEE Transactions on Speech and Audio Processing , 11(6):590–602,\\n2003.\\n[233] J. Ma and L. Deng. Target-directed mixture dynamic models for spon-\\ntaneous speech recognition. IEEE Transactions on Speech and Audio\\nProcessing, 12(1):47–58, 2004.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='54ac80d2-754c-48ca-8d32-7f4790e0e49a', embedding=None, metadata={'page_label': '369', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 369\\n[234] A. Maas, A. Hannun, and A. Ng. Rectiﬁer nonlinearities improve neural\\nnetwork acoustic models. International Conference on Machine Learn-\\ning (ICML) Workshop on Deep Learning for Audio, Speech, and Lan-\\nguage Processing, 2013.\\n[235] A. Maas, Q. Le, T. O’Neil, O. Vinyals, P. Nguyen, and P. Ng. Recurrent\\nneural networks for noise reduction in robust ASR. In Proceedings of\\nInterspeech. 2012.\\n[236] C. Manning, P. Raghavan, and H. Schütze. Introduction to Information\\nRetrieval. Cambridge University Press, 2009.\\n[237] J. Markoﬀ. Scientists see promise in deep-learning programs. New York\\nTimes, November 24 2012.\\n[238] J. Martens. Deep learning with hessian-free optimization. In Proceedings\\nof International Conference on Machine Learning (ICML) . 2010.\\n[239] J. Martens and I. Sutskever. Learning recurrent neural networks with\\nhessian-free optimization. In Proceedings of International Conference\\non Machine Learning (ICML). 2011.\\n[240] D. McAllester. A PAC-bayesian tutorial with a dropout bound. ArX-\\nive1307.2118, July 2013.\\n[241] I. McGraw, I. Badr, and J. R. Glass. Learning lexicons from speech\\nusing a pronunciation mixture model. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(2):357,366, February 2013.\\n[242] G. Mesnil, X. He, L. Deng, and Y. Bengio. Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding. InProceedings of Interspeech. 2013.\\n[243] Y. Miao and F. Metze. Improving low-resource CD-DNN-HMM using\\ndropout and multilingual DNN training. In Proceedings of Interspeech.\\n2013.\\n[244] Y. Miao, S. Rawat, and F. Metze. Deep maxout networks for low\\nresource speech recognition. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n[245] T. Mikolov. Statistical language models based on neural networks.\\nPh.D. thesis, Brno University of Technology, 2012.\\n[246] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of\\nword representations in vector space. In Proceedings of International\\nConference on Learning Representations (ICLR). 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4c24af42-c9be-4e7c-b991-3c91b44180d8', embedding=None, metadata={'page_label': '370', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='370 References\\n[247] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky. Strategies\\nfor training large scale neural network language models. In Proceedings\\nof the IEEE Automatic Speech Recognition and Understanding Work-\\nshop (ASRU). 2011.\\n[248] T. Mikolov, M. Karaﬁat, L. Burg et, J. Cernocky, and S. Khudanpur.\\nRecurrent neural network based language model. In Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP), pages 1045–1048. 2010.\\n[249] T. Mikolov, Q. Le, and I. Sutskever. Exploiting similarities among lan-\\nguages for machine translation. arXiv:1309.4168v1, 2013.\\n[250] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed\\nrepresentations of words and phrases and their compositionality. In\\nProceedings of Neural Information Processing Systems (NIPS). 2013.\\n[251] Y. Minami, E. McDermott, A. Nakamura, and S. Katagiri. A recogni-\\ntion method with parametric trajectory synthesized using direct rela-\\ntions between static and dynamic feature vector time series. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 957–960. 2002.\\n[252] A. Mnih and G. Hinton. Three new graphical models for statistical lan-\\nguage modeling. In Proceedings of International Conference on Machine\\nLearning (ICML), pages 641–648. 2007.\\n[253] A. Mnih and G. Hinton. A scalable hierarchical distributed lan-\\nguage model. In Proceedings of Neural Information Processing Systems\\n(NIPS), pages 1081–1088. 2008.\\n[254] A. Mnih and K. Kavukcuoglu. Learning word embeddings eﬃciently\\nwith noise-contrastive estimation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n[255] A. Mnih and W.-T. Teh. A fast and simple algorithm for training\\nneural probabilistic language models. In Proceedings of International\\nConference on Machine Learning (ICML), pages 1751–1758. 2012.\\n[256] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\\nstra, and M. Riedmiller. Playing arari with deep reinforcement learning.\\nNeural Information Processing Systems (NIPS) Deep Learning Work-\\nshop, 2013. also arXiv:1312.5602v1.\\n[257] A. Mohamed, G. Dahl, and G. Hinton. Deep belief networks for phone\\nrecognition. In Proceedings of Neural Information Processing Systems\\n(NIPS) Workshop Deep Learning for Speech Recognition and Related\\nApplications. 2009.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3203fcdb-259f-4527-8a0d-f3ffc167355f', embedding=None, metadata={'page_label': '371', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 371\\n[258] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep\\nbelief networks. IEEE Transactions on Audio, Speech, & Language Pro-\\ncessing, 20(1), January 2012.\\n[259] A. Mohamed, G. Hinton, and G. Penn. Understanding how deep belief\\nnetworks perform acoustic modelling. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[260] A. Mohamed, D. Yu, and L. Deng. Investigation of full-sequence train-\\ning of deep belief networks for speech recognition. In Proceedings of\\nInterspeech. 2010.\\n[261] N. Morgan. Deep and wide: Multiple layers in automatic speech recog-\\nnition. IEEE Transactions on Audio, Speech, & Language Processing ,\\n20(1), January 2012.\\n[262] N. Morgan, Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki,\\nM. Ostendorf, P. Jain, H. Hermansky, D. Ellis, G. Doddington, B. Chen,\\nO. Cretin, H. Bourlard, and M. Athineos. Pushing the envelope — aside\\n[speech recognition]. IEEE Signal Processing Magazine , 22(5):81–88,\\nSeptember 2005.\\n[263] F. Morin and Y. Bengio. Hierarchi cal probabilistic neural network lan-\\nguage models. In Proceedings of Artiﬁcial Intelligence and Statistics\\n(AISTATS). 2005.\\n[264] K. Murphy. Machine Learning — A Probabilistic Perspective.T h eM I T\\nPress, 2012.\\n[265] V. Nair and G. Hinton. 3-d object recognition with deep belief nets. In\\nProceedings of Neural Information Processing Systems (NIPS). 2009.\\n[266] T. Nakashika, R. Takashima, T. Takiguchi, and Y. Ariki. Voice conver-\\nsion in high-order eigen space using deep belief nets. In Proceedings of\\nInterspeech. 2013.\\n[267] H. Ney. Speech translation: Coupling of recognition and translation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 1999.\\n[268] J. Ngiam, Z. Chen, P. Koh, and A. Ng. Learning deep energy models. In\\nProceedings of International Conference on Machine Learning (ICML).\\n2011.\\n[269] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng. Multimodal\\ndeep learning. In Proceedings of International Conference on Machine\\nLearning (ICML). 2011.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='96759554-0935-4ba0-bf0b-787b4b53811c', embedding=None, metadata={'page_label': '372', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='372 References\\n[270] M. Norouzi, T. Mikolov, S. Bengio, J. Shlens, A. Frome, G. Corrado,\\nand J. Dean. Zero-shot learning by convex combination of semantic\\nembeddings. arXiv:1312.5650v2, 2013.\\n[271] N. Oliver, A. Garg, and E. Horvitz. Layered representations for learning\\nand inferring oﬃce activity from multiple sensory channels. Computer\\nVision and Image Understanding , 96:163–180, 2004.\\n[272] B. Olshausen. Can ‘deep learning’ oﬀer deep insights about visual rep-\\nresentation? Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.\\n[273] M. Ostendorf. Moving beyond the ‘beads-on-a-string’ model of speech.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 1999.\\n[274] M. Ostendorf, V. Digalakis, and O. Kimball. From HMMs to segment\\nmodels: A uniﬁed view of stochastic modeling for speech recognition.\\nIEEE Transactions on Speech and Audio Processing , 4(5), September\\n1996.\\n[275] L. Oudre, C. Fevotte, and Y. Gren ier. Probabilistic template-based\\nchord recognition. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(8):2249–2259, November 2011.\\n[276] H. Palangi, L. Deng, and R. Ward. Learning input and recurrent weight\\nmatrices in echo state networks.Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop, December 2013.\\n[277] H. Palangi, R. Ward, and L. Deng. Using deep stacking network to\\nimprove structured compressive sensing with multiple measurement vec-\\ntors. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n[278] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Adap-\\ntive multimodal fusion by uncertainty compensation with application to\\naudiovisual speech recognition.IEEE Transactions on Audio, Speech,\\nand Language Processing, 17:423–435, 2009.\\n[279] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Learning Representations (ICLR). 2014.\\n[280] R. Pascanu, T. Mikolov, and Y. Bengio. On the diﬃculty of training\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.\\n[281] J. Peng, L. Bo, and J. Xu. Conditional neural ﬁelds. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2009.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bf845109-60b5-4cb1-8403-91591cc054e7', embedding=None, metadata={'page_label': '373', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 373\\n[282] P. Picone, S. Pike, R. Regan, T. Kamm, J. bridle, L. Deng, Z. Ma,\\nH. Richards, and M. Schuster. Initial evaluation of hidden dynamic\\nmodels on conversational speech. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP). 1999.\\n[283] J. Pinto, S. Garimella, M. Magimai-Doss, H. Hermansky, and\\nH. Bourlard. Analysis of MLP-based hierarchical phone posterior prob-\\nability estimators. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(2), February 2011.\\n[284] C. Plahl, T. Sainath, B. Ramabhadran, and D. Nahamoo. Improved\\npre-training of deep belief networks using sparse encoding symmet-\\nric machines. InProceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n[285] C. Plahl, R. Schlüter, and H. Ney. Hierarchical bottleneck features for\\nLVCSR. In Proceedings of Interspeech. 2010.\\n[286] T. Plate. Holographic reduced representations. IEEE Transactions on\\nNeural Networks, 6(3):623–641, May 1995.\\n[287] T. Poggio. How the brain might work: The role of information and\\nlearning in understanding and replicating intelligence. In G. Jacovitt,\\nA. Pettorossi, R. Consolo, and V. Senni, editors, Information: Science\\nand Technology for the New Century , pages 45–61. Lateran University\\nPress, 2007.\\n[288] J. Pollack. Recursive distributed representations. Artiﬁcial Intelligence,\\n46:77–105, 1990.\\n[289] H. Poon and P. Domingos. Sum-product networks: A new deep archi-\\ntecture. In Proceedings of Uncertainty in Artiﬁcial Intelligence. 2011.\\n[290] D. Povey and P. Woodland. Minimum phone error and I-smoothing\\nfor improved discriminative training. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2002.\\n[291] R. Prabhavalkar and E. Fosler-Lussier. Backpropagation training for\\nmultilayer conditional random ﬁeld based phone recognition. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2010.\\n[292] A. Prince and P. Smolensky. Optimality: From neural networks to uni-\\nversal grammar. Science, 275:1604–1610, 1997.\\n[293] L. Rabiner. A tutorial on hidden markov models and selected applica-\\ntions in speech recognition. In Proceedings of the IEEE, pages 257–286.\\n1989.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='34a00fa6-d6cb-4c5f-bc65-6e56c35bd347', embedding=None, metadata={'page_label': '374', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='374 References\\n[294] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for\\ndeep belief networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n[295] M. Ranzato, S. Chopra, Y. LeCun, and F.-J. Huang. Energy-based\\nmodels in document recognition and computer vision. In Proceed-\\nings of International Conference on Document Analysis and Recognition\\n(ICDAR). 2007.\\n[296] M. Ranzato and G. Hinton. Modeling pixel means and covariances using\\nfactorized third-order boltzmann machines. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2010.\\n[297] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Eﬃcient learning\\nof sparse representations with an energy-based model. In Proceedings\\nof Neural Information Processing Systems (NIPS) . 2006.\\n[298] M. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative\\nmodels with applications to recognition. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2011.\\n[299] C. Rathinavalu and L. Deng. Construction of state-dependent dynamic\\nparameters by maximum likelihood: Applications to speech recognition.\\nSignal Processing, 55(2):149–165, 1997.\\n[300] S. Rennie, K. Fouset, and P. Dognin. Factorial hidden restricted boltz-\\nmann machines for noise ro bust speech recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n[301] S. Rennie, H. Hershey, and P. Olsen. Single-channel multi-talker speech\\nrecognition — graphical modeling approaches. IEEE Signal Processing\\nMagazine, 33:66–80, 2010.\\n[302] M. Riedmiller and H. Braun. A direct adaptive method for faster back-\\npropagation learning: The RPROP algorithm. In Proceedings of the\\nIEEE International Conference on Neural Networks . 1993.\\n[303] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive\\nautoencoders: Explicit invariance during feature extraction. In Proceed-\\nings of International Conference on Machine Learning (ICML) , pages\\n833–840. 2011.\\n[304] A. Robinson. An application o f recurrent nets to phone probability\\nestimation. IEEE Transactions on Neural Networks, 5:298–305, 1994.\\n[305] T. Sainath, L. Horesh, B. Kingsbury, A. Aravkin, and B. Ramabhad-\\nran. Accelerating hessian-free optimization for deep neural networks by\\nimplicit pre-conditioning and sampling. arXiv: 1309.1508v3, 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='345f85da-83a9-4995-b09f-3b2da0ec48b4', embedding=None, metadata={'page_label': '375', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 375\\n[306] T. Sainath, B. Kingsbury, A. Mohamed, G. Dahl, G. Saon, H. Soltau,\\nT. Beran, A. Aravkin, and B. Ramabhadran. Improvements to deep\\nconvolutional neural networks for LVCSR. In Proceedings of the Auto-\\nmatic Speech Recognition and Understanding Workshop (ASRU). 2013.\\n[307] T. Sainath, B. Kingsbury, A. Mohamed, and B. Ramabhadran. Learn-\\ning ﬁlter banks within a deep neural network framework. In Proceed-\\nings of The Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n[308] T. Sainath, B. Kingsbury, and B. Ramabhadran. Autoencoder bottle-\\nneck features using deep belief networks. InProceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[309] T. Sainath, B. Kingsbury, B. Ramabhadran, P. Novak, and\\nA. Mohamed. Making deep belief net works eﬀective for large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2011.\\n[310] T. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhad-\\nran. Low-rank matrix factorization for deep neural network training\\nwith high-dimensional output targets. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[311] T. Sainath, B. Kingsbury, H. Soltau, and B. Ramabhadran. Optimiza-\\ntion techniques to improve training speed of deep neural networks for\\nlarge speech tasks. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(11):2267–2276, November 2013.\\n[312] T. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Con-\\nvolutional neural networks for LVCSR. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[313] T. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and\\nD. Kanevsky. Exemplar-based sparse representation features: From\\nTIMIT to LVCSR.IEEE Transactions on Speech and Audio Processing,\\nNovember 2011.\\n[314] R. Salakhutdinov and G. Hinton. Semantic hashing. In Proceedings of\\nSpecial Interest Group on Information Retrieval (SIGIR) Workshop on\\nInformation Retrieval and Applications of Graphical Models . 2007.\\n[315] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In Pro-\\nceedings of Artiﬁcial Intelligence and Statistics (AISTATS) . 2009.\\n[316] R. Salakhutdinov and G. Hinton. A better way to pretrain deep boltz-\\nmann machines. In Proceedings of Neural Information Processing Sys-\\ntems (NIPS). 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9a4fa5db-5eda-481a-8272-2dfada90461e', embedding=None, metadata={'page_label': '376', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='376 References\\n[317] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny. Speaker adaptation\\nof neural network acoustic models using i-vectors. In Proceedings of the\\nAutomatic Speech Recognition and Understanding Workshop (ASRU) .\\n2013.\\n[318] R. Sarikaya, G. Hinton, and B. Ramabhadran. Deep belief nets for nat-\\nural language call-routing. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP), pages 5680–5683.\\n2011.\\n[319] E. Schmidt and Y. Kim. Learning emotion-based acoustic features with\\ndeep belief networks. In Proceedings IEEE of Signal Processing to Audio\\nand Acoustics. 2011.\\n[320] H. Schwenk. Continuous space translation models for phrase-based sta-\\ntistical machine translation. In Proceedings of Computional Linguistics.\\n2012.\\n[321] H. Schwenk, A. Rousseau, and A. Mohammed. Large, pruned or contin-\\nuous space language models on a gpu for statistical machine translation.\\nInProceedings of the Joint Human Language Technology Conference\\nand the North American Chapter of the Association of Computational\\nLinguistics (HLT-NAACL) 2012 Workshop on the future of language\\nmodeling for Human Language Technology (HLT), pages 11–19.\\n[322] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. On parallelizability of\\nstochastic gradient descent for speech DNNs. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n[323] F. Seide, G. Li, X. Chen, and D. Yu. Feature engineering in context-\\ndependent deep neural networks for conversational speech transcription.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU), pages 24–29. 2011.\\n[324] F. Seide, G. Li, and D. Yu. Conversational speech transcription using\\ncontext-dependent deep neural networks. In Proceedings of Interspeech,\\npages 437–440. 2011.\\n[325] M. Seltzer, D. Yu, and E. Wang. An investigation of deep neural net-\\nworks for noise robust speech recognition. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013.\\n[326] M. Shannon, H. Zen, and W. Byrne. Autoregressive models for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio, Speech,\\nLanguage Processing, 21(3):587–597, 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9a0cb540-8686-4678-9dc2-c88dd44d264a', embedding=None, metadata={'page_label': '377', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 377\\n[327] H. Sheikhzadeh and L. Deng. Waveform-based speech recognition using\\nhidden ﬁlter models: Parameter selection and sensitivity to power nor-\\nmalization. IEEE Transactions on on Speech and Audio Processing\\n(ICASSP), 2:80–91, 1994.\\n[328] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic\\nrepresentations using convolutional neural networks for web search. In\\nProceedings World Wide Web. 2014.\\n[329] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep ﬁsher networks for\\nlarge-scale image classiﬁcation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n[330] M. Siniscalchi, J. Li, and C. Lee. Hermitian polynomial for speaker\\nadaptation of connectionist speech recognition systems. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(10):2152–2161,\\n2013a.\\n[331] M. Siniscalchi, T. Svendsen, and C.-H. Lee. A bottom-up modular\\nsearch approach to large vocabulary continuous speech recognition.\\nIEEE Transactions on Audio, Speech, Language Processing, 21, 2013.\\n[332] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Exploiting deep neu-\\nral networks for detection-based speech recognition. Neurocomputing,\\n106:148–157, 2013.\\n[333] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Speech recognition using\\nlong-span temporal patterns in a deep network model. IEEE Signal\\nProcessing Letters, 20(3):201–204, March 2013.\\n[334] G. Sivaram and H. Hermansky. Sparse multilayer perceptrons for\\nphoneme recognition. IEEE Transactions on Audio, Speech, & Lan-\\nguage Processing, 20(1), January 2012.\\n[335] P. Smolensky. Tensor product variable binding and the representation\\nof symbolic structures in connectionist systems. Artiﬁcial Intelligence,\\n46:159–216, 1990.\\n[336] P. Smolensky and G. Legendre. T h eH a r m o n i cM i n d—F r o mN e u -\\nral Computation to Optimality-Theoretic Grammar .T h e M I T P r e s s ,\\nCambridge, MA, 2006.\\n[337] J. Snoek, H. Larochelle, and R. Adams. Practical bayesian optimization\\nof machine learning algorithms. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2012.\\n[338] R. Socher. New directions in deep learning: Structured models, tasks,\\nand datasets. Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d0d3cac3-afb8-468b-b303-38d9ea4e2848', embedding=None, metadata={'page_label': '378', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='378 References\\n[339] R. Socher, Y. Bengio, and C. Manning. Deep learning for NLP.\\nTutorial at Association of Computational Logistics (ACL), 2012, and\\nNorth American Chapter of the Association of Computational Linguis-\\ntics (NAACL), 2013. http://www.socher.o rg/index.php/DeepLearning\\nTutorial.\\n[340] R. Socher, D. Chen, C. Manning, and A. Ng. Reasoning with neural\\ntensor networks for knowledge base completion. InProceedings of Neural\\nInformation Processing Systems (NIPS). 2013.\\n[341] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised\\nsegmentation and annotation of images using unaligned text corpora.\\nInProceedings of Computer Vision and Pattern Recognition (CVPR) .\\n2010.\\n[342] R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. Manning, and A. Ng.\\nZero-shot learning through cross-modal transfer. In Proceedings of Neu-\\nral Information Processing Systems (NIPS). 2013b.\\n[343] R. Socher, Q. Le, C. Manning, and A. Ng. Grounded compositional\\nsemantics for ﬁnding and describing images with sentences. Neu-\\nral Information Processing Systems (NIPS) Deep Learning Workshop ,\\n2013c.\\n[344] R. Socher, C. Lin, A. Ng, and C. Manning. Parsing natural scenes\\nand natural language with recursive neural networks. In Proceedings of\\nInternational Conference on Machine Learning (ICML) . 2011.\\n[345] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Dynamic\\npooling and unfolding recursive autoencoders for paraphrase detection.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2011.\\n[346] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Semi-\\nsupervised recursive autoencoders f or predicting sentiment distribu-\\ntions. In Proceedings of Empirical Methods in Natural Language Pro-\\ncessing (EMNLP). 2011.\\n[347] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and\\nC. Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank. InProceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP). 2013.\\n[348] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep\\nboltzmann machines. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='041f26cd-b0ac-4b46-a6da-87d69affb61f', embedding=None, metadata={'page_label': '379', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 379\\n[349] N. Srivastava and R. Salakhutdinov. Discriminative transfer learning\\nwith tree-based priors. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n[350] R. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber.\\nCompete to compute. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n[351] T. Stafylakis, P. Kenny, M. Senoussaoui, and P. Dumouchel. Prelimi-\\nnary investigation of boltzmann machine classiﬁers for speaker recogni-\\ntion. In Proceedings of Odyssey, pages 109–116. 2012.\\n[352] V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk minimization of\\ngraphical model parameters given approximate inference, decoding, and\\nmodel structure. In Proceedings of Artiﬁcial Intelligence and Statistics\\n(AISTATS). 2011.\\n[353] H. Su, G. Li, D. Yu, and F. Seide. Error back propagation for sequence\\ntraining of context-dependent deep networks for conversational speech\\ntranscription. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n[354] A. Subramanya, L. Deng, Z. Liu, an d Z. Zhang. Multi-sensory speech\\nprocessing: Incorporating automatically extracted hidden dynamic\\ninformation. InProceedings of IEEE International Conference on Mul-\\ntimedia & Expo (ICME). Amsterdam, July 2005.\\n[355] J. Sun and L. Deng. An overlapping-feature based phonological model\\nincorporating linguistic constraints: Applications to speech recognition.\\nJournal on Acoustical Society of America, 111(2):1086–1101, 2002.\\n[356] I. Sutskever. Training recurrent neural networks. Ph.D. Thesis, Univer-\\nsity of Toronto, 2013.\\n[357] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\\nneural networks. InProceedings of International Conference on Machine\\nLearning (ICML). 2011.\\n[358] Y. Tang and C. Eliasmith. Deep networks for robust visual recogni-\\ntion. In Proceedings of International Conference on Machine Learning\\n(ICML). 2010.\\n[359] Y. Tang and R. Salakhutdinov. Learning Stochastic Feedforward Neural\\nNetworks. NIPS, 2013.\\n[360] A. Tarralba, R. Fergus, and Y. Weiss. Small codes and large image\\ndatabases for recognition. In Proceedings of Computer Vision and Pat-\\ntern Recognition (CVPR). 2008.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7df38628-c49f-4504-b76c-5937c67f857a', embedding=None, metadata={'page_label': '380', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='380 References\\n[361] G. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using\\nbinary latent variables. InProceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n[362] S. Thomas, M. Seltzer, K. Church, and H. Hermansky. Deep neural\\nnetwork features and semi-supervised training for low resource speech\\nrecognition. In Proceedings of Interspeech. 2013.\\n[363] T. Tieleman. Training restricted boltzmann machines using approx-\\nimations to the likelihood gradient. In Proceedings of International\\nConference on Machine Learning (ICML). 2008.\\n[364] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, H. Yamagishi, and K. Oura.\\nSpeech synthesis based on hidden markov models. Proceedings of the\\nIEEE, 101(5):1234–1252, 2013.\\n[365] F. Triefenbach, A. Jalalvand, K. Demuynck, and J.-P. Martens. Acoustic\\nmodeling with hierarchical reservoirs. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(11):2439–2450, November 2013.\\n[366] G. Tur, L. Deng, D. Hakkani-Tür, and X. He. Towards deep under-\\nstanding: Deep convex networks for semantic utterance classiﬁcation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2012.\\n[367] J. Turian, L. Ratinov, and Y. Bengio. Word representations: A simple\\nand general method for semi-supervised learning. In Proceedings of\\nAssociation for Computational Linguistics (ACL) . 2010.\\n[368] Z. Tüske, M. Sundermeyer, R. Schlüter, and H. Ney. Context-dependent\\nMLPs for LVCSR: TANDEM, hybrid or both? In Proceedings of Inter-\\nspeech. 2012.\\n[369] B. Uria, S. Renals, and K. Richmond. A deep neural network for\\nacoustic-articulatory speech inversion. Neural Information Processing\\nSystems (NIPS) Workshop on Deep Learning and Unsupervised Feature\\nLearning, 2011.\\n[370] R. van Dalen and M. Gales. Extended VTS for noise-robust speech\\nrecognition. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 19(4):733–743, 2011.\\n[371] A. van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based\\nmusic recommendation. In Proceedings of Neural Information Process-\\ning Systems (NIPS) . 2013.\\n[372] V. Vasilakakis, S. Cumani, and P. Laface. Speaker recognition by means\\nof deep belief networks. In Proceedings of Biometric Technologies in\\nForensic Science. 2013.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3c1f9f63-f654-4264-b95e-0ab6b7a1f423', embedding=None, metadata={'page_label': '381', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 381\\n[373] K. Vesely, A. Ghoshal, L. Burget, and D. Povey. Sequence-discriminative\\ntraining of deep neural networks. In Proceedings of Interspeech. 2013.\\n[374] K. Vesely, M. Hannemann, and L. Burget. Semi-supervised training of\\ndeep neural networks. In Proceedings of the Automatic Speech Recogni-\\ntion and Understanding Workshop (ASRU) . 2013.\\n[375] P. Vincent. A connection between score matching and denoising autoen-\\ncoder. Neural Computation, 23(7):1661–1674, 2011.\\n[376] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.\\nStacked denoising autoencoders: Lea rning useful representations in a\\ndeep network with a loca l denoising criterion. Journal of Machine\\nLearning Research, 11:3371–3408, 2010.\\n[377] O. Vinyals, Y. Jia, L. Deng, and T. Darrell. Learning with recursive\\nperceptual representations. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n[378] O. Vinyals and D. Povey. Krylov subspace descent for deep learning. In\\nProceedings of Artiﬁcial Intelligence and Statistics (AISTATS). 2012.\\n[379] O. Vinyals and S. Ravuri. Comparing multilayer perceptron to deep\\nbelief network tandem features for robust ASR. In Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n[380] O. Vinyals, S. Ravuri, and D. Povey. Revisiting recurrent neural net-\\nworks for robust ASR. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012.\\n[381] S. Wager, S. Wang, and P. Liang. Dropout training as adaptive reg-\\nularization. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2013.\\n[382] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme\\nrecognition using time-delay neural networks. IEEE Transactions on\\nAcoustical Speech, and Signal Processing, 37:328–339, 1989.\\n[383] G. Wang and K. Sim. Context-dependent modelling of deep neural\\nnetwork using logistic regression. InProceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n[384] G. Wang and K. Sim. Regression-based context-dependent modeling\\nof deep neural networks for speech recognition. IEEE/Association for\\nComputing Machinery (ACM) Transactions on Audio, Speech, and Lan-\\nguage Processing, 2014.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='afde8315-7d86-4f92-9144-94b4c1346b46', embedding=None, metadata={'page_label': '382', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='382 References\\n[385] D. Warde-Farley, I. Goodfellow, A. Courville, and Y. Bengi. An empir-\\nical analysis of dropout in piecewise linear networks. In Proceedings of\\nInternational Conference on Learning Representations (ICLR). 2014.\\n[386] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmo-\\nniums with an application to information retrieval. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2005.\\n[387] C. Weng, D. Yu, M. Seltzer, and J. Droppo. Single-channel mixed speech\\nrecognition using deep neural networks. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2014.\\n[388] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation:\\nLearning to rank with joint word-image embeddings.Machine Learning,\\n81(1):21–35, 2010.\\n[389] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large\\nvocabulary image annotation. In Proceedings of International Joint\\nConference on Artiﬁcial Intelligence (IJCAI) . 2011.\\n[390] S. Wiesler, J. Li, and J. Xue. Investigations on hessian-free optimization\\nfor cross-entropy training of deep neural networks. In Proceedings of\\nInterspeech. 2013.\\n[391] M. Wohlmayr, M. Stark, and F. Pernkopf. A probabilistic interac-\\ntion model for multi-pitch tracking with factorial hidden markov model.\\nIEEE Transactions on Audio, Speech, and Language Processing, 19(4),\\nMay 2011.\\n[392] D. Wolpert. Stacked generalization. Neural Networks, 5(2):241–259,\\n1992.\\n[393] S. J. Wright, D. Kanevsky, L. Deng, X. He, G. Heigold, and H. Li.\\nOptimization algorithms and applications for speech and language pro-\\ncessing. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(11):2231–2243, November 2013.\\n[394] L. Xiao and L. Deng. A geometric perspective of large-margin training\\nof gaussian models. IEEE Signal Processing Magazine , 27(6):118–123,\\nNovember 2010.\\n[395] X. Xie and S. Seung. Equivalence of backpropagation and contrastive\\nhebbian learning in a layered network. Neural computation, 15:441–454,\\n2003.\\n[396] Y. Xu, J. Du, L. Dai, and C. Lee. An experimental study on speech\\nenhancement based on deep neural networks. IEEE Signal Processing\\nLetters, 21(1):65–68, 2014.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d9ebb127-cf45-4ce4-93e7-59bba1881d03', embedding=None, metadata={'page_label': '383', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 383\\n[397] J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network\\nacoustic models with singular value decomposition. In Proceedings of\\nInterspeech. 2013.\\n[398] S. Yamin, L. Deng, Y. Wang, and A. Acero. An integrative and discrimi-\\nnative technique for spoken utterance classiﬁcation. IEEE Transactions\\non Audio, Speech, and Language Processing, 16:1207–1214, 2008.\\n[399] Z. Yan, Q. Huo, and J. Xu. A scalable approach to using DNN-derived\\nfeatures in GMM-HMM based acoustic modeling for LVCSR. In Pro-\\nceedings of Interspeech. 2013.\\n[400] D. Yang and S. Furui. Combining a two-step CRF model and a joint\\nsource-channel model for machine transliteration. In Proceedings of\\nAssociation for Computational Linguistics (ACL), pages 275–280. 2010.\\n[401] K. Yao, D. Yu, L. Deng, and Y. Gong. A fast maximum likelihood non-\\nlinear feature transformation method for GMM-HMM speaker adapta-\\ntion. Neurocomputing, 2013a.\\n[402] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong. Adaptation of\\ncontext-dependent deep neural networks for automatic speech recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n[403] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu. Recurrent neural\\nnetworks for language understanding. In Proceedings of Interspeech.\\n2013.\\n[404] T. Yoshioka and T. Nakatani. Noise model transfer: Novel approach to\\nrobustness against nonstationary noise. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(10):2182–2192, 2013.\\n[405] T. Yoshioka, A. Ragni, and M. Gales. Investigation of unsupervised\\nadaptation of DNN acoustic models with ﬁlter bank input. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n[406] L. Younes. On the convergence of markovian stochastic algorithms with\\nrapidly decreasing ergodicity rates. Stochastics and Stochastic Reports,\\n65(3):177–228, 1999.\\n[407] D. Yu, X. Chen, and L. Deng. Factorized deep neural networks for adap-\\ntive speech recognition. International Workshop on Statistical Machine\\nLearning for Speech Processing, March 2012b.\\n[408] D. Yu, D. Deng, and S. Wang. Learning in the deep-structured con-\\nditional random ﬁelds. Neural Information Processing Systems (NIPS)\\n2009 Workshop on Deep Learning for Speech Recognition and Related\\nApplications, 2009.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0505d1ca-7c72-4d4c-97ee-295b1560d4b2', embedding=None, metadata={'page_label': '384', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='384 References\\n[409] D. Yu and L. Deng. Solving nonlinear estimation problems using splines.\\nIEEE Signal Processing Magazine, 26(4):86–90, July 2009.\\n[410] D. Yu and L. Deng. Deep-structured hidden conditional random ﬁelds\\nfor phonetic recognition. In Proceedings of Interspeech. September 2010.\\n[411] D. Yu and L. Deng. Accelerated parallelizable neural networks learning\\nalgorithms for speech recognition. In Proceedings of Interspeech. 2011.\\n[412] D. Yu and L. Deng. Deep learning and its applications to signal and\\ninformation processing. IEEE Signal Processing Magazine, pages 145–\\n154, January 2011.\\n[413] D. Yu and L. Deng. Eﬃcient and eﬀective algorithms for training single-\\nhidden-layer neural networks. Pattern Recognition Letters, 33:554–558,\\n2012.\\n[414] D. Yu, L. Deng, and G. E. Dahl. Roles of pre-training and ﬁne-tuning in\\ncontext-dependent DBN-HMMs for real-world speech recognition. Neu-\\nral Information Processing Systems (NIPS) 2010 Workshop on Deep\\nLearning and Unsupervised Feature Learning, December 2010.\\n[415] D. Yu, L. Deng, J. Droppo, J. Wu, Y. Gong, and A. Acero. Robust\\nspeech recognition using cepstral m inimum-mean-square-error noise\\nsuppressor. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 16(5), July 2008.\\n[416] D. Yu, L. Deng, Y. Gong, and A. Acero. A novel framework and training\\nalgorithm for variable-parameter hidden markov models. IEEE Trans-\\nactions on Audio, Speech and Language Processing , 17(7):1348–1360,\\n2009.\\n[417] D. Yu, L. Deng, X. He, and A. Acero. Large-margin minimum clas-\\nsiﬁcation error training: A theoretical risk minimization perspective.\\nComputer Speech and Language, 22(4):415–429, October 2008.\\n[418] D. Yu, L. Deng, X. He, and X. Acero. Large-margin minimum classi-\\nﬁcation error training for large-scale speech recognition tasks. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2007.\\n[419] D. Yu, L. Deng, G. Li, and F. Seide. Discriminative pretraining of deep\\nneural networks. U.S. Patent Filing , November 2011.\\n[420] D. Yu, L. Deng, P. Liu, J. Wu, Y. Gong, and A. Acero. Cross-lingual\\nspeech recognition under runtime resource constraints. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009b.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c1179a6e-e91d-496b-a3d5-a787508d0c9b', embedding=None, metadata={'page_label': '385', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 385\\n[421] D. Yu, L. Deng, and F. Seide. Large vocabulary speech recognition using\\ndeep tensor neural networks. In Proceedings of Interspeech. 2012c.\\n[422] D. Yu, L. Deng, and F. Seide. The deep tensor neural network with\\napplications to large vocabulary speech recognition. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(2):388–396, 2013.\\n[423] D. Yu, J.-Y. Li, and L. Deng. Calibration of conﬁdence measures in\\nspeech recognition. IEEE Transactions on Audio, Speech and Language,\\n19:2461–2473, 2010.\\n[424] D. Yu, F. Seide, G. Li, and L. Deng. Exploiting sparseness in deep\\nneural networks for large vocabulary speech recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n[425] D. Yu and M. Seltzer. Improved bottleneck features using pre-trained\\ndeep neural networks. In Proceedings of Interspeech. 2011.\\n[426] D. Yu, M. Seltzer, J. Li, J.-T. Huang, and F. Seide. Feature learning in\\ndeep neural networks — studies on speech recognition. In Proceedings\\nof International Conference on Learning Representations (ICLR). 2013.\\n[427] D. Yu, S. Siniscalchi, L. Deng, and C. Lee. Boosting attribute and\\nphone estimation accura cies with deep neural networks for detection-\\nbased speech recognition. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[428] D. Yu, S. Wang, and L. Deng. Sequential labeling using deep-structured\\nconditional random ﬁelds. Journal of Selected Topics in Signal Process-\\ning, 4:965–973, 2010.\\n[429] D. Yu, S. Wang, Z. Karam, and L. Deng. Language recognition using\\ndeep-structured conditional random ﬁelds. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP),\\npages 5030–5033. 2010.\\n[430] D. Yu, K. Yao, H. Su, G. Li, and F. Seide. KL-divergence regularized\\ndeep neural network adaptation for improved large vocabulary speech\\nrecognition. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n[431] K. Yu, M. Gales, and P. Woodland. Unsupervised adaptation with dis-\\ncriminative mapping transforms. IEEE Transactions on Audio, Speech,\\nand Language Processing, 17(4):714–723, 2009.\\n[432] K. Yu, Y. Lin, and H. Laﬀerty. Learning image representations from\\nthe pixel level via hierarchical sparse coding. In Proceedings Computer\\nVision and Pattern Recognition (CVPR). 2011.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='091e90ce-6da3-405b-a465-46df3520f65c', embedding=None, metadata={'page_label': '386', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='386 References\\n[433] F. Zamora-Martínez, M. Castro-Bleda, and S. España-Boquera. Fast\\nevaluation of connectionist language models. International Conference\\no nA r t i ﬁ c i a lN e u r a lN e t w o r k s, pages 144–151, 2009.\\n[434] M. Zeiler. Hierarchical convolutional deep learning in computer vision.\\nPh.D. Thesis, New York University, January 2014.\\n[435] M. Zeiler and R. Fergus. Stochastic pooling for regularization of deep\\nconvolutional neural networks. In Proceedings of International Confer-\\nence on Learning Representations (ICLR). 2013.\\n[436] M. Zeiler and R. Fergus. Visualiz ing and understanding convolutional\\nnetworks. arXiv:1311.2901, pages 1–11, 2013.\\n[437] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolutional networks\\nfor mid and high level feature learning. In Proceedings of International\\nConference on Computer vision (ICCV) . 2011.\\n[438] H. Zen, M. Gales, J. F. Nankaku, and Y. K. Tokuda. Product of\\nexperts for statistical par ametric speech synthesis. IEEE Transactions\\non Audio, Speech, and Language Processing, 20(3):794–805, March 2012.\\n[439] H. Zen, Y. Nankaku, and K. Tokuda. Continuous stochastic feature\\nmapping based on trajectory HMMs. IEEE Transactions on Audio,\\nSpeech, and Language Processings, 19(2):417–430, February 2011.\\n[440] H. Zen, A. Senior, and M. Schuster. Statistical parametric speech syn-\\nthesis using deep neural networks. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP) , pages\\n7962–7966. 2013.\\n[441] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur. Improving deep\\nneural network acoustic models using generalized maxout networks. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2014.\\n[442] X. Zhang and J. Wu. Deep belief networks based voice activity detec-\\ntion. IEEE Transactions on Audio, Speech, and Language Processing ,\\n21(4):697–710, 2013.\\n[443] Z. Zhang, Z. Liu, M. Sinclair, A. Acero, L. Deng, J. Droppo, X. Huang,\\nand Y. Zheng. Multi-sensory microphones for robust speech detection,\\nenhancement and recognition. In Proceedings of International Confer-\\nence on Acoustics Speech and Signal Processing (ICASSP). 2004.\\n[444] Y. Zhao and B. Juang. Nonlinear compensation using the gauss-newton\\nmethod for noise-robust speech recognition. IEEE Transactions on\\nAudio, Speech, and Language Processing, 20(8):2191–2206, 2012.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='274f8d58-ac75-49f9-8f59-6a4e41fe8e58', embedding=None, metadata={'page_label': '387', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References 387\\n[445] W. Zou, R. Socher, D. Cer, and C. Manning. Bilingual word embed-\\ndings for phrase-based machine translation. In Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP). 2013.\\n[446] G. Zweig and P. Nguyen. A segmental CRF approach to large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2009.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create an Index"
      ],
      "metadata": {
        "id": "Mj7dKBQ57xS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=GPTVectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "ItJil79p7yLO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query the Index"
      ],
      "metadata": {
        "id": "z4yg_bX680XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "amSa4lOk81LB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Deep Learning?\")"
      ],
      "metadata": {
        "id": "XYA_RE9fMAjQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKpSGfPKMEAH",
        "outputId": "e8a20dd0-9a7e-4f34-8d30-96187d8185d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Deep Learning involves learning multiple levels of representation and abstraction to make sense of data such as images, sound, and text. It focuses on models with multiple layers of nonlinear information processing and methods for supervised or unsupervised learning of feature representation at increasingly abstract layers. Deep Learning is at the intersection of various research areas like neural networks, artificial intelligence, graphical modeling, optimization, pattern recognition, and signal processing. It has gained popularity due to advancements in chip processing abilities, larger training data sets, and progress in machine learning and signal processing research, enabling the effective utilization of complex functions, hierarchical feature representations, and both labeled and unlabeled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integrate with LangChain"
      ],
      "metadata": {
        "id": "9c77ky9H9Bqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C49Gz_D4MXPm",
        "outputId": "8a5c5896-7ad9-4a38-b30d-fda66b3d6534"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting openai<2.0.0,>=1.66.3 (from langchain_openai)\n",
            "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.3.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_openai) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain_openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.45-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.44\n",
            "    Uninstalling langchain-core-0.3.44:\n",
            "      Successfully uninstalled langchain-core-0.3.44\n",
            "Successfully installed langchain-core-0.3.45 langchain_openai-0.3.9 openai-1.66.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import LlamaIndexRetriever\n",
        "\n",
        "# Create the retriever\n",
        "retriever = LlamaIndexRetriever(index=index)\n",
        "#retriever.as_query_engine()\n",
        "\n",
        "# Create the LLM and RetrievalQA chain\n",
        "llm = OpenAI()\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Query using the QA Chain\n",
        "query = \"Explain the benefits of CNN in Deep Learning.\""
      ],
      "metadata": {
        "id": "59ErwdYo9Gba"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the index into a query engine and then query:\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "gu7QCbicNujL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZC0MjQGOHy1",
        "outputId": "b0f3ab08-bd8d-4ac4-ba75-3d1c3fa3e7a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='CNNs in Deep Learning offer several benefits. They are effective in tasks like computer vision and image recognition due to weight sharing in convolutional layers and subsampling in pooling layers, which provide properties like translation invariance. Additionally, CNNs have been found to be highly efficient in handling complex pattern recognition tasks. Moreover, with appropriate modifications, CNNs can also be applied successfully to speech recognition tasks. The use of non-saturating neurons like rectified linear units (ReLU) and regularization techniques like \"dropout\" further enhance the performance of CNNs by speeding up the training process and improving generalization.', source_nodes=[NodeWithScore(node=TextNode(id_='b625eb4d-7755-4bac-898f-e9de207833f5', embedding=None, metadata={'page_label': '325', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43bbdc59-84bc-4974-a2d0-1b50281a4f2c', node_type='4', metadata={'page_label': '325', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, hash='0e6f691b44f9bc7537a157d72c22a820967feef4e7dba914fcf076ead5100c3d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='10.2. Supervised feature learning and classiﬁcation 325\\nFigure 10.2: The original convolutional neural network that is composed of mul-\\ntiple alternating convolution and pooling layers followed by fully connected layers.\\n[after [212], @IEEE].\\nthe task is to train a model with 1.2 million high-resolution images\\nto classify unseen images to one of the 1000 diﬀerent image classes.\\nOn the test set consisting of 150k images, the deep CNN approach\\ndescribed in [198] achieved the error rates considerably lower than the\\nprevious state-of-the-art. Very large deep-CNNs are used, consisting of\\n60 million weights, and 650,000 neurons, and ﬁve convolutional layers\\ntogether with max-pooling layers. Additional two fully-connected layers\\nas in the DNN described previously are used on top of the CNN layers.\\nAlthough all the above structures were developed separately in earlier\\nwork, their best combination accounted for major part of the success.\\nSee the overall architecture of the deep CNN system in Figure 10.3. Two\\nadditional factors contribute to the ﬁnal success. The ﬁrst is a powerful\\nregularization technique called “dropout”; see details in [166] and a\\nseries of further analysis and improvement in [10, 13, 240, 381, 385]. In\\nparticular, Warde-Farley et al. [385] analyzed the disentangling eﬀects\\nof dropout and showed that it helps because diﬀerent members of the\\nbag share parameters. Applications of the same “dropout” techniques\\nare also successful for some speech recognition tasks [65, 81]. The\\nsecond factor is the use of non-saturating neurons or rectiﬁed linear\\nunits (ReLU) that compute f(x)=m a x ( x, 0), which signiﬁcantly\\nspeeds up the overall training process especially with eﬃcient GPU\\nimplementation. This deep-CNN system achieved a winning top-5 test\\nerror rate of 15.3% using extra training data from ImageNet Fall 2011\\nrelease, or 16.4% using only supplied training data in ImageNet-2012,', mimetype='text/plain', start_char_idx=0, end_char_idx=1917, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.84609285803694), NodeWithScore(node=TextNode(id_='b1189607-7c41-43b8-8ccf-e4ccb7bb3e05', embedding=None, metadata={'page_label': '225', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='950ba710-1b1a-4291-84eb-4399210d5e84', node_type='4', metadata={'page_label': '225', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, hash='8d9e6649ba413a9bb964eeed05c5b19cdc09aabf75561a1b983c763d9fa83d28')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='3.3. Deep networks for supervised learning 225\\na convolutional layer and a pooling layer. These modules are often\\nstacked up with one on top of another, or with a DNN on top of it, to\\nform a deep model [212]. The convolutional layer shares many weights,\\nand the pooling layer subsamples the output of the convolutional layer\\nand reduces the data rate from the layer below. The weight sharing\\nin the convolutional layer, together with appropriately chosen pool-\\ning schemes, endows the CNN with some “invariance” properties (e.g.,\\ntranslation invariance). It has been argued that such limited “invari-\\nance” or equi-variance is not adequate for complex pattern recognition\\ntasks and more principled ways of handling a wider range of invariance\\nmay be needed [160]. Nevertheless, CNNs have been found highly eﬀec-\\ntive and been commonly used in computer vision and image recognition\\n[54, 55, 56, 57, 69, 198, 209, 212, 434]. More recently, with appropri-\\nate changes from the CNN designed for image analysis to that taking\\ninto account speech-speciﬁc properties, the CNN is also found eﬀec-\\ntive for speech recognition [1, 2, 3, 81, 94, 312]. We will discuss such\\napplications in more detail in Section 7 of this monograph.\\nIt is useful to point out that the time-delay neural network (TDNN)\\n[202, 382] developed for early speech recognition is a special case and\\npredecessor of the CNN when weight sharing is limited to one of the\\ntwo dimensions, i.e., time dimension, and there is no pooling layer. It\\nwas not until recently that researchers have discovered that the time-\\ndimension invariance is less important than the frequency-dimension\\ninvariance for speech recognition [1, 3, 81]. A careful analysis on the\\nunderlying reasons is described in [81], together with a new strategy for\\ndesigning the CNN’s pooling layer demonstrated to be more eﬀective\\nthan all previous CNNs in phone recognition.\\nIt is also useful to point out that the model of hierarchical tempo-\\nral memory (HTM) [126, 143, 142] is another variant and extension of\\nthe CNN. The extension includes the following aspects: (1) Time or\\ntemporal dimension is introduced to serve as the “supervision” infor-\\nmation for discrimination (even for static images); (2) Both bottom-up\\nand top-down information ﬂows are used, instead of just bottom-up in\\nthe CNN; and (3) A Bayesian probabilistic formalism is used for fusing\\ninformation and for decision making.', mimetype='text/plain', start_char_idx=0, end_char_idx=2421, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8371579103042845)], metadata={'b625eb4d-7755-4bac-898f-e9de207833f5': {'page_label': '325', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}, 'b1189607-7c41-43b8-8ccf-e4ccb7bb3e05': {'page_label': '225', 'file_name': 'Deep Learning3.pdf', 'file_path': '/content/Documents/Documents/Deep Learning3.pdf', 'file_type': 'application/pdf', 'file_size': 8795503, 'creation_date': '2025-03-18', 'last_modified_date': '2025-03-18'}})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USKXbMr-OKTk",
        "outputId": "c855ea47-71a6-4b56-8c0d-e53f264c8888"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_index.core.base.response.schema.Response"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55DDX36LOQ_e",
        "outputId": "e9db2b31-ee35-4676-cc0d-bb5b0850d325"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b625eb4d-7755-4bac-898f-e9de207833f5': {'page_label': '325',\n",
              "  'file_name': 'Deep Learning3.pdf',\n",
              "  'file_path': '/content/Documents/Documents/Deep Learning3.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 8795503,\n",
              "  'creation_date': '2025-03-18',\n",
              "  'last_modified_date': '2025-03-18'},\n",
              " 'b1189607-7c41-43b8-8ccf-e4ccb7bb3e05': {'page_label': '225',\n",
              "  'file_name': 'Deep Learning3.pdf',\n",
              "  'file_path': '/content/Documents/Documents/Deep Learning3.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 8795503,\n",
              "  'creation_date': '2025-03-18',\n",
              "  'last_modified_date': '2025-03-18'}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "RuVB2no-OV5d",
        "outputId": "50554282-bf98-4071-dcb2-9a197dbf515c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CNNs in Deep Learning offer several benefits. They are effective in tasks like computer vision and image recognition due to weight sharing in convolutional layers and subsampling in pooling layers, which provide properties like translation invariance. Additionally, CNNs have been found to be highly efficient in handling complex pattern recognition tasks. Moreover, with appropriate modifications, CNNs can also be applied successfully to speech recognition tasks. The use of non-saturating neurons like rectified linear units (ReLU) and regularization techniques like \"dropout\" further enhance the performance of CNNs by speeding up the training process and improving generalization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Openai Embedding"
      ],
      "metadata": {
        "id": "Ao-CFMpcQkc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexopai=VectorStoreIndex.from_documents(documents,show_progress=True)"
      ],
      "metadata": {
        "id": "kKlWLm-XQlZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "2e5ab75ddb5c45c594efe23839b6304d",
            "d81724a18b96491eb045a28a6a157880",
            "5b61af2fcdaa4288a95bb36bc3001b97",
            "bb9384c27ee047dcbe046f6269337d6a",
            "14d676e5ce8646658b509aa317549222",
            "7eec4a6f1a6840199715e1ebbafb1ca1",
            "301de1218a224863b689c668cc0da964",
            "ffe323d6eec64305afdda93609afc5dc",
            "0557a96d527a499aba27fbc7acf6185c",
            "350d089dc83f43e3a61f11336bf2164d",
            "dfcda29dd0b64c86bb51f723e278beff",
            "1dbfdddc836a4cafabf53a9368fd0d46",
            "4ca702fe73044817a211115dd4c4286c",
            "f59374d6fbde467db971807f530bae71",
            "85ad4f6d5c9f4550870266182c8527b7",
            "343792ebfc6e4b1fbe780238e3d831a9",
            "d1cc7de5e4484c40a9c93420d5d5ab4b",
            "68b96396440142b293383ea30cfb9452",
            "80eb7b00f0b545f4ac6ee1382efdd8d9",
            "8f898e21b7e9486f9f55cb7e5650aae6",
            "018f258bb20346809591a4221b85d94e",
            "6db830500443440d8d413a7d98069f67"
          ]
        },
        "outputId": "10dbaff9-23cc-4641-a78a-5ecf22bdf172"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/362 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e5ab75ddb5c45c594efe23839b6304d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/361 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dbfdddc836a4cafabf53a9368fd0d46"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=indexopai.as_query_engine()"
      ],
      "metadata": {
        "id": "G3LsAkBzQqIg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "retriever=VectorIndexRetriever(index=indexopai,similarity_top_k=4)\n",
        "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.80)\n",
        "\n",
        "query_engine=RetrieverQueryEngine(retriever=retriever,\n",
        "                                  node_postprocessors=[postprocessor])"
      ],
      "metadata": {
        "id": "ZI87SyP7QsPH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is Recurrent Neural Networks in Python?\")"
      ],
      "metadata": {
        "id": "iD7ayZOEQuIt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "pprint_response(response,show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "AMDm2c6BQwxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d2fa2c-a881-47a7-e4d4-74c581fbcc77"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: Recurrent Neural Networks in Python are a type of\n",
            "neural network that are particularly suited for learning sequences.\n",
            "They are more expressive and powerful compared to models like Markov\n",
            "Models due to not being limited by the Markov assumption. In Python,\n",
            "Recurrent Neural Networks can be used for tasks such as language\n",
            "modeling, word embeddings, and predicting values in a sequence.\n",
            "______________________________________________________________________\n",
            "Source Node 1/4\n",
            "Node ID: b8e84477-3dee-46ce-b299-f8e089e3ca7b\n",
            "Similarity: 0.8324452926992131\n",
            "Text: Introduction                            Recurrent       Neural\n",
            "Networks        are     all     about   learning        sequences.\n",
            "You     may     have    already learned about   Markov  models  for\n",
            "sequence        modeling, which   make    use     of      the\n",
            "Markov  assumption,     p{      x(t)    |       x(t-1), ...,    x(1)\n",
            "}       =       p{      x(t)    | x(t-1)  }.      In      other\n",
            "words,  the     current value   depends only    on      the     last\n",
            "value.  While easy    to      train,  one     can     im...\n",
            "______________________________________________________________________\n",
            "Source Node 2/4\n",
            "Node ID: ac051359-b035-40d0-9f31-ddf7ccee1c19\n",
            "Similarity: 0.8306425106659705\n",
            "Text: Chapter   3:      Recurrent       Neural  Networks        for\n",
            "NLP                            NLP,    or      natural language\n",
            "processing,     is      basically       machine learning        for\n",
            "text, speech, and     language.                            It’s\n",
            "tightly coupled with    recurrent       neural  networks,       and\n",
            "a       lot     of      the     RNN examples        you’ll  see\n",
            "here    and     elsewhere       use     word    sequences       as\n",
            "examples,       for several reasons.                            First,\n",
            "language        is      ...\n",
            "______________________________________________________________________\n",
            "Source Node 3/4\n",
            "Node ID: 081c9a07-124a-4479-86c6-0e53ffb2f8bc\n",
            "Similarity: 0.8282229359815843\n",
            "Text: We        are     going   to      revisit a       classical\n",
            "neural  network problem -       the     XOR     problem, but     we’re\n",
            "going   to      extend  it      so      that    it      becomes the\n",
            "parity  problem -       you’ll  see that    regular feedforward\n",
            "neural  networks        will    have    trouble solving this\n",
            "problem but     recurrent       networks        will    work\n",
            "because the     key     is      to      treat   the     input   as\n",
            "a sequence.                            In      the     next\n",
            "secti...\n",
            "______________________________________________________________________\n",
            "Source Node 4/4\n",
            "Node ID: 793bfac2-c893-4671-bd63-fe5c20b122eb\n",
            "Similarity: 0.8225389175298623\n",
            "Text: With      recurrent       nets,   we      optimize        this\n",
            "joint   probability     implicitly,     so      we      never have\n",
            "to      actually        calculate       it,     thus    avoiding\n",
            "any     underflow.                            In      addition,\n",
            "recurrent       nets    need    not     make    the     Markov\n",
            "assumption.                            This    could   lead    us\n",
            "to      the     intuition       that    these   recurrent       neural\n",
            "networks        might   be more    powerful.\n",
            "So      to      conclude,       we’ve   ident...\n",
            "Recurrent Neural Networks in Python are a type of neural network that are particularly suited for learning sequences. They are more expressive and powerful compared to models like Markov Models due to not being limited by the Markov assumption. In Python, Recurrent Neural Networks can be used for tasks such as language modeling, word embeddings, and predicting values in a sequence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Storaging Response"
      ],
      "metadata": {
        "id": "C21wjTZyQ0tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "# check if storage already exists\n",
        "PERSIST_DIR = \"./storage\"\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # load the documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"/content/Documents/Documents/\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # store it for later\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "else:\n",
        "    # load the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "# either way we can now query the index\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"what is Recurrent Neural Networks in Python?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "WzD5W9RKQ234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a114c2-663c-4638-de60-d95b8e73ec8f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recurrent Neural Networks in Python are a type of neural network that excel at learning sequences. They are more expressive and powerful compared to models like Markov models due to not being limited by the Markov assumption. Recurrent Neural Networks are commonly used in tasks related to natural language processing, speech recognition, and other sequential data analysis tasks.\n"
          ]
        }
      ]
    }
  ]
}