{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Agents are intelligent entities designed to dynamically decide which actions to take based on the input they receive. They use large language models (LLMs) to interpret instructions and determine the best course of action, allowing for flexible and adaptive behavior in various applications."
      ],
      "metadata": {
        "id": "qz3YU0Na5yk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPbPvLqro1EV",
        "outputId": "a620ef5a-4641-4075-e28d-7a8683e59cc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.44)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting openai<2.0.0,>=1.66.3 (from langchain_openai)\n",
            "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.45-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.44\n",
            "    Uninstalling langchain-core-0.3.44:\n",
            "      Successfully uninstalled langchain-core-0.3.44\n",
            "Successfully installed langchain-core-0.3.45 langchain_openai-0.3.9 openai-1.66.3 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "SNhCtLAs05mJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agents"
      ],
      "metadata": {
        "id": "4SIcH8uGr5D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
      ],
      "metadata": {
        "id": "mzyhT8a4roAj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "id": "6qkwYDJJrzBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "189592c5-772f-438e-f534-d4d6a7f51fe9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.3)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20250306-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (2025.1.31)\n",
            "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading types_requests-2.32.0.20250306-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.21 types-requests-2.32.0.20250306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
      ],
      "metadata": {
        "id": "e8tbh8a6rs6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpoQQf8k1gVq",
        "outputId": "aaae9c97-23e9-489f-d2c3-0df6f36d165f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwikWs8l2DEj",
        "outputId": "d0458d72-4aea-453e-89ec-ce3423b1fd60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-7.5.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Collecting primp>=0.14.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.1)\n",
            "Downloading duckduckgo_search-7.5.2-py3-none-any.whl (20 kB)\n",
            "Downloading primp-0.14.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-7.5.2 primp-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Agents\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun # Import the necessary tool\n",
        "# Initialize the DuckDuckGo search tool (or any other tools you want to use)\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Define tools\n",
        "tools = [search]\n",
        "\n",
        "agent=create_openai_tools_agent(llm,tools,prompt)"
      ],
      "metadata": {
        "id": "UdEtZQgBr8nD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent Executer\n",
        "from langchain.agents import AgentExecutor\n",
        "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n"
      ],
      "metadata": {
        "id": "iiOn507ar9E9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
      ],
      "metadata": {
        "id": "YfLtEzkRsCbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0f4371-07e8-4a04-a7e4-bc7001f61100"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `duckduckgo_search` with `{'query': 'Langsmith'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mLangsmith is a framework that enhances and streamlines the development of natural language processing (NLP) applications. It integrates with LangChain, a library for chaining multiple language models, and offers tools for managing, deploying, and scaling NLP workflows. You might use LangChain to define the core logic of your application, LangSmith to monitor its performance and evaluate outputs, and LangGraph to orchestrate complex, multi-step workflows. LangSmith is a unified platform for debugging, testing, and monitoring LangChain applications. It acts as an observability layer, providing developers with detailed insights into the execution of their chains and agents. LangSmith is a monitoring and testing tool for LLM applications in production. It helps developers ensure their systems are efficient, reliable, and cost-effective. LangSmith is a unified platform for developing, testing, and monitoring LLM applications, whether you're using LangChain or not. Learn how to purchase LangSmith as an Azure Kubernetes Application and enjoy data security, ease of deployment, and co-sell benefits.\u001b[0m\u001b[32;1m\u001b[1;3mLangsmith is a framework that enhances and streamlines the development of natural language processing (NLP) applications. It integrates with LangChain, a library for chaining multiple language models, and offers tools for managing, deploying, and scaling NLP workflows. LangSmith is a unified platform for debugging, testing, and monitoring LangChain applications, providing developers with detailed insights into the execution of their chains and agents. It is a monitoring and testing tool for LLM applications in production, ensuring efficiency, reliability, and cost-effectiveness. LangSmith is available for purchase as an Azure Kubernetes Application, offering data security, ease of deployment, and co-sell benefits.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmU83dLa2WXD",
        "outputId": "458ada1b-7314-4af1-a8b8-c323ed91c649"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Tell me about Langsmith',\n",
              " 'output': 'Langsmith is a framework that enhances and streamlines the development of natural language processing (NLP) applications. It integrates with LangChain, a library for chaining multiple language models, and offers tools for managing, deploying, and scaling NLP workflows. LangSmith is a unified platform for debugging, testing, and monitoring LangChain applications, providing developers with detailed insights into the execution of their chains and agents. It is a monitoring and testing tool for LLM applications in production, ensuring efficiency, reliability, and cost-effectiveness. LangSmith is available for purchase as an Azure Kubernetes Application, offering data security, ease of deployment, and co-sell benefits.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "jTWY6emx2YXS",
        "outputId": "2af6f3f5-5d4e-4c8d-c490-d3f45c44db00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langsmith is a framework that enhances and streamlines the development of natural language processing (NLP) applications. It integrates with LangChain, a library for chaining multiple language models, and offers tools for managing, deploying, and scaling NLP workflows. LangSmith is a unified platform for debugging, testing, and monitoring LangChain applications, providing developers with detailed insights into the execution of their chains and agents. It is a monitoring and testing tool for LLM applications in production, ensuring efficiency, reliability, and cost-effectiveness. LangSmith is available for purchase as an Azure Kubernetes Application, offering data security, ease of deployment, and co-sell benefits.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = agent_executor.invoke({\"input\":\"What's the paper 1605.08386 about?\"})"
      ],
      "metadata": {
        "id": "EjKggrtmsFsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6840d6-4d9e-4734-d14c-a24c22e2f6ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `duckduckgo_search` with `{'query': 'paper 1605.08386'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mThe paper 1605.08386 is a review discussing the advancements and current state of deep learning techniques in the field of automatic speech recognition (ASR), covering various deep learning architectures and their applications in ASR, as well as challenges and future directions.' I am following Krish NAik's tutorial on Langchain and agents. I'm trying to build an agent executer using open_ai_tools_agent. My tools include a Wikipedia tool and an Arxiv tool. However, whenever I 使用ArXiv API获取论文信息 在设置好环境后，可以通过langchain包中的工具函数来加载ArXiv API，并使用代理服务以提高访问稳定性。以下是一个基本的实现： 在这段代码中，我们通过调用 玩转ArXiv API：用AI智能工具轻松获取论文信息 引言 在科学研究和技术开发中，查阅最新的学术论文是必不可少的。然而，由于数据库庞大且论文结构复杂，如何快速、有效地获取相关信息成为一大挑战。本文 抽取式摘要（Extractive Summarization）：通过从原文中抽取关键信息生成摘要，保留了原文的语言结构。生成式摘要（Abstractive Summarization）：通过模型生成新的文本来表达原文的核心信息，类似于人类写作摘要的方式。本项目中，我们将重点介绍基于深度学习的生成式摘要方法。\u001b[0m\u001b[32;1m\u001b[1;3mThe paper 1605.08386 is a review discussing the advancements and current state of deep learning techniques in the field of automatic speech recognition (ASR), covering various deep learning architectures and their applications in ASR, as well as challenges and future directions. \n",
            "\n",
            "If you are working on building an agent executer using open_ai_tools_agent and need assistance with using the ArXiv API to retrieve paper information, I can help guide you through the process. Let me know how you would like to proceed!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FclwhVTj2hhD",
        "outputId": "3fcccf23-b575-43e0-ebaa-922cbf838e28"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"What's the paper 1605.08386 about?\",\n",
              " 'output': 'The paper 1605.08386 is a review discussing the advancements and current state of deep learning techniques in the field of automatic speech recognition (ASR), covering various deep learning architectures and their applications in ASR, as well as challenges and future directions. \\n\\nIf you are working on building an agent executer using open_ai_tools_agent and need assistance with using the ArXiv API to retrieve paper information, I can help guide you through the process. Let me know how you would like to proceed!'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res1['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "NzDFAmvB2isa",
        "outputId": "24314c40-0829-425a-dbba-b3a67f52b079"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper 1605.08386 is a review discussing the advancements and current state of deep learning techniques in the field of automatic speech recognition (ASR), covering various deep learning architectures and their applications in ASR, as well as challenges and future directions. \\n\\nIf you are working on building an agent executer using open_ai_tools_agent and need assistance with using the ArXiv API to retrieve paper information, I can help guide you through the process. Let me know how you would like to proceed!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wikipedia Tool"
      ],
      "metadata": {
        "id": "lYSVmeEOq-N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "1bkMRZVD54iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb2d6d5-2d77-408f-9a47-90b61a280189"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=bb3ce4d42157031f914193ec51274d48236ee6182213ada40a7b716add66e3d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv langchain_community wikipedia langchain_openai faiss-cpu"
      ],
      "metadata": {
        "id": "Y5AM-deAq-kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a8ac2b-de13-47df-c31d-aaaf434817dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.66.3 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.66.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=e2910e83d617eed03ec4a6923f6eea84d4ad42993a7b65abdd1aa1be8a2919ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, faiss-cpu, arxiv\n",
            "Successfully installed arxiv-2.1.3 faiss-cpu-1.10.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "DTf3URb-59VZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools(['wikipedia', 'llm-math'], llm = llm)"
      ],
      "metadata": {
        "id": "FtkauU386CRw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools,\n",
        "                 llm,\n",
        "                 agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                 verbose = True)"
      ],
      "metadata": {
        "id": "LRuNhQ-F6Hq4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res3 = agent.run(\"Where is Rashtrapati Bhavan located?\")"
      ],
      "metadata": {
        "id": "HQkq4O3Z6J6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5782567a-28db-4e03-a5a1-9e1d85b2c672"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find the location of Rashtrapati Bhavan.\n",
            "Action: wikipedia\n",
            "Action Input: Rashtrapati Bhavan\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Rashtrapati Bhavan\n",
            "Summary: The Rashtrapati Bhavan (, ISO: Rāṣṭrapati Bhavana; lit. 'Presidential Palace'; formerly Viceroy's House (1931–1947) and Government House (1947–1950)) is the official residence of the President of the Republic of India at the western end of Rajpath, Raisina Hill in New Delhi. It was constructed during the British Raj.\n",
            "Rashtrapati Bhavan may refer to only the 340-room main building that has the president's official residence, including reception halls, guest rooms and offices, also called the mansion; it may also refer to the entire 130-hectare (320-acre) Presidential Estate that additionally includes the presidential gardens, large open spaces, residences of bodyguards and staff, stables, other offices and utilities within its perimeter walls. In terms of area, it is the second largest residence of any head of state in the world after the Quirinal Palace in Italy.\n",
            "The other presidential homes are the Rashtrapati Nilayam in Hyderabad, (Telangana), Rashtrapati Ashiana in Dehradun, (Uttarakhand), Rashtrapati Niwas, Mashobra and Rashtrapati Niwas in Shimla, (Himachal Pradesh).\n",
            "\n",
            "Page: Rashtrapati Bhavan Museum\n",
            "Summary: Rashtrapati Bhavan Museum (ISO: Rāṣṭrapati Bhavana Saṁgrahālaya lit. 'Presidential Palace Museum') is a public biographical museum located on the Raisina hill of New Delhi dedicated to the presidents of India  from its establishment as a republic to present day. It was established on 25 July 2014 by Pranab Mukherjee, who served as the president of India at the time. As the first underground museum of India, it was opened for the public on 24 July 2016.\n",
            "The museum contains artifacts and information surrounding the position of the president of India as well as the people who held the office. Aside from the presidents, it also displays information about the prime ministers  of India, the establishment of New Delhi during the colonial rule, Delhi Durbar, the Indian independence movement, the partition of India and drafting of the Indian constitution. The museum uses tools like virtual and augmented reality, interactive digital cascading tables, video walls, three-dimensional stereoscopic projections, holographic projections, and sound-light-video synchronized stage settings to display information.\n",
            "\n",
            "Page: Rashtrapati Nilayam\n",
            "Summary: Rashtrapati Nilayam (lit. 'The president's house') formerly known as Residency House is the official winter retreat of the President of India located in Hyderabad, Telangana. It was officially opened to the public in March 2023, allowing visitors to explore its vast grounds, thematic gardens, and architectural marvels. The President stays here for at least two weeks during their winter sojourn visit and conducts official business. It is also used as a guest house for visiting dignitaries. It is located in Bolaram in Secunderabad, a neighbourhood of Hyderabad.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: Rashtrapati Bhavan is located in New Delhi, India.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jjgBox8B29Kq",
        "outputId": "a67c14e7-3e75-4bb2-f758-0985d49b4d33"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rashtrapati Bhavan is located in New Delhi, India.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res4 = agent.invoke(\"How much is the Eiffel tower taller than the leaning tower of Pisa? What is the difference?\")"
      ],
      "metadata": {
        "id": "OVMeOx0E6OQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afed890c-f7fa-46fc-bf92-aade305e0fb3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to calculate the difference in height between the Eiffel Tower and the Leaning Tower of Pisa.\n",
            "Action: Calculator\n",
            "Action Input: Height of Eiffel Tower (330 meters) - Height of Leaning Tower of Pisa (56 meters)\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 274\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe Eiffel Tower is 274 meters taller than the Leaning Tower of Pisa.\n",
            "Final Answer: The Eiffel Tower is 274 meters taller than the Leaning Tower of Pisa.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhqWmTxl3JRr",
        "outputId": "25736cfa-1361-4e7b-e144-bb4be55f6245"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'How much is the Eiffel tower taller than the leaning tower of Pisa? What is the difference?',\n",
              " 'output': 'The Eiffel Tower is 274 meters taller than the Leaning Tower of Pisa.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res4['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5q5F3rZ13Li0",
        "outputId": "4164ec60-a14b-4408-9f6a-f93dc401c325"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Eiffel Tower is 274 meters taller than the Leaning Tower of Pisa.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "66P4JgiRrFFr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
        "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
      ],
      "metadata": {
        "id": "ehzPtyCGrHSz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "docs=loader.load()\n",
        "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
        "vectordb=FAISS.from_documents(documents,OpenAIEmbeddings())\n",
        "retriever=vectordb.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "id": "2ham294mrXES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f936709-668b-4eb8-b164-acfab7536023"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd93293ca90>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "retriever_tool=create_retriever_tool(retriever,\"langsmith_search\",\n",
        "                      \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")"
      ],
      "metadata": {
        "id": "nKToXYkKrasb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh59tvf83gME",
        "outputId": "098a9998-d3df-49c6-9216-e3f9e25062c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7fd93985db20>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd93293ca90>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7fd93935d580>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd93293ca90>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool.as_tool()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7nl_uX033TV",
        "outputId": "5b2e0b97-3443-400e-eb68-fa7904135a48"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='langsmith_search', description=\"Takes {'description': 'Input to the retriever.', 'properties': {'query': {'description': 'query to look up in retriever', 'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'RetrieverInput', 'type': 'object'}.\", args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function convert_runnable_to_tool.<locals>.invoke_wrapper at 0x7fd9328e84a0>, coroutine=<function convert_runnable_to_tool.<locals>.ainvoke_wrapper at 0x7fd9328ea520>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Tool"
      ],
      "metadata": {
        "id": "R67nw5OXrfL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Arxiv Tool\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun\n",
        "\n",
        "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
        "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
        "arxiv.name"
      ],
      "metadata": {
        "id": "p-ILxQ4wrfeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1614d63e-d53b-4001-d264-179a6f484710"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[wiki,arxiv,retriever_tool]"
      ],
      "metadata": {
        "id": "cLHmYECfrk78"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqC0U6J-4Rxu",
        "outputId": "eb81a5ba-ea8e-4294-9c05-1fd83a3e345b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/usr/local/lib/python3.11/dist-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
              " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
              " Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7fd93985db20>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd93293ca90>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7fd93935d580>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd93293ca90>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculator Tool"
      ],
      "metadata": {
        "id": "ZzgwcG2Oq40B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from typing import Optional  # Import Optional for type hinting\n",
        "\n",
        "\n",
        "class CalculatorTool(BaseTool):\n",
        "    \"\"\"A simple calculator tool for math operations.\"\"\"\n",
        "\n",
        "    name: str = \"Calculator\"  # Type annotation added\n",
        "    description: str = \"Perform basic arithmetic operations.\"  # Type annotation added\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        try:\n",
        "            result = eval(query)\n",
        "            return str(result)\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    async def _arun(self, query: str) -> str:\n",
        "        return self._run(query)  # Simplified for sync demonstration\n",
        "\n",
        "\n",
        "# Initialize tools\n",
        "calculator = CalculatorTool()\n",
        "\n",
        "# Use OpenAI as the language model (you need your OpenAI API key)\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize memory for conversation context\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Define tools available for the agent\n",
        "# Instead of Tool.from_tool, create a Tool instance directly\n",
        "tools = [Tool(\n",
        "    name=calculator.name,\n",
        "    func=calculator._run,\n",
        "    description=calculator.description\n",
        ")]\n",
        "\n",
        "# Create an agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "f0dPh0a0plUC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent with a query\n",
        "result = agent.run(\"What is 25 * 10?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oXWxP74icH",
        "outputId": "d67b61ba-dfa7-451e-d60b-d3f9df7cf1a4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the Calculator tool to perform multiplication.\n",
            "Action: Calculator\n",
            "Action Input: 25 * 10\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m250\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: 250\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McsRejzV4lCI",
        "outputId": "be97703b-cb18-422a-802b-a417394eb231"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#format_tool_to_openai_function"
      ],
      "metadata": {
        "id": "LVUbvDAgspRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool, format_tool_to_openai_function\n",
        "\n",
        "# Define a simple custom tool\n",
        "def greet_user(name: str) -> str:\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "greet_tool = Tool.from_function(\n",
        "    func=greet_user,\n",
        "    name=\"greet_user\",\n",
        "    description=\"Greets the user by name\"\n",
        ")\n",
        "\n",
        "# Convert the tool to an OpenAI-compatible function\n",
        "openai_function = format_tool_to_openai_function(greet_tool)\n",
        "print(\"OpenAI-Compatible Function:\", openai_function)"
      ],
      "metadata": {
        "id": "KDgDE5_Isrdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea111ee-e2a9-453a-ad81-15e5be8a197d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI-Compatible Function: {'name': 'greet_user', 'description': 'Greets the user by name', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YouTubeSearchTool"
      ],
      "metadata": {
        "id": "h0v2yk77sznF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-search-python"
      ],
      "metadata": {
        "id": "JfkdZeqos0VW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c240535c-0389-4713-de9a-95a4da36e3e5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-search-python\n",
            "  Downloading youtube_search_python-1.6.6-py3-none-any.whl.metadata (99 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.14.2 in /usr/local/lib/python3.11/dist-packages (from youtube-search-python) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.14.2->youtube-search-python) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.14.2->youtube-search-python) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.14.2->youtube-search-python) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.14.2->youtube-search-python) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.14.2->youtube-search-python) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.14.2->youtube-search-python) (1.3.1)\n",
            "Downloading youtube_search_python-1.6.6-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-search-python\n",
            "Successfully installed youtube-search-python-1.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-search"
      ],
      "metadata": {
        "id": "14rKt70ss2ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c876aad-78a6-467b-9170-c990d4ce01c3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-search\n",
            "  Downloading youtube_search-2.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-search) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-search) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-search) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-search) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-search) (2025.1.31)\n",
            "Downloading youtube_search-2.1.2-py3-none-any.whl (3.4 kB)\n",
            "Installing collected packages: youtube-search\n",
            "Successfully installed youtube-search-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import YouTubeSearchTool\n",
        "\n",
        "# Initialize the YouTube search tool\n",
        "youtube_tool = YouTubeSearchTool()\n",
        "\n",
        "# Search for AI-related content on YouTube\n",
        "result = youtube_tool.run(\"Generative AI tutorials\")\n",
        "print(\"YouTube Search Result:\", result)"
      ],
      "metadata": {
        "id": "ZNgOsNNTs7bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062924f0-2245-4f01-e1fd-a34d362f079e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube Search Result: ['https://www.youtube.com/watch?v=d4yCWBGFCEs&pp=ygUXR2VuZXJhdGl2ZSBBSSB0dXRvcmlhbHM%3D', 'https://www.youtube.com/watch?v=2p5OHDxR2l8&pp=ygUXR2VuZXJhdGl2ZSBBSSB0dXRvcmlhbHM%3D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MoveFileTool"
      ],
      "metadata": {
        "id": "qFTsTtUps-lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import MoveFileTool\n",
        "\n",
        "move_tool = MoveFileTool()\n",
        "\n",
        "# Move a file from source to destination\n",
        "source = \"NLP.txt\"\n",
        "destination = \"/content/MoveTo/\""
      ],
      "metadata": {
        "id": "Yd5ak1e8s_Ue"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = move_tool.invoke({\"source_path\": source, \"destination_path\": destination})"
      ],
      "metadata": {
        "id": "TVISPMDq6JdA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"File Move Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTb0VcCK5x1I",
        "outputId": "d3f79f71-2e0e-46f0-85ed-4b9d17fca964"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Move Result: File moved successfully from NLP.txt to /content/MoveTo/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RequestsGetTool"
      ],
      "metadata": {
        "id": "wtmfXgUFtGMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import RequestsGetTool\n",
        "from langchain.utilities import RequestsWrapper\n",
        "\n",
        "# Create a RequestsWrapper instance\n",
        "requests_wrapper = RequestsWrapper()\n",
        "\n",
        "# Initialize the RequestsGetTool with the requests_wrapper\n",
        "get_tool = RequestsGetTool(requests_wrapper=requests_wrapper, allow_dangerous_requests=True)"
      ],
      "metadata": {
        "id": "jDumQa5ItHBO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_tool.invoke(\"https://jsonplaceholder.typicode.com/posts/1\")"
      ],
      "metadata": {
        "id": "Ldh3W9wm6XgB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GET Request Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JThHyeRc6ceM",
        "outputId": "7439d14f-44bc-4ff8-d03f-8c602c60f59a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GET Request Response: {\n",
            "  \"userId\": 1,\n",
            "  \"id\": 1,\n",
            "  \"title\": \"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
            "  \"body\": \"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RequestsPostTool"
      ],
      "metadata": {
        "id": "kjySu0qctNdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import RequestsPostTool\n",
        "from langchain.utilities import RequestsWrapper\n",
        "\n",
        "# Create a RequestsWrapper instance\n",
        "requests_wrapper = RequestsWrapper()\n",
        "\n",
        "# Initialize the RequestsPostTool with allow_dangerous_requests set to True and requests_wrapper\n",
        "post_tool = RequestsPostTool(allow_dangerous_requests=True, requests_wrapper=requests_wrapper)"
      ],
      "metadata": {
        "id": "ezN9JmdqtOUd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = post_tool.invoke(\n",
        "    \"https://jsonplaceholder.typicode.com/posts\",  # URL\n",
        "    {\"title\": \"AI Post\", \"body\": \"This is about AI.\", \"userId\": 1}  # JSON data\n",
        ")"
      ],
      "metadata": {
        "id": "Zi-7Ghr86151"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POST Request Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5psd0Xt6j-t",
        "outputId": "15db6f2b-cb83-4749-d85e-4ad5ba0bfe07"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POST Request Response: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PythonREPLTool"
      ],
      "metadata": {
        "id": "VYXkJCHRtSKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental"
      ],
      "metadata": {
        "id": "ldD5bn7ztWH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd91680-a6ad-4aee-d419-69829a407005"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.19)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.45)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.10.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.6)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "python_tool = PythonREPLTool()"
      ],
      "metadata": {
        "id": "AZJuoTW_tbKX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = python_tool.invoke(\"2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aZG9Quk7FtU",
        "outputId": "783a8331-a7b0-4ad9-b267-12731af0378d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Python Code Execution Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T8jvSmQ7Z32",
        "outputId": "8eea5b8e-6431-4f4a-b7b9-05eb075a837e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Code Execution Result: \n"
          ]
        }
      ]
    }
  ]
}